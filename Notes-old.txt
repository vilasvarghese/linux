Day – 1
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
LINUX BASICS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. What is Linux?

Linux is an 
	open-source, 
	Unix-like operating system 
	kernel initially developed by Linus Torvalds in 1991. 
	Unlike proprietary operating systems such as Windows and macOS, 
		Linux is free to use, modify, and distribute. 
		
	While "Linux" strictly refers to the kernel—the core program that manages hardware resources and system operations—the term is commonly used to describe an entire operating system that combines the Linux kernel with various software tools and utilities.

Key Characteristics:

	Open Source: Linux’s source code is freely available, allowing anyone to study, modify, and distribute it.

	Portability: Linux runs on a wide range of hardware platforms, from personal computers to mainframes and IoT devices.

	Security: Its robust design and active community make Linux one of the most secure operating systems.

	Multi-user and Multi-tasking: 
		Linux supports multiple users and processes simultaneously.




	• Concept of an Operating System
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		Most important software that runs on a computer. 
		It manages the computer's 
			memory and 
			processes
			all software and hardware. 
		Allows to communicate with the computer without knowing it's language. 
		
		Your computer's operating system (OS) manages 
			software  
			hardware . 
		Several different programs are running at the same time
			They all need to access your computer's 
				CPU
				memory
				storage. 
		The operating system coordinates this.
		
		The three most common operating systems 
			Microsoft Windows, 
			macOS, and 
			Linux.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Salient Features of the Linux Operating System
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		Process management:- 
			OS creates and deletes processes. 
			synchronizes communication among processes.
		Memory management:- 
			allocate and de-allocates  memory space to programs.
		File management:- 
			Manages all the file-related activities such as 
				organization storage
				retrieval
				naming
				sharing
				protection of files.
		Device Management: 
			keeps tracks of all devices. Feature also known as the I/O controller. 
			Allocation and de-allocation of the devices.
		I/O System Management: 
			One of the main objects of any OS is to hide the peculiarities of that hardware devices from the user.
		Secondary-Storage Management: 
			Systems have several levels of storage -
				primary storage, 
				secondary storage
				cache storage. 
			Instructions and data must be stored in primary storage or cache so that a running program can reference it.
		Security:- 
			Security module protects the data and information of a computer system against malware threat and authorized access.
		Command interpretation: 
			Interprets commands given by user 
			acts as system resources to process that commands.
		Networking: 
			A distributed system is a group of processors which do not share memory, hardware devices, or a clock. The processors communicate with one another through the network.
		Job accounting: 
			Keeping track of time & resource used by various job and users.
		Communication management: 
			Coordination and assignment of 
				compilers, 
				interpreters
				another software resource of the various users of the computer systems.

		Protected and Supervisor mode
		---------------------------
		Protected and Supervisor Mode in Linux OS
Protected and Supervisor modes are essential components of modern operating systems like Linux, providing different levels of access to system resources to ensure security, stability, and efficient multitasking. Here's a detailed explanation of these concepts in Linux:

1. Protected Mode
	Protected mode refers to the operational state of the CPU where memory protection and privilege levels are enforced to ensure the stability and security of the system. It was introduced in Intel's x86 processors starting with the 80286 architecture.

	Key Features of Protected Mode:
	Memory Protection:

	Processes are given isolated memory spaces.
	The system prevents unauthorized access to memory regions.
	Multitasking:

	Allows multiple processes to run simultaneously while isolating them from each other.
	Process isolation ensures that one process cannot interfere with another.
	Access Levels:

	Implements privilege rings (from Ring 0 to Ring 3) to restrict resource access based on the mode of operation:
	Ring 0: Kernel mode (highest privilege).
	Ring 3: User mode (restricted access).
	Segmentation and Paging:

	Segmentation: Divides memory into segments for better organization.
	Paging: Enables virtual memory by dividing memory into fixed-size pages.
	Linux and Protected Mode:
	Linux runs in protected mode by default on modern CPUs.
	The kernel operates in Ring 0 (full access to hardware), while user applications run in Ring 3 (limited access).
	Switching between these rings occurs via system calls and interrupts.
2. Supervisor Mode (Kernel Mode)
	Supervisor mode, also known as kernel mode, is a privileged operational state where the operating system kernel has unrestricted access to all hardware and system resources.

	Key Characteristics of Supervisor Mode:
	Full Resource Access:

	The kernel can execute privileged instructions like I/O operations, memory management, and CPU scheduling.
	Privileged Instructions:

	Directly manages hardware via privileged CPU instructions.
	Examples: Enabling/disabling interrupts, accessing hardware registers.
	Kernel Space:

	In supervisor mode, the CPU operates in kernel space, which is reserved for critical system operations.
	User applications cannot directly access kernel space to prevent accidental or malicious interference.
	Context Switching:

	The kernel handles context switching between processes, ensuring each process gets CPU time without interfering with others.
	Linux and Supervisor Mode:
	When a system call is made (e.g., file I/O, memory allocation), the CPU transitions from user mode to supervisor mode to execute the request.
	The transition is achieved via trap instructions or interrupts.
3. User Mode vs Supervisor Mode
	Feature	User Mode	Supervisor Mode
	Access Level	Limited access to resources.	Full access to hardware and system.
	Execution Context	Runs applications and user-level code.	Runs the kernel and system services.
	Memory Access	Access only to its own memory space.	Access to all memory regions.
	Privileged Instructions	Not allowed.	Allowed.
	Stability and Security	Isolated to prevent system crashes.	Full control, but improper use can cause crashes.
4. Transition Between Modes
	The system alternates between user mode and supervisor mode as needed:
	User Mode to Supervisor Mode:
	Triggered by a system call, interrupt, or fault.
	Example: A user process requests file access via a system call (open()).
	Supervisor Mode to User Mode:
	Happens after the kernel completes its task and returns control to the user process.
5. Use Cases of Protected and Supervisor Mode in Linux
	Security:
	By running user applications in protected mode, the kernel ensures they cannot access or modify critical system resources.
	Stability:
	Errors in user applications cannot directly crash the kernel, as they are isolated in user mode.
	Efficient Multitasking:
	The kernel manages scheduling, memory allocation, and hardware access in supervisor mode, enabling smooth multitasking.
	Hardware Abstraction:
	Supervisor mode provides an abstraction layer, ensuring user applications do not interact with hardware directly.
6. Examples of Mode Operations
	User Mode:
	Running a program like a text editor (vim) or browser.
	The program operates in its own virtual memory space.
	Supervisor Mode:
	Kernel handling disk I/O when the text editor saves a file.
	Kernel executing process scheduling to allocate CPU time.
7. How to Verify Mode Transitions in Linux
	System Calls:

	Use strace to observe system calls made by a process:
	 
	 
	 
	strace ls

	This shows how user mode requests are handled by the kernel.
	Kernel Logs:

	Check /var/log/messages or dmesg for kernel-related logs.
	Debugging Tools:

	Use tools like perf or gdb to trace kernel and user mode transitions.
	Conclusion
	Protected and supervisor modes in Linux are foundational to its robust and secure multitasking capabilities. By isolating processes in user mode and reserving privileged operations for supervisor mode, Linux ensures system integrity, security, and stability. Understanding these modes is crucial for system administrators, kernel developers, and anyone working with Linux internals.
			
			
		Memory management and virtual memory multitasking
		---------------------------
		
Memory Management in Linux
	Memory management in Linux is a critical feature of the operating system that efficiently handles system memory (RAM) and ensures that processes run smoothly. It includes allocation, protection, sharing, and the management of free memory.

Key Components of Memory Management
Physical Memory (RAM):

	Linux directly interacts with the physical memory, but its use is abstracted through virtual memory.
	Physical memory is divided into pages, typically 4 KB in size.
Virtual Memory:

	Allows processes to use more memory than physically available by creating an abstraction of the actual memory.
	Each process has its own virtual address space, preventing conflicts.
Page Tables:

	Maintain mappings between virtual addresses and physical memory.
	Use multi-level structures to reduce memory overhead.
Swapping:

	When physical memory runs out, inactive pages are moved to the swap space on disk.
	This helps manage memory-intensive tasks but may degrade performance (swap thrashing).
Kernel Memory Management:

	Kernel manages memory for system operations, drivers, and buffers.
	Memory allocation in the kernel uses structures like slab allocator for efficiency.
Virtual Memory Multitasking
	Virtual memory multitasking enables multiple processes to run simultaneously without interfering with each other. It relies on the kernel to manage memory mappings for each process.

How Virtual Memory Multitasking Works
Process Isolation:

	Each process is given its own isolated virtual memory space.
	The Memory Management Unit (MMU) translates virtual addresses to physical addresses.
Demand Paging:

	Pages are loaded into memory only when accessed, reducing the initial memory footprint.
	This uses a page fault mechanism, where missing pages are fetched from disk.
Shared Memory:

	Allows processes to share memory regions for inter-process communication (IPC).
	This is efficient for shared libraries, as the same code is loaded into memory once and reused.
 -On-Write (COW):

	Optimizes memory by allowing processes to share pages until a write operation occurs.
	Useful for fork() system calls, where child processes share the parent's memory space.
Swapping and Thrashing:

	When active memory exceeds physical RAM, the kernel uses swap space to offload less-used pages.
	Excessive swapping can lead to thrashing, severely impacting performance.
Key Linux Memory Management Commands
View System Memory Usage:
free -h: Displays total, used, and available memory in a human-readable format.
Inspect Virtual Memory Statistics:
vmstat: Shows memory, CPU, and process statistics.
Check Swap Usage:
swapon -s: Lists active swap partitions and their usage.
Process-Specific Memory:
top or htop: Provides memory usage details for individual processes.
ps aux: Lists processes along with their memory footprint.
Memory Allocation and Page Usage:
cat /proc/meminfo: Displays detailed memory statistics.
pmap: Provides memory mapping for a specific process.
Advantages of Virtual Memory Multitasking
Efficient Resource Utilization:
Allows the system to run more processes than physically possible by using disk space as an extension of RAM.
Process Isolation:
Prevents processes from interfering with each other's memory, ensuring security and stability.
Ease of Development:
Developers can assume a large contiguous memory space for their applications.
Swapping for Heavy Workloads:
Enables handling of memory-intensive applications without crashing.
Example Scenario: Virtual Memory in Action
Suppose a system runs multiple applications, such as a browser, a video player, and a spreadsheet.

The kernel assigns separate virtual address spaces to each application.
Common libraries used by all applications (like the C standard library) are shared in memory.
When a user switches between applications, the kernel uses the page table and MMU to translate addresses and fetch necessary data from RAM or swap.
Challenges and Optimization
Swap Overhead:
Excessive swapping can degrade performance; increasing physical RAM or reducing memory usage helps.
Thrashing:
Avoid by optimizing workloads and setting appropriate swap partition sizes.
Kernel Tunables:
Adjust /proc/sys/vm/swappiness to control the frequency of swapping.
Memory Limits:
Use ulimit to restrict memory usage for processes.
Conclusion
Memory management and virtual memory multitasking in Linux play a vital role in ensuring efficient system operation. By leveraging techniques like paging, swapping, and copy-on-write, Linux can handle large-scale multitasking and memory-intensive applications effectively. Proper understanding and management of these features are crucial for system administrators and developers to optimize performance and stability.

Let me know if you'd like this refined or added to the existing document!
		
		
		


		Error detection and handling
		---------------------------
		2. Error Handling Mechanisms in Linux
Kernel-Level Error Handling
Panic and Recovery:

	A critical kernel error triggers a kernel panic, halting the system to prevent further damage.
	Recovery options include rebooting or using kdump for debugging.
Error Correction:

ECC (Error-Correcting Code) memory is supported for automatic correction of single-bit errors.
RAID configurations provide redundancy to handle disk errors.
Logging and Reporting:

Kernel logs (dmesg, /var/log/kern.log) report errors.
Hardware Monitoring Tools:
lm-sensors: Monitors hardware health (CPU, GPU, temperature).
smartctl: Checks hard disk health using S.M.A.R.T.
User-Level Error Handling
Exit Status Codes:

Programs return exit codes to indicate success or failure:
0: Success.
Non-zero: Error (e.g., 1 for general errors, 127 for command not found).
Signal Handling:

Signals notify processes of errors (e.g., SIGSEGV, SIGPIPE).
Applications can catch and handle signals using libraries like signal.h.
c
 
 
#include <signal.h>
void handler(int sig) {
    printf("Caught signal %d\n", sig);
}
signal(SIGINT, handler);
Error Reporting with Logs:

Applications log errors to files or use logging daemons like rsyslog or journald.
Custom log files can be configured for application-specific errors.
3. Tools for Error Detection and Handling
System Logs:

journalctl: Access logs managed by systemd.
 
 
 
journalctl -xe
/var/log/: Directory containing various system and application logs.
Monitoring Tools:

top/htop: Monitor CPU, memory, and process-level errors.
iotop: Monitor disk I/O for bottlenecks or errors.
netstat/ss: Detect and diagnose network issues.
Debugging Tools:

gdb: Debug application crashes.
valgrind: Detect memory leaks and errors.
strace: Trace system calls and signals.
lsof: List open files to diagnose file-related errors.
File System Tools:

fsck: Check and repair file systems.
e2fsck: Specific to ext file systems.
Hardware Diagnostic Tools:

smartctl: S.M.A.R.T. diagnostics for disks.
memtest86+: Tests for memory errors.
4. Common Scenarios and Examples
Example 1: Disk Errors
Detect:
 
 
 
dmesg | grep -i error
Handle:
 
 
 
fsck /dev/sda1
Example 2: Application Crash
Debug with gdb:
 
 
 
gdb ./myprogram core
Example 3: Network Errors
Check connection:
 
 
 
ping -c 4 google.com
Analyze traffic:
 
 
 
tcpdump -i eth0
5. Best Practices for Error Handling in Linux
Proactive Monitoring:

Use monitoring tools like Nagios, Zabbix, or Prometheus for real-time alerts.
Centralized Logging:

Centralize logs using tools like ELK Stack (Elasticsearch, Logstash, Kibana).
Error Mitigation:

Use redundancy (RAID, load balancers).
Automate backups and recovery plans.
Secure Configurations:

Validate configurations with tools like ansible-lint.
Regular Updates:

Keep the kernel and software up to date for patches and fixes.



		
		
		Information and Resource Protection
		---------------------------
		Information and Resource Protection in Linux OS
Linux is renowned for its robust security mechanisms and resource protection. It achieves this through a combination of user privileges, resource control, process isolation, and encryption. Below is a detailed explanation of how Linux manages and protects information and resources.

1. File System Permissions
User, Group, and Others
Linux uses a file permission model that controls access to files and directories:
User (u): Owner of the file.
Group (g): Users who share access via a group.
Others (o): All other users.
Permission Types
Read (r): View the contents of a file or list a directory.
Write (w): Modify a file or create/delete files in a directory.
Execute (x): Run a file as a program or enter a directory.
Viewing and Changing Permissions
View permissions using ls -l:
 
 
 
	ls -l
Change permissions with chmod:
 
 
 
	chmod u+rwx,g+rx,o-r file.txt
Change ownership with chown:
 
 
 
	chown user:group file.txt
2. Access Control Lists (ACLs)
ACLs provide fine-grained permissions beyond the standard model.
Example commands:
Set ACL for a user:
 
 
 
	setfacl -m u:username:rwx file.txt
View ACLs:
 
 
 
	getfacl file.txt
3. User and Group Management
User Authentication
Password Authentication: Managed through /etc/passwd and /etc/shadow.
Passwords are stored as hashed values for security.
Group Management
Groups manage collective permissions.
Add users to groups:
 
 
 
	usermod -aG groupname username
4. Process Isolation
User Space and Kernel Space
Linux separates processes into user space (applications) and kernel space (system-level operations) to ensure isolation.
Process Permissions
Processes run with the permissions of the user who started them.
View process details:
 
 
 
	ps aux
Security with setuid and setgid
Some binaries can execute with elevated privileges using the setuid and setgid bits.
Example:
 
 
 
chmod u+s /path/to/binary
5. Resource Management
Quota Management
Disk quotas limit the amount of disk space or the number of files a user can use.
Set quotas with edquota:
 
 
 
edquota -u username
Check quota usage:
 
 
 
quota username
Control Groups (cgroups)
Manage CPU, memory, and I/O resources for processes.
Example:
Create a cgroup:
 
 
 
cgcreate -g memory:/test_group
Set memory limits:
 
 
 
echo 100M > /sys/fs/cgroup/memory/test_group/memory.limit_in_bytes
6. Encryption
File Encryption
Use gpg for file encryption:
 
 
 
gpg -c file.txt
Full Disk Encryption
Use tools like LUKS and dm-crypt to encrypt disks.
7. Network Security
Firewall
Use iptables or firewalld to configure network rules.
 
 
 
iptables -A INPUT -p tcp --dport 22 -j ACCEPT
SELinux (Security-Enhanced Linux)
Enforces mandatory access controls.
View SELinux status:
 
 
 
sestatus

8. Logging and Monitoring
	System Logs
	Logs are stored in /var/log.
	Use journalctl for viewing logs:
 
 
 
	journalctl -xe
	Intrusion Detection
	Use tools like fail2ban or auditd for monitoring suspicious activity.

9. Backup and Recovery
	Backup Tools
	Common tools: rsync, tar, duplicity.
	Automated Backups
	Schedule backups with cron or systemd timers.
10. Security Best Practices
	Keep the kernel and software updated.
	Use strong passwords and authentication methods.
	Enable firewalls and configure SELinux or AppArmor.
	Encrypt sensitive data and secure communication channels.
	Monitor logs regularly and set up alerting systems.

		
		
		
	Popular Linux Distributions
	
		---------------------------
		A Linux distribution (or distro) combines 
		---------------------------
		the Linux kernel with various software packages to create a complete operating system. Different distributions 
			cater to different use cases, 
			from general-purpose computing to 
			specialized tasks like server management 
			or penetration testing
		---------------------------.

		4.1. General-Purpose Distributions:

			Ubuntu: 
				User-friendly and 
				beginner-friendly, 
				widely used for desktops and servers.
			Fedora: 
				A cutting-edge distro 
				focused on innovation and 
				open-source principles.
			Debian: 
				Known for its 
					stability and 
					extensive software repository.
			Linux Mint: 
				A popular choice for desktop users 
					due to its ease of use and 
					elegant interface.

		4.2. Server and Enterprise Distributions:

			Red Hat Enterprise Linux (RHEL): 
				Widely used in enterprise environments, with commercial support.
			CentOS: 
				A free, community-supported version of RHEL.
			SUSE Linux Enterprise Server (SLES): 
				Focused on enterprise-grade stability and support.

		4.3. Specialized Distributions:

			Kali Linux: 
				Designed for 
					penetration testing and 
					security auditing.
			Arch Linux: 
				A minimalist distro for advanced users who 
					prefer manual configuration.
			Raspberry Pi OS: 
				Optimized for Raspberry Pi devices.

	
		
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Linux architecture
	https://tecadmin.net/tutorial/linux/linux-architecture/
	
	OS
		Interface between user and  computer hardware. 
		Other software applications run on operating system 
			manage hardware resources on a computer.

	The Linux system basically works on 4 layers. 
		Image in 
		https://tecadmin.net/tutorial/linux/linux-architecture/
			Hardware
				Physical devices attached to the System. 
				For e.g.: 
					Hard disk drive, 
					RAM, 
					Motherboard, 
					CPU etc.
			Kernel
				Kernel is the core component for any (Linux) operating system 
				Directly interacts with the hardware.
			Shell
				Interface 
					takes input 
						from Users 
						sends instructions to the Kernel
					takes the output 
						from Kernel  
						send the result back to output shell.
			Applications
				utility programs which runs on Shell. 
				All application like 
					web browser, 
					media player, 
					text  or etc.
			User
				We users

		Linux
		-----
		Linux is an open-source OS 
		Can be installed on a variety of different types of hardware 
		At the heart of Linux is the kernel. 
		Linux was developed in 
			C and 
			assembly language 
			to run on i386 personal computers
		ported to more hardware than just about any other operating system in history. 
		
		Typically administered from a command line interface (CLI)
			also known as a shell. 
		Besides the kernel, Linux distributions include a 
			collection of Linux software, such as 
				device drivers for accessing and controlling hardware, 
				shared libraries, 
				applications and 
				system daemons, 
					which run the in background and respond to network requests. 
		
				
		The kernel 
		----------
		Special piece of the operating system 
			controls the CPU hardware, 
			allocates memory, 
			accesses data, 
			schedules processes, 
			runs the applications and 
			protects them from each other. 
		First program loaded on the computer when the computer starts up. 
		Most critical pieces of code in the kernel are loaded into protected areas of memory 
			so that they can’t be overwritten 
		
		Understanding user space vs. kernel space
		-----------------------------------------
		Operating systems all execute their kernel in 
			protected and restricted memory 
				called kernel space 
				prevent the kernel from terminating and crashing the system. 
		An application or tool executes in user space. 
		Applications can come from a variety of sources, 
			may be poorly developed or originate unknown sources. 
		By running these applications separate from kernel space, 
			they can’t tamper with the kernel resources and cause the system to panic (crash).

		
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Kernel, Shell & the Software Tools & Applications
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	
THE LINUX FILE SYSTEM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Concept of File System

		In Linux/UNIX-like operating systems 
			“everything is a file”. 
		Files represents 
			network device, 
			disk, 
			hardware devices etc. 
		These are known as special files.

		File Types in Linux:
		There are multiple types of files. 
			The first character in file permissions under ls -l commands shows the type of file. 
				(-)
					Regular file 	 
						Text files, 
						image files, 
						executable files
				(d)
					Directory file 	 
					Simple directory or folder contained files
				Special files
				(b)
					Block file 
				(c)
					Character device file 
				(p)
					Named pipe file 
				(l)
					Symbolic link file 
				(s)
					Socket file


continue from here jan 24th					
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Different Types of Files in the Linux File System
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	“In Linux, everything is a File”.
		ls -ltr: First character there can be
			"-": Normal file or 
				Hard link: Additional name for existing file.
			"d": Normal directory
			"l": symbolic link. Shotcut to a file or directory
			"s": socket. Used to pass 
			"p": Named pipe. Users can't directly work. Additional details to follow
			"c": Character device. Processes character hw communication.
			"b": Block device. Major and monor numbers for controlling dev.
			
	In Unix/Linux
		everything is considered as a file except
			running processes

	Linux system does not differentiate between 
		files and 
		directories
	Directories store other files 
	
	All your hardware components are represented as files 
	System communicates with them using these files.

	Input/output resources such as 
		documents, 
		directories (folders in Mac OS X and Windows), 
		keyboard, 
		monitor, 
		hard-drives, 
		removable media, 
		printers, 
		modems, 
		virtual terminals 
		inter-process communication 
		network communication 
			are streams of bytes defined in files.

	In Linux there are basically three types of files:
		Ordinary/Regular files
			Files data contain text, data or program instructions 
			common type of files on Linux:
			e.g.
				Readable files
				Binary files
				Image files
				Compressed files etc.
		Special files
			Block files : 
				device files that provide buffered access to system hardware components. 
				Provide a method of communication with device drivers through the file system.

				Can transfer a large block of data and information at a given time.
				
				ls -l /dev | grep "^b"

			Character files : 
				Type of device files 
				Provide unbuffered serial access to system hardware components. 
				Provide a way of communication with devices by transferring data one character at a time.

				Listing character files sockets in a directory:

				ls -l /dev | grep "^c"

				Listing block files sockets in a directory
				
				ls -l /dev | grep "^c"

	
	
			Character files : 
				Type of device files 

				provide unbuffered serial access to system hardware components.
				Provides a way of communication with devices by transferring data one character at a time.

				Listing character files sockets in a directory:

				# ls -l /dev | grep "^c"

			Symbolic link files : 
				Reference to another file on the system. 
					can be file or
					directory
				Listing symbolic link sockets in a directory:

				# ls -l /dev/ | grep "^l"
			
				N.B: 
				Symbolic links can be created using ln utility in Linux.
					touch file1.txt
					ln -s file1.txt /home/vilas/file1.txt 
					ls -l /home/vilas/ | grep "^l"

			Pipes or Named pipes : 
				Files that allow inter-process communication 
					connects the output of one process to the input of another.

				A named pipe is a file that is used by two process to communicate with each and it acts as a Linux pipe.

				Listing pipes sockets in a directory:
				
				ls -l | grep "^p"
		
				# mkfifo pipe1
				# echo "This is named pipe1" > pipe1
		
				Above we created a named pipe called pipe1, 
				then we passed some data to it using the echo command, 
				now the shell became un-interactive while processing the input.

				Then we opened another shell and ran the following command.
				
				while read line ;do echo "This was passed-'$line' "; done<pipe1
		
			Socket files : 
				Files that provide a means of inter-process communication
				Can transfer data and information between process running on different environments.

				Sockets provide data and information transfer between process running on different machines on a network.

				For e.g 
					a web browser making a connection to a web server.

				# ls -l /dev/ | grep "^s"
		
				In C
				int socket_desc= socket(AF_INET, SOCK_STREAM, 0 );
					AF_INET is the address family(IPv4)
					SOCK_STREAM is the type (connection is TCP protocol oriented)
					0 is the protocol(IP Protocol)
				To refer to the socket file, use the socket_desc, which is the same as the file descriptor, and use read() and write() system calls to read and write from the socket respectively.

								
		Directories
			Special files 
			store both ordinary and other special files 
			organized on the Linux file system in a hierarchy starting from the root (/) directory.

				Listing sockets in a directory:

				# ls -l / | grep "^d"
			
			
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Hierarchy of the File System
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Linux File Hierarchy Structure or the Filesystem Hierarchy Standard (FHS) 
	-------------------------------------------------------------------------
		Defines the directory structure and directory contents in Unix-like operating systems.
		Maintained by the Linux Foundation.

		All files and directories appear under the root directory /
			even if they are stored on different physical or virtual devices.
		Some of these directories only exist on a particular system 
			if certain subsystemse. e.g. X Window System, are installed.
		Most of these directories exist in all UNIX operating systems 
		Generally used in all linux/unix in a similar way
		
		1. / (Root) : 
			Primary hierarchy root 
			Root directory of the entire file system hierarchy.
			Every single file and directory starts from the root directory
			By default only root user has the right to write under this directory
			/root is root user’s home directory
				not same as /


		2. /bin : 
			Essential command binaries  
				for all users, e.g., cat, ls, cp.
			Contains binary executables
			Common linux commands used 
				in single-user modes are located under this directory.
					e.g. cp, mv etc.
				by all the users of the system are located here 
					e.g. ps, ls, ping, grep, cp



		3. /boot : 
			Boot loader files, 
			Kernel initrd, vmlinux, grub files are located under /boot
			Example: initrd.img-2.6.32-24-generic, vmlinuz-2.6.32-24-generic

		4. /dev : Essential device files, e.g., /dev/null.
			These include terminal devices, usb, or any device attached to the system.
			Example: /dev/tty1, /dev/usbmon0
			
			What is /dev/null and How to Use It
			-----------------------------------
				/dev/null. 
				It’s a special file that’s present in every Linux system. 
				Unlike most other virtual files, 
					instead of reading, 
					it’s used to write. 
				Whatever you write to /dev/null will be 
					discarded, 
					forgotten into the void. 
				It’s known as the null device in a UNIX system.

				Whenever any command-line utility is run, 
					it generates two outputs. 
					The output goes to stdout 
					error (if generated) goes to stderr. 
				By default, both these data streams are associated with the terminal.

				e.g.
					echo "Hello World" - goes to stdout
					run invalid command
						a;skdj;jo
							errors generated would go to stderror
							
				
				File descriptor: 
					In the UNIX ecosystem, File descriptor are integer values assigned to a file. 
						stdout (file descriptor = 1) and 
						stderr (file descriptor = 2) 
						Using the file descriptor (1 and 2 in this situation), 
							we can redirect the stdout and stderr to other files.

					a;skdj;jo 2> error.txt
						error messages should have gone to stderror.
						stderror will get redirected to error.txt
				Returning back to /dev/null	
					grep -r a /sys/
						Execute above command as a normal user
						This would generate lot of errors (Permission denied error)
					
				grep -r a /sys/ 2> /dev/null
					This way errors will be redirected to /dev/null
					grep result will only include stdout. 
					It will not include errors

			If entire output is useless and you want to dump it
			
			grep -r a /sys/ > /dev/null 2>&1

				grep -r a /sys/ > /dev/null
					This will dump output (stdout) to /dev/null
				2>&1
					This will combine error along with stdout and redirect to /dev/null.
					
				Hence both would be ignored.
				
		5. /etc : 
			Host-specific system-wide configuration files.

			Contains configuration files required by all programs.
			This also contains 
				startup and shutdown shell scripts 
					used to start/stop individual programs.
			e.g.:
				/etc/resolv.conf, 
				/etc/logrotate.conf.

		6. /home : 
			Users’ home directories
			containing 
				saved files, 
				personal settings, etc.

				Home directories for all users to store their personal files.
				e.g.: /home/vilas
				
		7. /lib : 
			Libraries essential for the binaries in 
				/bin/ and 
				/sbin/.

				Library filenames are either ld* or lib*.so.*
				Example: ld-2.11.1.so, libncurses.so.5.7
		
		8. /media : Mount points for removable media such as CD-ROMs (appeared in FHS-2.3).

Temporary mount directory for removable devices.
Examples, /media/cdrom for CD-ROM; /media/floppy for floppy drives; /media/cdrecorder for CD writer


9. /mnt : Temporarily mounted filesystems.

Temporary mount directory where sysadmins can mount filesystems.


	10. /opt : Optional application software packages.
	
Contains add-on applications from individual vendors.
Add-on applications should be installed under either /opt/ or /opt/ sub-directory.



11. /sbin : Essential system binaries, e.g., fsck, init, route.

Just like /bin, /sbin also contains binary executables.
The linux commands located under this directory are used typically by system aministrator, for system maintenance purpose.
Example: iptables, reboot, fdisk, ifconfig, swapon


12. /srv : Site-specific data served by this system, such as data and scripts for web servers, data offered by FTP servers, and repositories for version control systems.

srv stands for service.
Contains server specific services related data.
Example, /srv/cvs contains CVS related data.

13. /tmp : Temporary files. Often not preserved between system reboots, and may be severely size restricted.

Directory that contains temporary files created by system and users.
Files under this directory are deleted when system is rebooted.


14. /usr : Secondary hierarchy for read-only user data; contains the majority of (multi-)user utilities and applications.

Contains binaries, libraries, documentation, and source-code for second level programs.
/usr/bin 
	contains binary files for user programs. 
		If you can’t find a user binary under /bin, look under /usr/bin. 
			For example: at, awk, cc, less, scp
/usr/sbin 
	contains binary files for system administrators. 
		If you can’t find a system binary under /sbin, look under /usr/sbin. For example: atd, cron, sshd, useradd, userdel
/usr/lib 
	contains libraries for /usr/bin and /usr/sbin
/usr/local 
	contains users programs that you install from source. For example, when you install apache from source, it goes under /usr/local/apache2
/usr/src holds the Linux kernel sources, header-files and documentation.


15. /proc : Virtual filesystem providing process and kernel information as files. In Linux, corresponds to a procfs mount. Generally automatically generated and populated by the system, on the fly.

Contains information about system process.
This is a pseudo filesystem contains information about running process. For example: /proc/{pid} directory contains information about the process with that particular pid.
This is a virtual filesystem with text information about system resources. For example: /proc/uptime


Modern Linux distributions include a /run directory as a temporary filesystem (tmpfs) which stores volatile runtime data, following the FHS version 3.0. According to the FHS version 2.3, such data were stored in /var/run but this was a problem in some cases because this directory is not always available at early boot. As a result, these programs have had to resort to trickery, such as using /dev/.udev, /dev/.mdadm, /dev/.systemd or /dev/.mount directories, even though the device directory isn’t intended for such data.Among other advantages, this makes the system easier to use normally with the root filesystem mounted read-only. For example, below are the changes Debian made in its 2013 Wheezy release:

	/dev/.* ? /run/*
	/dev/shm ? /run/shm
	/dev/shm/* ? /run/*
	/etc/* (writeable files) ? /run/*
	/lib/init/rw ? /run
	/var/lock ? /run/lock
	/var/run ? /run
	/tmp ? /run/tmp


	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Significance of the root & the Other Directories.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	The root folder
		called the root directory / root
			of any partition or folder 
			"highest" directory in the hierarchy. 
		You can also think of it in general as 
			start or beginning of a particular folder structure.


	The root directory contains all other folders in the drive or folder, and can, of course, also contain files. You can visualize this with an upside-down tree where the roots (the root folder) are at the top and the branches (subfolders) fall below; the root is what holds together all of its lower items.

	

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Logging into LINUX system
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Prerequisites
Following are the prerequisites

	Remote computer must be 
		turned on 
		must have a network connection.
	SSH client and server applications need to be installed and enabled.

--------------------------------------------------------------
Install openss-server
Open ubuntu vm 
execute the following command 
- sudo su
- apt update -y
- apt install openssh-server -y
- ip a 
- Note down the ip address from etho0 section
Shutdown the ubuntu 
From powershell - Start-VM -Name "ubuntu" -Passthru
From gitbash - ssh user@<ip address noted above"
enter password and yes to loging.
--------------------------------------------------------------	
	You should have the 
			IP address or 
			the name of the 
		remote machine you want to connect to.
		
	You need to have the necessary permissions to access the remote computer.
	Firewall settings need to allow the remote connection
	
	
	What is SSH?
	------------
	Secure Shell / Secure Socket Shell
		A protocol which allows you to connect securely to a remote computer or a server by using a text-based interface.

		When a secure SSH connection is established, 
			a shell session will be started
			we will be able to manipulate the server by typing commands within the client on your local computer.

	How Does SSH Work?
	-------------------
	To establish an SSH connection
	we need two components: 
		a client and 
		corresponding server-side component. 
	An SSH client 
		an application you install on the computer 
		we use it to connect to another computer or a server. 
	The client uses the provided remote host information to initiate the connection 
		if the credentials are verified, establishes the encrypted connection.

	On the server’s side, there is a component called an SSH daemon 
		that is constantly listening to a specific TCP/IP port (default:22) for possible client connection requests. 
	Once a client initiates a connection
		SSH daemon will respond with the software and the protocol versions it supports 
	Two will exchange their identification data. 
	If the provided credentials are correct, 
		SSH creates a new session for the appropriate environment.

	The default SSH protocol version for SSH server and SSH client communication is version 2.





	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	10. /opt : Optional application software packages.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Linux Boot Process – various stages during booting
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
	
	
	1. BIOS/UEFI (Basic Input/Output System / Unified Extensible Firmware Interface)
Function:
	The BIOS/UEFI is the firmware that initializes hardware components when the system is powered on.
Process:
	POST (Power-On Self Test): 
		Verifies hardware functionality (e.g., CPU, memory, disk drives).
	Identifies bootable devices using the boot order 
		(e.g., hard disk, CD-ROM, USB, or network).
	Loads the first sector of the bootable device 
		(the Master Boot Record (MBR) 
		or 
		GPT (GUID Partition Table)).
	Transfers control to the bootloader.
2. Bootloader
	Function:
	The bootloader 	
		responsible for loading the kernel into memory and 
		initializing the operating system.

	Examples:
		GRUB (GRand Unified Bootloader) is the most commonly used bootloader in Linux.
		Alternatives: LILO, systemd-boot.
	Process:
	Stage 1: Bootloader Initialization

		The bootloader is loaded by the BIOS/UEFI.
		Reads the configuration file (e.g., /boot/grub/grub.cfg).
		Presents a menu to select an operating system or kernel version.
	Stage 2: Kernel Loading

		Loads the selected kernel image 
			(e.g., vmlinuz) into memory.
		Loads the initial RAM disk (initramfs or initrd), 
			which contains essential drivers for booting.
3. Kernel Initialization
	Function:
	The kernel is the core of the operating system that manages hardware resources and system processes.

	Process:
	Hardware Detection:
		The kernel initializes hardware devices 
			(e.g., disk controllers, network interfaces).
		Mounts the initial RAM disk (initramfs) as the temporary root filesystem.
		Device Discovery:
			Uses udev (device manager) to detect and configure hardware dynamically.
			Filesystem Mounting:
				Searches for the real root filesystem (e.g., /).
				Mounts the root filesystem from disk.
4. init System or Systemd
	Function:
		The init system (or systemd, 
			commonly used in modern Linux distributions) 
			initializes the userspace environment and manages system processes.

	Process:
	initrd Handoff:

		Once the kernel is ready, control is handed off to the init system.
		The root filesystem from initramfs is replaced with the actual root filesystem on disk.
	System Initialization:

		Reads configuration files (/etc/inittab for init, or systemd unit files for systemd).
		Starts essential services (e.g., udev, network, and logging).
		Mounts additional filesystems (e.g., /home, /var).
Runlevel/Target Selection:

	For traditional init: Executes startup scripts (e.g., /etc/rc.d/rc*.d) based on the default runlevel.
	For systemd: Executes unit files based on the default target (e.g., graphical.target for GUI or multi-user.target for CLI).
5. Services and Daemons
	Function:
	Starts background services and daemons essential for system operation.

	Process:
	Examples of services:
	Network services (e.g., NetworkManager or systemd-networkd).
	SSH daemon (sshd).
	Display manager (e.g., GDM, LightDM) for GUI login.
	Logs are written to system logs (e.g., journalctl for systemd or /var/log/messages).
6. User Space and Login Prompt
	Function:
	Provides the user with an interface to interact with the system.

	Process:
	If the system is in CLI mode:
	Presents a text-based login prompt (via getty or systemd-logind).
	If the system is in GUI mode:
	Launches the display manager (e.g., GDM, LightDM) to present a graphical login screen.

Summary of Stages
-------------------------------------------------------------------------------
Stage					Component			Key Actions
-------------------------------------------------------------------------------
1. BIOS/UEFI			Firmware			Hardware initialization, boot device selection.
2. Bootloader			GRUB/LILO			Kernel loading and initial RAM disk setup.
3. Kernel Initialization	Linux Kernel	Hardware detection, root filesystem mounting.
4. init/Systemd			init/systemd		System initialization, service management.
5. Services and 		Daemons	Services	Start background processes, log services.
6. User Space			Login Prompt		Provides a terminal or GUI for user interaction.
-------------------------------------------------------------------------------

Commands and Files to Explore the Boot Process

GRUB Configuration:
	File: /boot/grub/grub.cfg
	Command: sudo update-grub
Kernel Logs:
	Command: dmesg
Systemd Boot Logs:
	Command: journalctl -b
Runlevel Information:
	Command: runlevel
Startup Services:
	Command: systemctl list-units --type=service
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Various run-levels using init command
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	Runlevels Overview
Runlevel		Description
0				System halt (shutdown).
1				Single-user mode for administrative tasks (no networking, minimal services).
2				Multi-user mode without networking (varies by distribution, rarely used).
3				Full multi-user mode with networking (CLI mode, no graphical interface).
4				Undefined/custom runlevel (can be used for specific tasks or custom configurations).
5				Full multi-user mode with networking and a graphical user interface (GUI).
6				Reboot the system.
Runlevel Descriptions
Runlevel 0 (Shutdown Mode):

Used to safely shut down the system.
Halts all processes and powers off the machine (if supported by the hardware).
Command: init 0
Runlevel 1 (Single-User Mode):

Used for system maintenance and troubleshooting.
Only the root user has access; no network or multi-user capabilities.
Command: init 1
Runlevel 2 (Multi-User Mode Without Networking):

Multi-user mode without starting network services.
Rarely used in modern systems.
Command: init 2
Runlevel 3 (Multi-User Mode with Networking):

A full CLI-based mode with networking enabled.
Suitable for servers or systems without a graphical environment.
Command: init 3
Runlevel 4 (Custom Mode):

Reserved for custom configurations.
Not defined by default, allowing admins to set specific tasks.
Command: init 4
Runlevel 5 (Multi-User Mode with GUI):

Same as runlevel 3 but with a graphical user interface (e.g., GNOME, KDE).
Commonly used for desktop environments.
Command: init 5
Runlevel 6 (Reboot Mode):

Reboots the system safely.
All processes are stopped, and the system restarts.
Command: init 6
Checking and Changing Runlevels
1. Viewing Current Runlevel
Command: runlevel
Example output:
mathematica
Copy
Edit
N 5
N: The previous runlevel (if none, N is shown).
5: The current runlevel.
2. Changing Runlevel
Command: init <runlevel>
Example:
To switch to runlevel 3 (CLI mode): sudo init 3
To reboot the system: sudo init 6
3. Default Runlevel
The default runlevel is specified in /etc/inittab.
Example /etc/inittab entry for default runlevel 5:
bash
Copy
Edit
id:5:initdefault:
To change the default runlevel:
Open the file: sudo nano /etc/inittab
Modify the id entry to the desired runlevel.
Runlevels in Modern Systems (systemd)
Modern Linux distributions using systemd replace runlevels with targets. For example:

runlevel 3 (CLI mode) is equivalent to multi-user.target.
runlevel 5 (GUI mode) is equivalent to graphical.target.
Commands with systemd:

Check the current target: systemctl get-default
Change the target:
CLI: sudo systemctl isolate multi-user.target
GUI: sudo systemctl isolate graphical.target
Set the default target:
CLI: sudo systemctl set-default multi-user.target
GUI: sudo systemctl set-default graphical.target

	
=======

The Linux boot process involves a series of stages to bring the system up and running. Here's a breakdown of the key steps:   

1. Power-On Self-Test (POST)

	Initiated by BIOS/UEFI: When the computer is powered on, the BIOS (Basic Input/Output System) or UEFI (Unified Extensible Firmware Interface) takes control.   
	Hardware Checks: POST performs a series of tests on the system's hardware components (CPU, RAM, hard drive, etc.) to ensure they are functioning correctly.   
	Boot Device Selection: Based on the boot order in the BIOS/UEFI settings, it determines the boot device (usually the hard drive).
2. Boot Loader

	Loading the Boot Loader: The BIOS/UEFI loads the boot loader from the boot device (typically located in the Master Boot Record (MBR) or the GUID Partition Table (GPT)). Common boot loaders include GRUB (Grand Unified Bootloader) and LILO (Linux Loader).   
	Boot Loader Functions:
	Kernel Loading: The boot loader locates the Linux kernel image on the hard drive and loads it into memory.   
	Kernel Parameters: It may allow the user to pass kernel parameters (e.g., boot options, kernel debugging flags) during the boot process.   
	Kernel Transfer: The boot loader transfers control to the loaded Linux kernel.
3. Kernel Initialization

	Kernel Startup: The Linux kernel begins its initialization process.   
	Hardware Initialization: The kernel initializes hardware devices (e.g., CPU, memory, disk drives, network interfaces) and loads necessary drivers.   
	Filesystem Initialization: The kernel mounts the root filesystem, which contains the core system files and user data.   
4. Init Process

	init Process: The kernel starts the init process, which is the first process to run in the Linux system.
	Runlevel Management: init is responsible for setting the system's runlevel, which determines the services and processes that are started.
	Runlevels: Different runlevels define different system states (e.g., single-user mode, multi-user mode, graphical mode).   
5. System Startup

	Service Startup: init or its successor (like systemd) starts system services (daemons) such as network services (SSH, DHCP), file systems, and graphical display managers.
	User Login: Once the system is ready, users can log in to their accounts.
6. User Interface

	Graphical User Interface (GUI): If a graphical desktop environment is installed, the display manager starts and presents the user with a login screen.
	Command-Line Interface (CLI): In text-based environments, users can log in directly to a command-line prompt.   

Key Points:

	The boot process involves a complex interplay between hardware and software components.   
	The specific steps and their order may vary slightly depending on the Linux distribution and system configuration.
	The boot loader plays a crucial role in the booting process, selecting the kernel and providing initial parameters.



	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Various run-levels using init command
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The init command and its associated runlevels are a core concept in older Linux distributions. However, with the rise of systemd, the traditional runlevels have become less prominent.

Here's a breakdown of the common runlevels associated with the init command:

Runlevel 0: Halt - Shuts down the system completely.
Runlevel 1: Single-user Mode - A maintenance mode with limited functionality. Networking is typically disabled, and only the root user can log in.
Runlevel 2: Multi-user Mode - Similar to Runlevel 3, but with networking disabled.
Runlevel 3: Multi-user Mode with Networking - The standard runlevel for most systems. Allows multiple users to log in and provides full network connectivity.
Runlevel 4: User-definable - Can be customized for specific system needs.
Runlevel 5: Graphical Mode - Starts the X Window System for a graphical user interface.
Runlevel 6: Reboot - Restarts the system.
Important Notes:

Systemd: In modern Linux distributions, systemd has largely replaced init as the process manager.
Target States: systemd uses "target states" instead of traditional runlevels. These target states (e.g., multi-user.target, graphical.target) provide a more flexible and descriptive way to define system states.
Runlevel 3 is often the default runlevel in older systems.
While the concept of runlevels is still relevant, it's important to understand that the implementation and usage may vary significantly between different Linux distributions.

>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	• Starting the system in Single User mode
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
=======


GRUB and systemd-boot: These are common boot loaders used in Linux systems.
Edit Boot Options: You need to access the boot options during the startup process.
Append "single": Adding the word "single" to the kernel command line instructs the system to boot into single-user mode.
Save and Boot: After modifying the boot options, you need to save the changes and proceed with the boot process.
Note:

The exact steps and key combinations might vary slightly depending on the specific bootloader and Linux distribution.
Booting into single-user mode provides access to the system with root privileges for maintenance and troubleshooting.
It's crucial to proceed with caution in single-user mode as changes made in this mode can have significant system-wide effects.
I hope this helps! Let me know if you have any other questions.
1. During Boot Process:

GRUB (Grand Unified Boot Loader):

If you're using GRUB, at the GRUB boot prompt, press the "e" key to edit the boot options.
Locate the line starting with "linux" or "linuxefi".
Append the word "single" to the end of that line.
Press Ctrl+x to save the changes and boot into single-user mode.
systemd-boot:

If you're using systemd-boot, press the "e" key at the systemd-boot prompt.
Locate the line starting with "linux" or "linuxefi".
Append "single" to the end of that line.
Press Ctrl+x to save the changes and boot into single-user mode.
2. After Boot (if you have access to a console):

If you can already access a running system, you can often reboot into single-user mode by pressing a specific key during the boot process (usually Ctrl+Alt+Del or Ctrl+Alt+F1). The exact key combination varies depending on the system and its configuration.
Key Points:

Single-user mode provides a way to access and repair the system without the usual graphical interface or network services.
It's often used for system administration tasks, troubleshooting, and recovering from system failures.
Be cautious when working in single-user mode, as you have direct access to the system and can potentially cause damage if not done carefully.

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	
	1. During the Boot Process (GRUB)

Interrupt the Boot Process:

At the GRUB boot menu (usually displayed briefly when the system starts), press the "e" key to edit the boot options.
Modify the Kernel Line:

	Locate the line starting with "linux" or "linuxefi".
	Add the word "single" to the end of that line.
	For example, if the line looks like this:
	linux /boot/vmlinuz-6.0.0-rc8-generic root=UUID=... ro quiet splash 
Change it to:
	linux /boot/vmlinuz-6.0.0-rc8-generic root=UUID=... ro quiet splash single
	Save and Boot: Press Ctrl+x to save the changes and boot the system with the modified kernel line.

2. During the Boot Process (systemd-boot)

Interrupt the Boot Process:
	At the systemd-boot prompt, press the "e" key to edit the boot options.
	Locate the line starting with "linux" or "linuxefi".
	Add "single" to the end of that line.
	Press Ctrl+x to save the changes and boot the system with the modified kernel line.
Key Points:

	Single-user Mode: This mode provides a command-line environment with root privileges.
	Limited Functionality: Networking and graphical interfaces are typically disabled in single-user mode.
	Troubleshooting: It's often used for system administration tasks, troubleshooting, and recovering from system failures.
	Caution: Be careful when working in single-user mode, as you have direct access to the system and can potentially cause damage if not done carefully.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Accessing help using man, info, what is and -- help.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
	
	1. man (Manual Pages)

	Purpose: 
		Provides comprehensive, detailed information about 
			commands, 
			system calls, 
			libraries, and 
			other system components.
	Usage: man <command_name>
	Example: 
		man ls (displays the manual page for the ls command)
	Key Features:
		Detailed descriptions of command usage, options, and examples.
		Often includes a SYNOPSIS section summarizing the command's syntax.
		Can be lengthy and may require navigation within the manual page.
2. info

	Purpose: An alternative to man for accessing online documentation.
	Usage: info <command_name>
	Example: info ls
	Key Features:
		Uses a hypertext-like format for easier navigation.
		Often provides more extensive documentation than man for some commands.
		May require the installation of the info package.
3. whatis

	Purpose: Provides a short, one-line description of a command or function.
	Usage: whatis <command_name>
	Example: whatis ls (displays a brief description of the ls command)
4. --help (Built-in Help)

	Purpose: Many commands have a built-in --help option that provides a concise summary of the command's usage and available options.
	Usage: <command_name> --help
	Example: ls --help
In Summary:

	man: The most comprehensive source for detailed information about commands.
	info: An alternative to man with hypertext-like navigation.
	whatis: Provides a quick overview of a command.
	--help: Offers a brief, built-in summary of a command's usage.
=======

1. man (Manual Pages)

Purpose: Provides comprehensive, detailed information about commands, system calls, libraries, and other system components.
Usage: man <command_name>
Example: man ls (displays the manual page for the ls command)
Key Features:
Detailed descriptions of command usage, options, and examples.
Often includes a SYNOPSIS section summarizing the command's syntax.
Can be lengthy and may require navigation within the manual page.
2. info

Purpose: An alternative to man for accessing online documentation.
Usage: info <command_name>
Example: info ls
Key Features:
Uses a hypertext-like format for easier navigation.
Often provides more extensive documentation than man for some commands.
May require the installation of the info package.
3. whatis

Purpose: Provides a short, one-line description of a command or function.
Usage: whatis <command_name>
Example: whatis ls (displays a brief description of the ls command)
4. --help (Built-in Help)

Purpose: Many commands have a built-in --help option that provides a concise summary of the command's usage and available options.
Usage: <command_name> --help
Example: ls --help
In Summary:

man: The most comprehensive source for detailed information about commands.
info: An alternative to man with hypertext-like navigation.
whatis: Provides a quick overview of a command.
--help: Offers a brief, built-in summary of a command's usage.

>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	• Basic commands: pwd, date, clear, ls, cal, who
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		ls folder
		pwd 
		
		ls: Lists files and directories in the current directory.
		ls -l: Lists files with detailed information (permissions, owner, size, etc.).
		ls -a: Lists all files and directories, including hidden files (starting with a dot).
		ls -lh: Lists files with human-readable sizes (e.g., 1.5M instead of 1536000).

		who -H - Show header in 'who' command output
    
<<<<<<< HEAD
	
	
	date: Displays the current date and time.
	date +%Y-%m-%d: Displays the date in YYYY-MM-DD format (e.g., 2024-07-05).
	date +%H:%M:%S: Displays the time in HH:MM:SS format (e.g., 15:30:00).
	date "+%Y-%m-%d %H:%M:%S": Displays both date and time in a combined format.
	date -u: Displays the date and time in UTC (Coordinated Universal Time).
	sudo date --set="YYYY-MM-DD HH:MM:SS": Sets the system date and time (requires root privileges).

clear

clear: Clears the terminal screen.

reset: Clears the screen and resets terminal settings (colors, attributes, etc.).

ls (List Files)

	ls: Lists files and directories in the current directory.
	ls -l: Lists files with detailed information (permissions, owner, size, etc.).
	ls -a: Lists all files and directories, including hidden files (those starting with a dot).
	ls -h: Displays file sizes in a human-readable format (e.g., 1K, 2M, 1G).
	ls -R: Recursively lists files and directories in the current directory and all subdirectories.
	ls -t: Sorts files by modification time (newest first).
	ls -r: Reverses the order of the listing.

cal (Calendar)
=======
1. date

date: Displays the current date and time.
date +%Y-%m-%d: Displays the date in YYYY-MM-DD format.
date +%H:%M:%S: Displays the time in HH:MM:SS format.
date "+%Y-%m-%d %H:%M:%S": Displays both date and time in a combined format.
date -u: Displays the date and time in UTC (Coordinated Universal Time).
date --set="2024-07-04 10:00:00": Sets the system date and time (requires root privileges).
2. clear

clear: Clears the terminal screen.
reset: Clears the screen and resets terminal settings (colors, attributes, etc.).
3. ls

ls: Lists files and directories in the current directory.
ls -l: Lists files with detailed information (permissions, owner, size, etc.).
ls -a: Lists all files and directories, including hidden files (starting with a dot).
ls -h: Displays file sizes in a human-readable format (e.g., 1K, 2M, 1G).
ls -R: Recursively lists files and directories in the current directory and all subdirectories.
ls -t: Sorts files by modification time (newest first).
4. cal
>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1

cal: Displays the calendar for the current month.
cal <month> <year>: Displays the calendar for a specific month and year (e.g., cal 7 2024).
cal <year>: Displays the calendar for the entire year.
<<<<<<< HEAD

who

	who: Displays a list of currently logged-in users.
	who -u: Displays more detailed information about logged-in users, including login time and terminal.
=======
5. who

who: Displays a list of currently logged-in users.
who -u: Displays more detailed information about logged-in users, including login time and terminal	


>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
		
			
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Wild card characters: *,?,[],^,-
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
	
	Example:
ls * : Lists all files and directories in the current directory.
rm *.txt : Deletes all files ending with ".txt".
cp * /backup : Copies all files and directories in the current directory to the "backup" directory.
2. ? (Question Mark)

Matches any single character.

Example:
ls f?.txt : Lists files starting with "f", followed by any single character, and ending with ".txt" (e.g., "f1.txt", "f2.txt", "fa.txt").
my_file_?.txt : Matches files like "my_file_1.txt", "my_file_a.txt", etc.
3. [] (Square Brackets)

Matches any single character within the brackets.

Example:
ls f[oa]t.txt : Matches "fat.txt" and "fot.txt".
ls f[0-9].txt : Matches files starting with "f", followed by a single digit (0-9), and ending with ".txt".
ls f[a-z].txt : Matches files starting with "f", followed by any lowercase letter, and ending with ".txt".
4. ^ (Caret within Square Brackets)

Matches any character not within the brackets.

Example:
ls f[^a-c].txt : Matches files starting with "f", followed by any character except "a", "b", or "c", and ending with ".txt".
5. - (Hyphen within Square Brackets)

Specifies a range of characters.

Example:
ls f[a-z].txt : Matches files starting with "f", followed by any lowercase letter, and ending with ".txt".
ls [0-9]_file.txt : Matches files starting with a digit (0-9), followed by "_file.txt".
Important Notes:

Wildcards are often used with commands like ls, rm, cp, mv, and find.


	
	
=======
Wildcards are special characters used in file and directory names to represent a set of characters or any number of characters. Here's a breakdown of the common wildcards and their usage:

* (Asterisk)

	Matches any sequence of characters (including zero characters).
	Examples:
	ls * : Lists all files and directories in the current directory.
	rm *.txt : Deletes all files ending with ".txt".
	? (Question Mark)

Matches any single character.
Examples:
	ls f?.txt : Lists files starting with "f", followed by any single character, and ending with ".txt".
	ls my_file_?.txt : Matches files like "my_file_1.txt", "my_file_a.txt", etc.
	[] (Square Brackets)

Matches any single character within the brackets.
Examples:
	ls f[oa]t.txt : Matches "fat.txt" and "fot.txt".
	ls f[0-9].txt : Matches files starting with "f", followed by a single digit (0-9), and ending with ".txt".
	^ (Caret within square brackets)

Matches any character not within the brackets.
Example: ls f[^a-c].txt : Matches files starting with "f", followed by any character except "a", "b", or "c", and ending with ".txt".
	- (Hyphen within square brackets)

Specifies a range of characters.
	Example: ls f[a-z].txt : Matches files starting with "f", followed by any lowercase letter, and ending with ".txt".
Important Notes:

These wildcards are often used with commands like ls, rm, cp, mv, and find.
Be cautious when using wildcards, especially with commands like rm, as they can unintentionally delete or modify multiple files.
The specific behavior of wildcards may vary slightly depending on the shell you are using (e.g., Bash, Zsh).

>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Managing directories: mkdir, cd, rmdir, $HOME, ~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		mkdir command 
			Used for creating new directories/folder on Unix/Linux systems.
			Refer mkdir folder
			mkdir new_directory: Creates a new directory named "new_directory".
			mkdir -p dir1/dir2/dir3: Creates directories recursively.
			
		
		cd
			“change directory”. 
			
			cd <Path to new dir>
				Path
					absolute path
					relative path

			
			Special notation 
				Following works with most commands including cd.
				"."	: Current directory
					cd ./abc/
				"..": Parent directory	
				"~"	:	home directory
				"–"	: 	previos directory
				
				
				cd ..: Moves to the parent directory.
				cd /: Moves to the root directory.
				cd /home/user: Moves to the home directory of user.
				cd Documents: Moves to the "Documents" directory in the current directory.

		rmdir
			rmdir empty_dir: Removes an empty directory named "empty_dir".
			rmdir -p dir1/dir2/dir3: Removes directories recursively if they are empty.
			
		$HOME 
			~
			
			
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Using absolute and relative path method for changing directories
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	cd /home/user - absolute
	cd user - relative 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
LINUX VI  OR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Introduction to vi Editor
1. What is vi?
	vi (Visual Editor) is a powerful, lightweight text editor available in Unix/Linux.
	Pre-installed on almost every Unix-based system.
	Known for speed and efficiency for editing files.
2. Why Use vi?
	Lightweight and fast.
	Available in server environments without GUIs.
	Extensive keyboard shortcuts for rapid editing.
	Getting Started with vi
1. Launching vi
	Command: vi filename
	Opens the specified file or creates a new file if it doesn't exist.
	Examples:
vi example.txt
	sudo vi /etc/hosts
2. vi Modes
	Command Mode: Default mode for navigation and commands.
	Insert Mode: For typing and editing text.
		Enter with i, a, o.
	Visual Mode: For selecting text (useful for copying, cutting).
		Enter with v, V, or Ctrl+v.
Ex Mode: For saving, quitting, and running commands.
Enter with :.
Basic Navigation in Command Mode
1. Cursor Movement
	Arrow Keys or:
	h: Left
	j: Down
	k: Up
	l: Right
2. Word and Line Navigation
	w: Move to the next word.
	b: Move to the previous word.
	0: Move to the beginning of the line.
	^: Move to the first non-whitespace character of the line.
	$: Move to the end of the line.
Editing Text
1. Entering Insert Mode
	i: Insert before the cursor.
	a: Append after the cursor.
	o: Open a new line below the current line.
2. Deleting Text
	x: Delete a character under the cursor.
	dd: Delete the entire line.
	dw: Delete a word.
3. Undo and Redo
	u: Undo the last change.
	Ctrl+r: Redo the last undone change.

Saving and Quitting
1. Save Changes
	:w: Save the file.
2. Quit vi
	:q: Quit if no changes were made.
	:q!: Force quit without saving changes.

3. Save and Quit
	:wq or ZZ: Save changes and quit.
	Copying, Cutting, and Pasting

1. Copying
	yy: Copy the current line.
	y: Copy text in visual mode.

2. Cutting
	dd: Cut the current line.
	d: Cut selected text in visual mode.

3. Pasting
	p: Paste after the cursor.
	P: Paste before the cursor.
Searching and Replacing
1. Search
	/pattern: Search forward for "pattern".
	?pattern: Search backward for "pattern".
	n: Repeat the search in the same direction.
	N: Repeat the search in the opposite direction.
2. Replace
	:%s/old/new/g: Replace all occurrences of "old" with "new".
	:%s/old/new/: Replace "old" with "new" in the current line.

Advanced Features
1. Line Numbering
	:set number: Show line numbers.
	:set nonumber: Hide line numbers.
	:set paste 
2. Split Windows
	:split filename: Open a file in a horizontal split.
	:vsplit filename: Open a file in a vertical split.
	Use Ctrl+w followed by arrow keys to navigate between splits.
3. Macros
	Start recording: q<register> (e.g., qa to record in register a).
	Stop recording: q.
	Replay macro: @<register> (e.g., @a).
	Customizing vi

1. vi Configuration File
	Location: ~/.vimrc.
	Common settings:
	set number: Enable line numbers.
	set tabstop=4: Set tab width to 4 spaces.


	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Working modes in vi  or
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
		already covered 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Creatinging text files
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		echo "testing" >> test.txt 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Save and quit commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		:wq
		:w
		:q!
		:q
=======

To open a file with vi, use the following command:

vi filename

	If the file exists, vi will open it for editing.
	If the file does not exist, vi creates a new file with the specified name.

4. Navigating in vi

Once in Command Mode, use the following keys to navigate:

4.1 Moving the Cursor

	h: Move left
	l: Move right
	j: Move down
	k: Move up

4.2 Word Navigation

	w: Move to the beginning of the next word.
	e: Move to the end of the current/next word.
	b: Move to the beginning of the current/previous word.

4.3 Line Navigation

	0: Move to the beginning of the current line.
	^: Move to the first non-whitespace character on the current line.
	$: Move to the end of the current line.
4.4 Scrolling

	Ctrl-d: Scroll down half a screen.
	Ctrl-u: Scroll up half a screen.
	Ctrl-f: Scroll forward one full screen.
	Ctrl-b: Scroll backward one full screen.

5. Switching to Insert Mode

	To start inserting or editing text, switch to Insert Mode by pressing:

	i: Insert text before the current cursor position.
	I: Insert text at the beginning of the current line.
	a: Append text after the current cursor position.
	A: Append text at the end of the current line.
	o: Open a new line below the current line and insert text.
	O: Open a new line above the current line and insert text.

To return to Command Mode, press Esc.

6. Saving and Exiting

	In Ex Mode, you can save and exit using the following commands:

	:w: Save the file without exiting.
	:q: Exit if no changes have been made.
	:wq or :x: Save and exit.
	:q!: Quit without saving changes.

7. Deleting Text

	In Command Mode, use the following commands to delete text:

	x: Delete the character under the cursor.
	X: Delete the character before the cursor.
	dd: Delete the current line.
	d$: Delete from the cursor to the end of the line.
	d0: Delete from the cursor to the beginning of the line.

8. Copying and Pasting Text

	vi uses registers for copying and pasting text:
	yy: Copy (yank) the current line.
	yw: Copy from the cursor to the end of the word.
	y$: Copy from the cursor to the end of the line.
	p: Paste the copied text after the cursor.
	P: Paste the copied text before the cursor.

9. Searching and Replacing Text

9.1 Searching

	/pattern: Search for pattern forward.
	?pattern: Search for pattern backward.
	n: Repeat the search in the same direction.
	N: Repeat the search in the opposite direction.

9.2 Replacing

	:s/old/new/: Replace the first occurrence of old with new on the current line.
	:s/old/new/g: Replace all occurrences of old with new on the current line.
	:%s/old/new/g: Replace all occurrences of old with new in the entire file.
	:%s/old/new/gc: Replace all occurrences with confirmation for each change.

10. Undo and Redo

	u: Undo the last change.
	U: Undo all changes on the current line.
	Ctrl-r: Redo the undone change.

11. Advanced Features

11.1 Working with Multiple Files
	:e filename: Open another file in vi.
	:n: Move to the next file.
	:prev: Move to the previous file.

11.2 Splitting Windows

	:split filename: Open a file in a new split window.
	Ctrl-w: Switch between split windows.

11.3 Recording Macros
	q<register>: Start recording a macro into a register (e.g., qa).
	Perform the actions you want to record.
	q: Stop recording.
	@<register>: Play back the recorded macro.

12. Customizing vi

	You can customize vi through the .vimrc (or .exrc) configuration file. Examples:
	Enable line numbers:
	set number
	Enable syntax highlighting:
	syntax on
	Set auto-indentation:
	set autoindent
	:set paste 
		retain the paste formating

13. Tips and Best Practices

	Practice frequently to become familiar with commands.
	Use :help within vi to access detailed documentation.
	Create backups of configuration files before editing.
	Customize your .vimrc for a personalized editing experience.

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Creating text files
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
already covered
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Save and quit commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

already covered	
>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Text   mode commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

already covered	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Cursor movement commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

already covered	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Text deletion commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
		dw 
		dd
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Replacing text commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	:s/from/to/g
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	•  /Cut & Paste commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		yy
		yd 
		p 
=======

already covered	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Replacing text commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:%s/old_text/new_text/g
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	•  /Cut & Paste commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

already covered		
>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Other miscellaneous commands in vi  or
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
already covered		
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
MANAGING FILES & DIRECTORIES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Creating empty files: touch
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	touch command is used to 
		create empty files 
	or 
		change time stamp for existing files
		
		touch newfile.txt
		ls -ltr
		touch existingfile.txt
		ls -ltr
		
		
		
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Displaying contents of file: cat
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		Linux cat command is used to 
			display file content
			create new files 
			  file contents to other files.
			
			
			Refer cat directory (more detailed)
				cat file1.txt: Displays the contents of "file1.txt" on the screen.
				cat file1.txt file2.txt > output.txt: Concatenates the contents of "file1.txt" and "file2.txt" and redirects the output to "output.txt".
				
			create a new file with cat 
				cat > file.txt 
				
			apppend a file with cat 
				cat >> file.txt 
				
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	•  Copying files and directories: cp
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		cp file1.txt new_file1.txt: Copies "file1.txt" to "new_file1.txt".
		cp -r dir1 dir2: Recursively copies the directory "dir1" to "dir2".
		cp -R equivalent to cp -r 
		
		The cp command in Linux is used to copy files and directories. Here's a breakdown of its syntax, common options, and examples:

Basic Syntax:

 

	cp [OPTION]... SOURCE DESTINATION
	SOURCE: The file or directory to be copied.
	DESTINATION: The destination where the file or directory will be copied.
Common Options:

	-r, -R:

		Recursive: Copies directories recursively, including all subdirectories and files within them. This is essential for copying entire directory structures.
	-a:

		Archive mode: Similar to -r, 
			but also preserves file attributes such as 
				ownership, 
				permissions, 
				timestamps
				, and symbolic links.
	-i:

		Interactive: Prompts the user for confirmation before overwriting existing files.
	-f:

		Force: Overwrites existing files without prompting.
	-v:

		Verbose: Displays detailed information about the copying process.
	-p:

		Preserve: Preserves file attributes (ownership, permissions, timestamps).
	-u:

		Update: Copies only if the source file is newer than the destination file.
		Examples:

		Copy a single file:

		 

		cp myfile.txt /home/user/Documents/ 
		Copy a directory recursively:

		 

		cp -r my_directory /backup/ 
		Copy multiple files to a directory:

		 

		cp file1.txt file2.txt file3.txt /destination/directory/
		Copy with archive mode:

		 

		cp -a my_directory /backup/ 
		Force overwrite:

		 

		cp -f myfile.txt existing_file.txt 
		By using these options, you can tailor the cp command to your specific needs and ensure that files are copied correctly and efficiently.
				
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Removing files: rm
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		rm file1.txt: Deletes the file "file1.txt".
		rm -r dir1: Recursively deletes the directory "dir1" and all its contents.
	
	The rm command in Linux is used to delete files and directories from the file system.

	Basic Syntax:

	 

		rm [OPTIONS] file1 file2 ... directory1 directory2 ...
		Common Options:

		-f, --force:

			Forces removal without prompting for confirmation.
			Caution: Use with extreme care, as it can lead to accidental data loss.
		-i, --interactive:

			Prompts for confirmation before deleting each file or directory.
		-I:

			Prompts only once before removing more than three files or directories.
		-r, -R:

			Recursive: Deletes directories and all their contents recursively.
			Extremely dangerous: Use with extreme caution, as it can permanently delete large amounts of data.
		-v, --verbose:

			Displays the names of files and directories as they are being deleted.
		-d, --directory:

			Removes only empty directories.
		Examples:

		Delete a single file:

		 

			rm myfile.txt 
		Delete multiple files:

		 

			rm file1.txt file2.txt 
		Delete a directory and its contents:

		 

			rm -r my_directory 
		Delete files interactively:

		 

			rm -i *.txt 
		Important Notes:

		Use with extreme caution: The rm command can permanently delete files and directories.
		Always double-check your commands before executing them, especially when using the -r or -f options.
		Consider backing up important data before using the rm command for critical files or directories.
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Renaming Files and directories: mv
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		mv file1.txt new_location/: Moves "file1.txt" to the "new_location" directory.
		mv file1.txt new_filename.txt: Renames "file1.txt" to "new_filename.txt".
		
		The mv command in Linux is a versatile tool used to move files and directories from one location to another, or to rename them. Here's a breakdown of its usage and common options:

Basic Syntax:

 

mv [OPTION]... SOURCE DESTINATION
SOURCE: The file or directory to be moved or renamed.
DESTINATION: The new location or the new name for the file or directory.
Common Options:

-i, --interactive: Prompts the user for confirmation before overwriting existing files.
-f, --force: Overwrites existing files without prompting.
-v, --verbose: Displays detailed information about the move operation.
-n, --no-clobber/don't overwrite : Prevents overwriting existing files.
Examples:

Move a file to a new location:

 

mv myfile.txt /home/user/Documents/
Rename a file:

 

mv old_filename.txt new_filename.txt
Move a directory:

 

mv my_directory /backup/ 
Rename a directory:

 

mv old_directory_name new_directory_name
Move multiple files:

 

mv file1.txt file2.txt file3.txt /destination/directory/
Important Notes:

Use with caution: The mv command can permanently move or rename files.
Backups: It's always a good practice to back up important files before using mv, especially when moving files to a different location.
Overwriting: Be mindful of the -f option as it can overwrite existing files without warning.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• File locating command: find
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	find (Search for Files and Directories)
		find . -name "filename.txt": 
			Searches for the file named "filename.txt" in the current directory and its subdirectories.
		find . -type f: 
			Finds all files (not directories) in the current directory and its subdirectories.

The find command in Linux is a powerful tool for locating files and directories within a file system. Here's a breakdown of its usage and some of the most commonly used options:

Basic Syntax

 

	find [path] [options] [expression]
		path: The starting directory for the search. If omitted, the current directory (.) is used.
		options: Various flags that modify the search criteria.
		expression: Criteria for filtering and locating files.
Common Options

	-name "filename": Finds files with the specified name.
Example: find . -name "myfile.txt"
	-iname "filename": Finds files with the specified name, ignoring case.
Example: find . -iname "MyFile.txt"
	-type f: 
		Finds only files (not directories).
	-type d: 
		Finds only directories.
	-size +10M: 
		Finds files larger than 10 megabytes.
	-size -10M: 
		Finds files smaller than 10 megabytes.
	-user user: 
		Finds files owned by the specified user.
	-group group: 
		Finds files owned by the specified group.
	-mtime +7: 
		Finds files modified more than 7 days ago.
	-mtime -7: 
		Finds files modified within the last 7 days.
	-ctime +7: 
		Finds files whose status (ownership, permissions) has changed more than 7 days ago.
	-perm 777: 
		Finds files with specific permissions (octal notation).
	-exec command {} ;: 
		Executes a command on each file found.
Example: find . -name "*.txt" -exec grep "keyword" {} \; (searches for "keyword" within all .txt files)
-print: Displays the full path of each file found.
Combining find with other commands

	find ... -exec rm {} \;: Deletes files found by the find command. Use with extreme caution!
	find ... -exec cp {} /backup/ \;: Copies files found by the find command to the /backup directory.
	find ... -exec grep "keyword" {} \;: Searches for a specific keyword within all files found by the find command.
Example:

Find all files ending with ".txt" in the current directory and its subdirectories:

 

	find . -name "*.txt" 
Find all files owned by the user "john" in the /home/john directory:

 

	find /home/john -user john
Find all files larger than 1GB in the /var/log directory:

 

	find /var/log -type f -size +1G
Find all files modified within the last 24 hours:

 

	find . -mtime -1 
The find command is a powerful tool with numerous options and combinations. By mastering its usage, you can efficiently locate and manage files within your Linux system.


	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Sorting file contents: sort
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The sort command in Linux is a powerful tool for organizing and arranging lines of text within a file or from standard input. Here's a breakdown of its usage and key options:

Basic Usage:

	sort filename: Sorts the lines in the file filename alphabetically in ascending order and displays the output to the terminal.
Key Options:

	-r: Reverses the sorting order (descending).

Example: sort -r filename
	-n: Sorts numerically. This is crucial for sorting lines containing numbers.

Example: sort -n numbers.txt
	-R: Sorts randomly.

	-u: Removes duplicate lines.

	-k <field>: Sorts based on a specific field within each line.

	By default, fields are delimited by whitespace.
	Example:
	sort -k 2 filename (sorts by the second field)
	If your data is comma-separated, use -t ',' to specify the comma as the field delimiter: sort -t ',' -k 2n filename (sorts by the second field numerically)
	-k <start>,<end>: Sorts based on a range of fields.

	-f: Ignores case differences during sorting (e.g., treats "A" and "a" as the same).

	-M: Sorts lines according to month names (January, February, etc.).

	-o <output_file>: Writes the sorted output to a specified file instead of displaying it on the terminal.

Example: sort -o sorted_data.txt filename
Example:

Let's say you have a file named data.txt with the following content:

apple
banana
cherry
apple 
orange
sort data.txt:

Output: apple apple banana cherry orange
sort -u data.txt:
	No duplicate data 
Output: apple banana cherry orange
(Removes the duplicate "apple")
sort -r data.txt:
	reverse sort 

Output: orange pear cherry banana apple
Using sort with -k:

If data.txt contains lines like this:

apple 2
banana 5
cherry 1
sort -k 2n data.txt:
Sorts the lines based on the second column (numbers) numerically.
Output: cherry 1 apple 2 banana 5
Combining sort with other commands:


df -h | sort -k -h 4 

You can often combine sort with other commands like ls, grep, cut, and awk to perform complex data manipulation tasks.

By mastering the sort command and its options, you can effectively organize and analyze data from various sources in the Linux environment.

continue from here 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Extracting specific characters and fields: cut
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		Used to extract the specific column from a file. 
		Define the column number and column delimiter with command.
		
		cut -d<delimiter> -f<field1,field2,...> filename
			-d: delimiter
			-f: column number
		
		To extract 1st and 6th column from /etc/passwd file.
			cut -d":" -f1,6 /etc/passwd
			
		-f can be a range too
			cut -d":" -f1-4,6 /etc/passwd
			cut -d":" -f1-3,5-7 /etc/passwd
			
			
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Other commands 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			
	less (View File Content Page by Page)
		less file1.txt: 
			Displays the contents of "file1.txt" one screen at a time, allowing you to scroll through the file.
	
	head (Display the Beginning of a File)
		head -n 10 file1.txt: Displays the first 10 lines of "file1.txt".
	
	tail (Display the End of a File)
		tail -n 10 file1.txt: Displays the last 10 lines of "file1.txt".
		tail -f logfile.txt: Continuously displays the latest lines added to "logfile.txt".
	
	
	man (Display the Manual Page for a Command)
		man ls: Displays the manual page for the "ls" command.


	kill (Terminate Processes)

		kill <process_id>: Sends a signal to the process with the specified ID to terminate it.
		kill -9 <process_id>: Sends a stronger signal (SIGKILL) to the process to force its termination.
	
	
	
	tar (Archive Files)

		tar -cvzf archive.tar.gz file1.txt file2.txt: Creates a compressed tar archive named "archive.tar.gz" containing "file1.txt" and "file2.txt".
		tar -xvf archive.tar 
		tar -xvfz archive.tar.gz: Extracts the contents of "archive.tar.gz".
		
		
	df
		* Purpose: 
			Displays disk space usage.
		* Example: df -h (displays disk space usage in human-readable format)
		Details:
			This command displays the available and used disk space on all mounted filesystems in a human-readable format (e.g., GB, MB).
		How it works:
			It retrieves filesystem information and formats it for display.
		Options:
			-T: Include the filesystem type.
		 
		df -hT
			df -hT - Display disk space usage with filesystem type
		df -i
			-i: Show inode usage instead of disk space.
	 
		The df command in Linux displays disk space usage information. Here are some of the most commonly used options:

		-h: Displays disk space information in a human-readable format (e.g., 1G, 2M, 1K). This is generally the most useful option for quickly understanding disk space usage.

		-k: Displays disk space information in kilobytes.

		-m: Displays disk space information in megabytes.

		-a: Displays information about all file systems, including those with zero block size.

		-T: Displays the type of file system for each partition.

		-t <type>: Displays information only for file systems of the specified type (e.g., -t ext4).

		-x <type>: Excludes file systems of the specified type from the output.

		-l: Displays information only for local file systems.

		Examples:

		df -h: Displays disk space usage for all mounted file systems in a human-readable format.
		df -h /: Displays disk space usage for the root file system.
		df -h /home: Displays disk space usage for the /home directory.
		df -T: Displays the type of each mounted file system.
		df -h -t ext4: Displays disk space usage for ext4 file systems only.
		

	du
		* Purpose: 
			Estimates file and directory space usage.
		* Example: du -sh (displays disk usage in human-readable format for the current directory and its subdirectories)
		du - Estimate File and Directory Space Usage
		Command Example:
		 
		 
		 
		du -h
		Details:
			This command estimates the disk space usage of 
				files and directories, 
				showing sizes in human-readable format.
			How it works:
			It traverses the directory structure and calculates the size of each file and directory.
			Options:
			-d 1: Display the size of directories one level deep.
			du -h -d 1
				Display the size of the directories in the current directory
			-s: Show only the total size.
			
			 
			du -sh
			The du command in Linux is used to estimate disk space usage. Here are some of the most commonly used options:

			-h: Displays disk usage in a human-readable format (e.g., 1G, 2M, 1K). This is generally the most useful option for quickly understanding disk space usage.

			-s: Summarize: Displays only the total disk usage for the specified directory, without showing the usage of individual files or subdirectories.

			-a or --all: Displays disk usage for all files and directories, including hidden ones.

			-d <depth> or --max-depth=<depth>`: Limits the depth of directory traversal.

			For example, -d 1 will only display the disk usage of the specified directory and its immediate subdirectories.
			-c or --total: Displays the total disk usage of all directories listed.

			--apparent-size: Displays the apparent size of directories, which may be larger than the actual disk usage due to hard links.

			Examples:

			du -h: Displays disk usage for the current directory and all subdirectories in a human-readable format.
			du -h ~/Documents: Displays disk usage for the Documents directory in the home directory.
			du -h -d 1 ~/: Displays disk usage for the home directory and its immediate subdirectories.
			du -sh ~/: Displays the total disk usage of the home directory.
	
	
	free
		* Purpose: 
			Displays system memory usage (total, used, free, etc.).
		
		The free command in Linux provides information about the system's memory usage, including RAM, swap space, and buffers/cache. Here are some of the most commonly used options:

		-h or --human: Displays memory usage in a human-readable format (e.g., 1G, 2M, 1K). This makes the output easier to understand.

		-m or --mega: Displays memory usage in megabytes.

		-g or --giga: Displays memory usage in gigabytes.

		-k or --kilo: Displays memory usage in kilobytes (default).

		-t or --total: Displays the total amount of memory (RAM + Swap).

		-l or --lohi: Shows detailed low and high memory statistics.

		-s <seconds>: Continuously displays the output after a specified number of seconds. This is useful for monitoring memory usage over time.

		-c <count>: Displays the output count number of times; works with the -s option.

		Example:

		free -h: Displays memory usage in a human-readable format (recommended for most users).

		free -m: Displays memory usage in megabytes.

		free -s 2: Continuously displays memory usage every 2 seconds.

		free -h -t: Displays memory usage in a human-readable format and includes the total amount of memory.

		By using these options, you can effectively monitor your system's memory usage and identify potential memory-related issues.
		

	ifconfig
		* Purpose: (Older command) Configure and display network interface information.
		old command - ignore 

	netstat
		* Purpose: Display network statistics and connections.
		* Example: netstat -an (displays all network connections)
		The netstat command in Linux provides information about network connections, routing tables, and network interface statistics. Here are some of the most commonly used options:

		-a: Displays all network connections, including listening sockets.

		-n: Displays addresses and port numbers numerically instead of resolving hostnames.

		-l: Displays only listening sockets.

		-r: Displays the routing table.

		-t: Displays TCP connections.

		-u: Displays UDP connections.

		-p: Displays the PID (Process ID) of the process that owns the connection.

		-s: Displays network statistics for each protocol (e.g., TCP, UDP).

		-i: Displays network interface statistics.

		-e: Displays Ethernet statistics (if supported).

		Some common combinations:

		netstat -an: 
		Displays all network connections with addresses and port numbers in numerical format.
		netstat -tulnp: Displays listening TCP connections with the associated process information.
		netstat -ru: Displays UDP connections and statistics.
		netstat -r: Displays the routing table.
		netstat -i: Displays network interface statistics.
		Example:

		 

		netstat -an | grep LISTEN 
		This command will display all listening TCP and UDP sockets on the system.

	scp
		* Purpose: Securely   files between local and remote systems.
		* Example: scp local_file.txt user@remote_server:/path/to/remote/directory
		scp - Securely   Files Between Local and Remote Systems
		Command Example:
		 
		scp file.txt user@remote_server:/path/to/destination
		Details:
		This command copies file.txt from the local system to the specified path on the remote server.
		How it works:
		Uses SSH for secure file transfer.
		Options:
		-r:   directories recursively.
		 
		 
		 
		scp -r /local/directory user@remote_server:/remote/destination

		The scp (Secure Copy) command is used to securely transfer files between computers over an SSH connection. Here are some of the most commonly used options:

		-r: Recursively copy entire directories. This is essential for copying folders and their contents.

		Example: scp -r my_directory user@remote_host:/path/to/remote/directory
		-p: Preserve file modification times, access times, and modes (permissions). This ensures that the copied files retain their original attributes.

		-q: Quiet mode. Suppresses progress meters, warnings, and diagnostic messages.

		-P port: Specifies the port number to use for the SSH connection. The default port is 22.

		Example: scp -P 2222 myfile.txt user@remote_host:/path/to/remote/file
		-i identity_file: Specifies the path to the private key file for authentication. This is useful when using public-key authentication.

		Example: scp -i ~/.ssh/id_rsa myfile.txt user@remote_host:/path/to/remote/file
		-v: Verbose mode. Displays detailed information about the transfer process.

		-c cipher: Specifies the cipher algorithm to use for encryption.

		Basic Usage Examples:

		Copy a file from local to remote:

		 

		scp myfile.txt user@remote_host:/path/to/remote/file
		Copy a file from remote to local:

		 

		scp user@remote_host:/path/to/remote/file my_local_file.txt
		Copy a directory recursively from local to remote:

		 

		scp -r my_directory user@remote_host:/path/to/remote/directory
		Important Notes:

		scp requires an SSH server to be running on the remote host.
		Ensure that you have the necessary permissions to copy files to the specified locations on both the local and remote hosts.
		For security reasons, it's recommended to use public-key authentication instead of passwords for SSH connections.

	wget
		* Purpose: Download files from the internet.
		* Example: wget http://example.com/file.zip


		The wget command in Linux is a powerful tool for downloading files from the internet. Here are some of the most commonly used options:

		1. Basic Downloading:

		wget <URL>: Downloads the file specified by the URL to the current directory.
		Example: wget https://www.example.com/file.zip
		2. Specifying Output File:

		-O <filename>: Saves the downloaded file with the specified filename.
		Example: wget -O my_file.zip https://www.example.com/file.zip
		3. Background Download:

		-b: Downloads the file in the background.
		Example: wget -b https://www.example.com/large_file.iso
		4. Recursive Download:

		-r or --recursive: Recursively downloads all files and directories within a specified URL. This is useful for mirroring websites.
		Example: wget -r http://www.example.com/
		5. Limit Download Rate:

		--limit-rate=rate: Limits the download speed to the specified rate (e.g., --limit-rate=20k).
		6. Resuming Downloads:

		-c or --continue: Resumes an interrupted download.
		7. Quiet Mode:

		-q: Quiet mode. Suppresses most output except for error messages.
		8. Verbose Mode:

		-v: Verbose mode. Displays detailed information about the download process.
		9. User Authentication:

		-u username:password: Provides username and password for authentication.
		10. Downloading from a List:

		-i filename: Reads URLs from a file and downloads them sequentially.
		Example:

		 

		wget -r -l 2 -np -nd -k -N http://www.example.com/ 
		-r: Recursive download.
		-l 2: Limit recursion to a depth of 2 levels.
		-np: Do not download files in directories that have already been visited.
		-nd: Do not create subdirectories.
		-k: Convert relative links to absolute links.
		-N: Don't download files unless newer than a local copy.
		By using these options, you can customize wget to fit your specific download needs, such as downloading files in the background, resuming interrupted downloads, and efficiently retrieving large amounts of data from the internet.

		Note: Always use wget responsibly and respect the terms of service of the websites you are downloading from.


	curl
		* Purpose: 
			Transfer data with URLs (more versatile than wget, can handle various protocols like HTTP, HTTPS, FTP).
		
		more comfortable to make POST, PUT, DELETE request than wget.
		
		The curl command is a versatile command-line tool used for transferring data from or to a server, using various protocols such as 
			HTTP, 
			HTTPS, 
			FTP, etc. 
			Here are some of the most commonly used options:

		1. Fetching Data:

			-O: Downloads the file and saves it with the original filename from the server.
			-o <filename>: Downloads the file and saves it with the specified filename.
			curl -o index.html https://www.example.com/
		2. Displaying Data:

			-I or --head: Displays only the HTTP headers of the response.
			-v or --verbose: Displays detailed information about the transfer, including request and response headers.
		3. Making Requests:

			-X <method>: Specifies the HTTP method to use (e.g., -X POST, -X PUT, -X DELETE).
			-d <data>: Sends data as a POST request.
			curl -d "name=John&city=NewYork" https://example.com/api
		4. Following Redirects:

			-L or --location: Follows HTTP redirects (e.g., 301, 302).
		5. Authentication:

			-u <username:password>: Provides username and password for basic authentication.
		6. Proxies:

			-x <proxy_host:proxy_port>: Specifies a proxy server to use for the connection.
		7. Headers:

			-H "Header-Name: Header-Value": Adds custom HTTP headers to the request.
		curl -H "User-Agent: My-Custom-Agent" https://www.example.com
		8. Cookies:

			-c <cookie_file>: Reads cookies from a file.
			-b <cookie_file>: Writes received cookies to a file.
		9. SSL/TLS:

			--ssl: Enables SSL/TLS support.
			--insecure: Disables SSL/TLS certificate verification (use with caution).
		10. Output to File:

			> filename: Redirects the output of the command to a file.
			curl https://www.example.com/ > index.html
			These are just a few of the many options available with curl. It's a versatile tool with a wide range of capabilities for interacting with web servers.

		Note: Always use curl responsibly and respect the terms of service of the websites you are interacting with.


	shutdown
	--------
		* Purpose: Shutdown or restart the system.
		* Example: shutdown -h now (shuts down the system immediately)
		

		The shutdown command in Windows is used to shut down, restart, or log off the computer. Here are some of the most commonly used options:

		/s: Shuts down the computer.
		/r: Shuts down and restarts the computer.
		/l: Logs off the current user.
		/f: Forces running applications to close.
		/t xx: Sets the timer for system shutdown in xx seconds. The default is 30 seconds.
		Example: shutdown /s /t 60 (shuts down the computer after 60 seconds)
		/a: Aborts a system shutdown that has been scheduled.
		Example:

		shutdown /s: Shuts down the computer immediately.
		shutdown /r /t 30: Restarts the computer after 30 seconds.
		shutdown /l: Logs off the current user.
		shutdown /a: Cancels a previously scheduled shutdown.
		Note:

		The shutdown command requires administrator privileges.
		You can use the shutdown /? command to see a complete list of available options and their descriptions.


	reboot
	--------
		Purpose: Restart the system.
		reboot -h now - Reboot the system immediately

	
		The reboot command in Linux is used to restart the system. Here are some of the most common options:

			-f or --force: Forces an immediate reboot, bypassing any normal shutdown procedures.

			Warning: Use with caution as it can potentially lead to data corruption.
			-n or --no-sync: Skips the file system synchronization step before rebooting. This can speed up the reboot process but may result in data loss if unsaved data is present.

			-w or --wtmp-only: Only updates the wtmp file (which records system logins and shutdowns) and does not actually reboot the system.

			-h or --help: Displays help information about the reboot command.

			Examples:

			sudo reboot: Reboots the system normally.

			Requires root privileges (use sudo to gain root access).
			sudo reboot -f: Forces an immediate reboot.

			Important Notes:

			Always save any unsaved work before rebooting your system.
			Use the -f option with caution as it can lead to data loss.
			If you encounter issues after rebooting, check the system logs for any error messages.
	
	uname
	--------
		* Purpose: Display system information (kernel name, hostname, etc.).
		uname -a
			The output 
				MINGW64_NT-10.0-22631 LAPTOP-BUA2IBR3 3.4.10-87d57229.x86_64 2024-02-14 20:17 UTC x86_64 Msys from the uname -a command provides information about your system:

			MINGW64_NT-10.0-22631: 
				This indicates that you are using the MinGW-w64 environment
					a port of the GNU Compiler Collection (GCC) to the Windows operating system.

			MINGW64: 
				Stands for "Minimalist GNU for Windows 64-bit."
			NT-10.0-22631: 
				Refers to the Windows NT version, specifically Windows 10, with build number 22631.
			LAPTOP-BUA2IBR3: 
				This is the hostname of your computer.

			3.4.10-87d57229.x86_64: 
				This is the kernel version.

			3.4.10: 
				The major, minor, and patch version of the kernel.
			-87d57229.x86_64: 
				A specific build or revision number, along with the architecture (x86_64, indicating 64-bit).
			2024-02-14 20:17 UTC: 
				This is the date and time the kernel was compiled.

			x86_64: 
				This indicates the system architecture (64-bit).

			Msys: 
				This signifies that you are using the MSYS2 environment, which provides a more comprehensive set of tools and libraries for developing with MinGW-w64 on Windows.

	who
	--------
		* Purpose: Display currently logged-in users.
	whoami
	--------
	
	history
	--------
		* Purpose: Display a list of previously executed commands.
		

	alias
	--------
		* Purpose: Create shortcuts for frequently used commands.
		* Example: alias ll='ls -l' (creates an alias "ll" for the command "ls -l")
		


		The alias command in Linux/Unix shells allows you to create shortcuts for frequently used commands. Here's a breakdown of its usage and common options:

		Key Options:

		alias [name]=[command]:

		Creates a new alias.
		name is the shortcut name you want to use.
		command is the actual command or sequence of commands that the alias will execute.
		Example: alias ll='ls -l' (creates an alias named ll for the ls -l command)
		alias:

		Without any arguments, displays a list of all currently defined aliases.
		unalias [name]:

		Removes a specific alias.
		Example: unalias ll (removes the ll alias)
		unalias -a:

		Removes all defined aliases.
		Example:

		Create an alias:

		 

		alias la='ls -a' 
		This creates an alias named la that executes the ls -a command (lists all files, including hidden ones).

		Use the alias:

		 

		la 
		This will execute the ls -a command.

		List all aliases:

		 

		alias
		Remove an alias:

		 

		unalias la
		Tips for Effective Alias Usage:

		Keep aliases concise and meaningful: Choose short and descriptive names for your aliases to improve readability.
		Consider using shell configuration files: To make aliases permanent across sessions, add them to your shell's configuration file (e.g., . rc, .zshrc).
		Avoid conflicts: Avoid creating aliases that conflict with existing commands or other aliases.
		Use aliases judiciously: While aliases can improve productivity, overuse can make your commands less portable and harder to understand for others.

		
	
	
	gzip - Compress Files
	---------------------
	Command Example:
		gzip filename.txt
		Details:
			This command compresses the file filename.txt into a .gz file (filename.txt.gz) to save disk space.
		How it works:
			gzip uses a compression algorithm to reduce the file size.
			The original file is replaced with the compressed .gz version.
		Options:
			-v: Verbose mode; displays compression details.
		gzip -v filename.txt
			-k: Keep the original file after compression.
		gzip -k filename.txt

		


	unzip - Extract Files from a ZIP Archive
	---------------------
	Command Example:
		unzip archive.zip
		Details:
			This command extracts the contents of archive.zip into the current directory.
		How it works:
		It reads the ZIP archive and decompresses its contents.
		Options:
			-l: List the contents of the ZIP file without extracting.
		unzip -l archive.zip
			unzip -l - List the contents of a ZIP archive
		unzip -d 
			-d: Extract the contents to a specific directory.
			unzip archive.zip -d /path/to/destination


	Add the below commands also 
	---------------------------
	echo - Display a message or value
	---------------------
	
		echo - Display a message or value

		Purpose: Prints text or the value of variables to the standard output (usually the terminal).

		Common Uses:

		Displaying text: echo "Hello, world!"
		Displaying variable values: name="John"; echo "My name is $name"
		Creating new files: echo "This is some text" > new_file.txt
		Appending to files: echo "More text" >> existing_file.txt
    
	which - Show the full path of a command
	---------------------
		Purpose: Locates the executable file associated with a given command.

		Common Use:

		Finding the path: which ls (shows the full path to the ls command, e.g., /usr/bin/ls)

	
    whois - Look up domain registration information
    ---------------------
		whois - Look up domain registration information

		Purpose: Retrieves information about a domain name's registration, such as the owner, registrar, and expiration date.

		Common Use:

		Domain lookup: whois google.com
	
	
	nc - Network utility for reading/writing data across networks
	---------------------
		Purpose: A versatile tool for various network tasks, including:

		Connecting to ports: nc google.com 80 (attempts to connect to port 80 on google.com)
		Listening on ports: nc -lvp 8080 (listens on port 8080)
		Transferring files: nc -q 1 < file.txt > received_file.txt (sends file.txt)
		Port scanning: nc -zv google.com 1-100 (checks open ports 1 through 100)
		Common Uses (Examples):

		Simple chat server: nc -lvp 8080 (on one terminal) and nc localhost 8080 (on another)
		Banner grabbing: nc target_ip 80 (retrieves the web server's banner)
	
	ssh-keygen - Generate SSH key pairs
    ---------------------
		Purpose: Creates SSH key pairs used for secure, passwordless authentication to remote servers.

		Common Use:

		Key generation: ssh-keygen -t rsa (generates an RSA key pair)
		Specifying output file: ssh-keygen -t rsa -f my_key (saves the key to my_key)
	
	
    ln -s - Create symbolic links to files or directories
    ---------------------
		ln -s - Create symbolic links to files or directories

		Purpose: Creates symbolic links (soft links), which are like shortcuts to files or directories.

		Common Use:

		Creating a link: ln -s /path/to/original_file link_name

	time - Measure the execution time of a command
    ---------------------
	Purpose: Measures how long it takes for a command to run.

	Common Use:

	Timing a command: time ls -l
	
    	
	
	
	history -c - Clear command history
	---------------------
    sudo !! - Re-run the last command with sudo
    ---------------------
	
	sudo su - #switch user 
	---------------------
		sudo su in Linux effectively grants you root privileges. Here's a breakdown:

			sudo: 
				This command allows a user with appropriate permissions (typically members of the sudo group) to execute commands as another user, usually root.
			su: 
				This command stands for "switch user." 
				It's used to switch to another user account.
			Therefore, sudo su essentially does the following:

				Elevation of Privileges: 
					sudo temporarily elevates the current user's privileges to 
						allow them to execute commands as root.
				Switching to Root: 
					su then switches the user's session to the root account.
			In essence, sudo su provides a way to gain root privileges within a single command.

			Key Points:

				Security: 
					Using sudo responsibly is crucial. 
					Only use it when necessary and for legitimate system administration tasks.
				Root Privileges: 
					With root privileges, 
						you have the power to make significant changes to the system. 
						Exercise caution to avoid unintended consequences.
				sudoers File: 
					The sudo command relies on the sudoers file to 
						define which users are allowed to use 
							sudo and what commands they can execute.
	
	
	Control + R to search for a command 
	---------------------
		search for a command 
		
	
	locate - Find files by name
	---------------------
		The locate command in Linux is a powerful tool for quickly finding files and directories on your system. It works by searching a pre-built database of file locations, making it much faster than the find command, which searches the filesystem directly.

		Here are some of the most commonly used options with the locate command:

		-i: Performs a case-insensitive search.

			Example: locate -i myfile will find "myfile", "MyFile", and "myfile.txt".
		-c: Displays only the count of matching files, without listing their paths.

		-n <number>: Limits the number of results displayed.

			Example: locate -n 10 myfile will display only the first 10 matches.
		
		-r: Uses regular expressions for pattern matching.

			Example: locate -r "my.*" will find all files starting with "my".
		
		--regex: 
			Similar to -r, 
			uses extended regular expressions for more complex pattern matching.

		-l <limit>: 
			Sets the maximum number of results to display.

		-d <database>: 
			Specifies the database file to use.

		Example:

			locate myfile.txt: Searches for the file named "myfile.txt" in the database.

			locate -i myfile: Searches for all files containing "myfile" (case-insensitive).

			locate -r "my.*": Searches for all files starting with "my".

		Important Notes:

		The locate command relies on a pre-built database of file locations. This database is typically updated periodically by a cron job.
		To use locate effectively, you need to ensure that the database is up-to-date. You can usually update the database by running the updatedb command as root.


	
    du -h -d 1 - 
	---------------------
	

		The command du -h -d 1 in Linux is used to display disk space usage information in a human-readable format for the current directory and its immediate subdirectories.

		Here's a breakdown:

		du: 
			"disk usage." 
			It's used to estimate disk space usage.
		-h: 
			Displays disk space usage in a 
				human-readable format (e.g., 1G, 2M, 1K). This makes the output easier to understand.
		-d 1: 
			Limits the depth of directory traversal to one level. 
			This means it will only display the disk usage 
				of the current directory and 
				its immediate subdirectories, 
					not any deeper levels.
		In essence, du -h -d 1 provides a concise overview of disk space usage for the current directory and its immediate subfolders, making it easy to identify which directories are consuming the most space.

		Example:

		If you execute du -h -d 1 in your home directory, it will show you the disk space usage of your home directory and all the immediate subdirectories within it. This can be helpful in quickly identifying which subdirectories are taking up the most space on your system.

    
	zip -r - Create a ZIP archive recursively
<<<<<<< HEAD
	---------------------
		The zip -r 
			command in Linux/Unix 
			used to create a compressed archive of a directory and all its contents recursively.

		Here's a breakdown:

		zip: 
			This is the core command for creating and manipulating ZIP archives.
		-r: 
			The recursive option. This is crucial for archiving directories. It tells zip to include all files and subdirectories within the specified directory.
		Basic Usage:

		 

		zip -r archive_name.zip directory_to_archive
		This command creates a ZIP archive named archive_name.zip containing all the files and subdirectories within the directory_to_archive.
		Example:

		 

		zip -r my_documents.zip Documents 
		This command will create a ZIP archive named my_documents.zip containing all the files and subdirectories within the Documents directory.

		Key Points:

		Compression: By default, zip uses a compression algorithm to reduce the size of the archive.
		File Permissions: The zip command can preserve file permissions and ownership information (if supported by the ZIP format).
		Other Options: zip offers many other options for customizing the archiving process, such as:
		-v: Verbose output (displays detailed information about the archiving process).
		-x <filename>`: Excludes specific files from the archive.
		-i <filename>`: Includes only files listed in a specified file.
		By using zip -r, you can efficiently create compressed archives of directories and their contents, making it easier to store, transport, and back up data.

    zip -d - 
	---------------------
		Remove files from a ZIP archive
		The zip -d command in Linux/Unix is used to delete files from an existing ZIP archive.

		Basic Syntax:

		 

		zip -d archive_name.zip file1.txt file2.txt ...
		zip: 
			The core command for creating and manipulating ZIP archives.
		-d: 
			The option for deleting files from the archive.
		archive_name.zip: The name of the existing ZIP archive.
		file1.txt file2.txt ...: The names of the files to be deleted from the archive.
		Example:

		 

		zip -d my_archive.zip file1.txt important_document.pdf
		This command will remove the files file1.txt and important_document.pdf from the my_archive.zip archive.

		Important Notes:

		Modifying Archives: Modifying ZIP archives directly can sometimes lead to unexpected behavior or corruption.
		Backup: It's always a good practice to create a backup of the original archive before making any modifications.
		By using zip -d, you can selectively remove files from existing ZIP archives, which can be useful for managing and optimizing archive size.
		

=======
    zip -d - 
		Remove files from a ZIP archive
    ps -e | awk '{print $1}'  | sort pcpu 
		filter command output using awk 
	lsof -i - Display information about open internet connections
    killall - Signal processes by name
    top -u username - Monitor resource usage for a specific user
>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
    
	lsof -i - Display information about open internet connections
	---------------------
		lsof -i 
			list open files in Linux, 
			specifically focusing on those related to network connections.

		Here's a breakdown:

		lsof: Stands for "List Open Files." It's a powerful command-line utility that provides information about files currently open by processes on the system.

		-i: This option specifically tells lsof to list only files associated with network connections (e.g., sockets, TCP connections, UDP connections).

		What lsof -i typically displays:


		Process ID (PID): The unique identifier of the process that has the file open.
		User: The user who owns the process.
		Command: The name of the command or program that the process is executing.
		Protocol: The network protocol used (e.g., TCP, UDP).
		IP Address: The IP address of the remote host (if applicable).
		Port: The port number used for the connection.
		State: The current state of the connection (e.g., ESTABLISHED, LISTEN, CLOSE_WAIT).
		Example:

		 

		lsof -i 
		This command will list all open network connections on the system.

		Other useful lsof options with -i:

		lsof -i tcp: List only TCP connections.
		lsof -i udp: List only UDP connections.
		lsof -i :80: List connections on port 80.
		lsof -i :8080: List connections on port 8080.
		lsof -i :80-90: List connections on ports 80 to 90.
		lsof -i 4: List connections on IPv4.
		lsof -i 6: List connections on IPv6.
		Important Notes:

		lsof requires root privileges for full functionality.
		The output of lsof can be quite detailed and may require some familiarity with network concepts to fully understand.
		By using lsof -i and its various options, you can gain valuable insights into network activity on your system, identify potential issues, and troubleshoot network-related problems.
		
		
    killall - Signal processes by name	
	---------------------
	The killall command in Linux is used to terminate all processes with a specific name.

		Basic Syntax:

		 

		killall [options] <process_name> 
		Common Options:

		-s signal: Specifies the signal to be sent to the processes.

		-s SIGTERM (default): Requests graceful termination.
		-s SIGKILL (9): Forces immediate termination. Use with caution, as it can lead to data loss.
		-s SIGINT (2): Simulates a keyboard interrupt (Ctrl+C).
		-I: Prompts for confirmation before killing each process.

		-v: Verbose mode. Displays the names of the processes that are being killed.

		-e, --exact: Requires an exact match for the process name.

		-I, --ignore-case: Performs case-insensitive matching of process names.

		Examples:

		killall firefox: Terminates all processes named "firefox."

		killall -9 chromium: Forcibly terminates all processes named "chromium."

		killall -s SIGINT chrome: Sends the SIGINT signal to all processes named "chrome."

		Important Notes:

		Use with caution: killall can terminate critical system processes if not used carefully.
		Always check the output of ps aux to identify the processes you want to terminate.
		Consider using -I for confirmation to avoid accidental termination of important processes.
		Alternatives:

		pkill: A similar command that allows for more flexible matching criteria, such as regular expressions.
	
	
	
	top -u username - Monitor resource usage for a specific user
	---------------------	
	
	

 
 
	print error message in a log file 

		curl https://url(github url) | grep ERROR 

	
 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Day – 2
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
MANAGING FILES & DIRECTORIES (cont’d)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• File comparing commands: cmp, comm, diff, patch
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

<<<<<<< HEAD
1. cmp

	Purpose: Compares two files byte-by-byte.
	Syntax: cmp [OPTION]... FILE1 FILE2
	Common Options:
	-b: Ignore differences due to differing block sizes.
	-c: Display differing bytes in octal.
	-i: Ignore case differences.
	-l: Output line numbers of differing bytes.
	Example:
	cmp file1.txt file2.txt: Compares file1.txt and file2.txt and displays an error message if they differ.
	cmp -l file1.txt file2.txt: Displays the line numbers of any differing bytes.
2. comm

	Purpose: Compares lines in two sorted files.
	Syntax: comm [OPTION]... FILE1 FILE2
	Common Options:
	-1: Suppress column 1 (lines unique to FILE1).
	-2: Suppress column 2 (lines unique to FILE2).
	-3: Suppress column 3 (lines common to both files).
	Example:
	comm file1.txt file2.txt: Displays three columns: lines unique to FILE1, lines unique to FILE2, and lines common to both files.
	comm -13 file1.txt file2.txt: Displays only the lines that are unique to FILE2.
3. diff

	Purpose: Shows the differences between two files.
	Syntax: diff [OPTION]... FILE1 FILE2
	Common Options:
	-u: Unified format output (shows changes in a concise way).
	-r: Recursively compare directories.
	-c: Context diff (shows a few lines of context around each change).
	-b: Ignore whitespace changes.
	Example:
	diff file1.txt file2.txt: Shows the differences between the two files in a standard format.
	diff -u file1.txt file2.txt: Shows the differences in a unified format.
4. patch

	Purpose: Applies a set of changes (usually generated by diff) to a file or set of files.
	Syntax: patch [OPTION]... [FILE]
	Common Options:
	-p<number>: Strip the specified number of leading directory components from file names in the patch.
	-s: Suppress diff output during patching.
	-b: Backup files before applying patches.
	Example:
	patch < my_patch.diff: Applies the patch in my_patch.diff to the current directory.
	Key Points:

	cmp: Best for byte-by-byte comparisons of files.
	comm: Useful for comparing sorted lists of lines.
	diff: The most versatile tool for comparing files and generating patches.
	patch: Applies changes generated by diff to files.
	
=======

1 cmp compares two files byte by byte and reports the first point of difference.

Syntax:

cmp [options] file1 file2
Example:
	cmp file1.txt file2.txt

Output:

If the files differ, cmp displays the byte and line number where the first difference occurs.

If the files are identical, there is no output.

Common Option:

-l: Lists all differing bytes.

2. comm

comm compares two sorted files line by line and displays three columns:

Lines unique to the first file.

Lines unique to the second file.

Lines common to both files.

Syntax:

comm [options] file1 file2

Example:

comm file1.txt file2.txt

Common Options:

-1: Suppress lines unique to the first file.

-2: Suppress lines unique to the second file.

-3: Suppress lines common to both files.

3. diff

diff displays the differences between two files line by line. It is more descriptive and widely used for text comparison.

Syntax:

diff [options] file1 file2

Example:

diff file1.txt file2.txt

Output:

Differences are shown with line numbers and markers (< for file1, > for file2).

Common Options:

-u: Unified format, displaying changes in context.

-c: Context format, showing more detailed differences.

4. patch

patch applies changes to a file using a patch file, typically generated by diff.

Syntax:

patch [options] original_file patch_file

Example:

Generate a patch file:

diff -u file1.txt file2.txt > changes.patch

Apply the patch:

patch file1.txt < changes.patch

Output:

The original file is updated with the changes specified in the patch.

Common Options:

-pN: Strips N leading components from file paths in the patch file.

>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• File utilities: wc, join
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

<<<<<<< HEAD
1. wc (Word Count)

	Purpose: Counts lines, words, and characters in a file or input.
	Syntax: wc [OPTION]... [FILE]...
	
	cmp -l test.txt test1.txt
		5  12  61

	5th character is different 
	12: This is the byte offset (position) within line 
	
	Common Options:
		-l: Counts the number of lines.
	

		man cmp 
		
	--files0-from=FILE: Reads input from the list of files specified in FILE. Each filename is terminated by a null character.
	Example:
		wc myfile.txt: Displays the number of lines, words, and characters in myfile.txt.
		wc -l myfile.txt: Displays only the number of lines in myfile.txt.
		wc -c myfile1.txt myfile2.txt: Displays the character count for both files.
2. join

	Purpose: Joins lines from two files on a common field.
	Syntax: join [OPTION]... FILE1 FILE2
	Common Options:
		join file1.txt file2.txt > file3.txt 
			not working in amazon linux 
	
	
	wc: A simple yet powerful tool for quickly analyzing the content of files.
		wc test.txt 
			output: lines words bytes
	
=======
Purpose: Counts the number of lines, words, and characters in a file.

Syntax: wc [OPTION]... [FILE]...

Common Options:

-l: Counts the number of lines.
-w: Counts the number of words.
-c: Counts the number of characters.
-L: Prints the length of the longest line.
-m: Counts the number of characters (same as -c).
--files0-from=FILE: Reads input from the list of files delimited by the null character.
Examples:

wc myfile.txt: Displays the number of lines, words, and characters in "myfile.txt".
wc -l myfile.txt: Displays only the number of lines in "myfile.txt".
wc -w myfile.txt: Displays only the number of words in "myfile.txt".
wc -c myfile.txt: Displays only the number of characters in "myfile.txt".
wc -L myfile.txt: Displays the length of the longest line in "myfile.txt".
join

Purpose: Joins lines from two files based on a common field.

Syntax: join [OPTION]... FILE1 FILE2

Common Options:

-1 FIELD: Specifies the field number to join on in FILE1.
-2 FIELD: Specifies the field number to join on in FILE2.
-a FILE: Display unpairable lines from FILE.
-e FIELD: Replace missing fields with FIELD.
-o FORMAT: Output fields in the specified format.
Example:

Let's say you have two files:

file1.txt: Contains a list of IDs and names (e.g., "1 John", "2 Jane", "3 David")
file2.txt: Contains a list of IDs and ages (e.g., "1 25", "2 30", "3 28")
To join these files based on the ID:

Bash

join -1 1 -2 1 file1.txt file2.txt
This command will join the lines in file1.txt and file2.txt based on the first field (ID) in each file, producing an output like:

1 John 25
2 Jane 30
3 David 28

>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Pattern locating command: grep, fgrep, egrep
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	grep: “Global Regular Expression Print“. 
	Used for searching content from files based on a pattern or regular expression.

			Synatx:
			grep "PATTERN" [FILE]
			
		1. Search all users under /etc/passwd have the   shell.
			grep " " /etc/passwd
			similar to cat as all lines will include " ". but if any line doesn't inlcude shell it wil be missed.
			

		2. Grep command can take the output of another command as input using pipes. 
		
			cat /etc/passwd | grep " "
			
			
		3. Case Sensitive Search
			Grep uses -i option to run NOT case-sensitive search.
			grep -i "SearchPattern" filename
			grep -i " " /etc/passwd

		4. Search Recursively 
			Using the -r switch grep to search for pattern recursively for all files under the specified directory and their subdirectories.

			grep -r "SearchPattern" <directory>
			grep -r " " /etc
			sudo grep -r " " /etc
			
		5. Print Matching Filename Only
			The default grep 
				prints the matching content  
			You can hide the content 
				display only filename in grep output.

			Use -l to print pattern matching filenames.

			grep -rl "SearchPattern" /home/rahul
			Use -L to revert the output. This will print only those file where no match found.

			grep -rL "SearchPattern" /home/rahul
			Print Before/After Lines of Matching Pattern
			This is a useful feature of grep command. You can print the defined number of lines just before line matches the pattern or just after lines of matches pattern.

			Use -A followed by number of lines to print lines before the matching pattern line.

			grep -A2 "SearchPattern" myfile.csv
			Use -B followed by number of lines to print lines after the matching pattern line.

			grep -B2 "SearchPattern" myfile.csv
			Use -C followed by number of lines to print lines before and after the matching pattern line.

			grep -B2 "SearchPattern" myfile.csv
			Tagsgrep, grep command, linux command
			Doc navigation← Linux - teeLinux - wc →

	
		
			grep -v "pattern" - Inverse search; display lines that do not match the pattern
			
			grep -C 2 "pattern" - Display lines before and after the matched pattern
			
			grep -rli "pattern" . - Search for a pattern in files and display matching filenames only
		
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Translate : tr
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
<<<<<<< HEAD
	The tr command 
		tool for translating or deleting characters from standard input. 
		It's incredibly useful for text manipulation tasks. 
		Here's a breakdown of its usage and common options:

Basic Syntax:

 

tr [OPTION]... SET1 [SET2]
	SET1: The set of characters to be translated or deleted.
	SET2: (Optional) The set of characters to replace SET1 with. If SET2 is omitted, characters in SET1 are deleted.
Common Options:

-d: Delete characters in SET1.
-s: Squeeze repeated characters in SET1 to a single character.
-c: Complement SET1 (translate or delete characters not in SET1).
-t: Same as SET2 (for clarity).
-d: Delete characters in SET1.
Examples:

Convert lowercase to uppercase:

 

	echo "hello world" | tr '[:lower:]' '[:upper:]' 
Output: HELLO WORLD

Delete all vowels:

 

	echo "hello world" | tr -d 'aeiouAEIOU'
Output: hll wrld

	Replace 'e' with 'E' and 'o' with 'O':

 

		echo "hello world" | tr 'eo' 'EO'
Output: HeLlO wOrld

Squeeze repeated spaces to a single space:

 

	echo "   hello   world  " | tr -s ' ' 
Output: " hello world "

Delete all non-alphanumeric characters:

 

	echo "hello, world! 123" | tr -d '[:punct:]' 

Output: helloworld123

By combining these options, you can create complex character translations and transformations to manipulate text data effectively.

Key Points:

tr works on character streams, so it's often used in conjunction with other commands like 
	echo, cat, grep, and sed.

The [:lower:], [:upper:], [:digit:], [:punct:], [:space:] character classes are often used within SET1 and SET2.





=======

Purpose:

It's a command-line utility used to translate or delete characters from standard input (or a file) and write the results to standard output.
It can be used for various text manipulation tasks, such as:
Character translation: Replace one set of characters with another.
Character deletion: Remove specific characters from input.
Character squeezing: Replace multiple occurrences of a character with a single occurrence.
Basic Syntax:

Bash

tr [OPTION]... SET1 SET2
SET1: Specifies the set of characters to be translated or deleted.
SET2: Specifies the set of characters to be substituted for the characters in SET1. If SET2 is shorter than SET1, the last character in SET2 is repeated.
Common Options:

-d: Delete characters in SET1.
-s: Squeeze repeated characters in SET1.
-c: Complement SET1 (translate or delete characters not in SET1).
-t: Same as -s (squeeze repeated characters).
-squeezeset: Squeeze repeated characters in SQUEEZESET.
Examples:

Translate lowercase letters to uppercase:

Bash

echo "hello world" | tr 'a-z' 'A-Z' 
Delete all vowels:

Bash

echo "hello world" | tr -d 'aeiouAEIOU'
Replace all spaces with underscores:

Bash

echo "hello world" | tr ' ' '_'
Squeeze multiple spaces into a single space:

Bash

echo "hello   world" | tr -s ' ' 
>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
INPUT/OUTPUT REDIRECTION & PIPES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


<<<<<<< HEAD
1. < (Input Redirection)

Purpose: Directs the input of a command to come from a file instead of the keyboard.

Example:

 

cat < myfile.txt #same as cat myfile.txt  #but cat will wait for input from keyboard.
This command will display the contents of the file myfile.txt on the screen. Instead of reading input from the keyboard, the cat command will read input from the file.

2. > (Output Redirection - Overwrite)

Purpose: Directs the output of a command to a file, overwriting the file if it already exists.

Example:

 

ls -l > file_listing.txt
This command will execute the ls -l command (which lists files in long format) and redirect the output to a file named file_listing.txt. If file_listing.txt already exists, it will be overwritten.

3. >> (Output Redirection - Append)

Purpose: Directs the output of a command to a file, appending the output to the end of the file if it already exists.

Example:

 

date >> system_log.txt
This command will execute the date command and append the current date and time to the end of the file system_log.txt. If system_log.txt doesn't exist, it will be created.

Key Concepts

Standard Input (stdin): Typically refers to the keyboard.
Standard Output (stdout): Typically refers to the screen.
Standard Error (stderr): Used for displaying error messages.
=======


echo Testing >> test.txt











>>>>>>> 2be8a02f92b4b012c49f216a0a2235bd6e050fd1

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Concepts of file descriptors: Standard input, output and error files
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• 	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Input redirection
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Output redirection
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Error redirection
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	

1. Standard Error (stderr)

By default, error messages from commands are sent to standard error (stderr).
This is a separate stream from standard output (stdout), which is typically used for regular output. 
2. Redirecting stderr (2>)

2>: This symbol specifically redirects standard error to a file or another stream.

Example:

 

command 2> error.log 
	This command executes the command and redirects any error messages to a file named "error.log".

3. Redirecting both stdout and stderr

&> or >: This redirects both standard output (stdout) and standard error (stderr) to the same destination.

Example:

 

	command &> output.txt
This command redirects both the normal output and any error messages from the command to the file "output.txt".

 

	command > output.txt 2>&1 
This is another way to achieve the same result. It first redirects standard output to "output.txt" and then redirects standard error to the same destination as standard output. 

4. Combining Redirection:

You can combine redirection to send standard output to one file and standard error to another:

 

command > output.txt 2> error.log
Key Concepts:

File Descriptors: In Linux, standard input, standard output, and standard error are represented by file descriptors:
stdin: 0
stdout: 1
stderr: 2 
 
Flexibility: Error redirection provides flexibility in handling both successful and unsuccessful command executions.
By understanding and utilizing error redirection, you can effectively manage the output of commands, isolate error messages for debugging, and create cleaner and more organized scripts.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Usage of pipes and tee command
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Used to route output data to multiple outputs. 
	For e.g. Tee can display output on 
		STDOUT as well ass 
		write to file at a time.
	
	1. The following command will 
		display list of files in current directory on screen and
		write in list.txt file.

		ls | tee list.txt
		
	2. Append Data to File
		The default tee overwrites the file content. You can use -a to append data to file.

		ls | tee –a list.txt
		
	3. Write data to Multiple File
		You can also write the output to multiple files in a single command. The example is below

		ls | tee list1.txt list2.txt



	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
FILE ACCESS PERMISSIONS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Permission Groups
		Each file and directory has three user based permission groups:

		owner – 
			The Owner permissions 
				apply only to the owner of the file or directory, 
				will not impact the actions of other users.
		group – 
			The Group permissions 
				apply only to the group that has been assigned to the file or directory
				will not effect the actions of other users.
		all users – 
			All Users permissions 
				apply to all other users on the system
				Be cautious of this from a security point of view.

	Permission Types
		Each file or directory has three basic permission types:

		read – 
			User’s capability to read the contents of the file.
		write
			User’s capability to write or modify a file or directory.
		execute – 
			User’s capability to execute a file or 
			view the contents of a directory.


	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Long listing of files: ls -l
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	View the permissions by executing “ls -l” command 
		Will list files in current directory

		The permission in the command line is displayed as: _rwxrwxrwx 1 owner:group

		First Character: User rights/Permissions
			(_ here) is the special permission flag that can vary.
		The following set of three characters (rwx) 
			owner permissions.
		The second set of three characters (rwx) 
			Group permissions.
		The third set of three characters (rwx) 
			All Users permissions.
		Following that grouping since the integer/number 
			displays the number of hardlinks to the file.
		last piece 
			Owner and Group assignment formatted as Owner:Group.


	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Types of users: owner, group and others
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	covered
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Types of permissions: read, write and execute
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	covered
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Changing file access permission: chmod
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Modifying the Permissions
		From command line, 
			the permissions are  ed by using the command chmod. 
			You can assign the permissions explicitly or by using a binary reference as described below.

		Explicitly Defining Permissions
		-------------------------------
		To explicity define permissions 
			we define the permission for Permission Group and Permission Types.

		The Permission Groups used are:
			u – Owner
			g – Group
			o – Others
			a – All users

		The potential Assignment Operators are 
			+ (plus) and 
			– (minus); 
			these are used to tell the system whether to add or remove the specific permissions.

		The Permission Types that are used are:

		r – Read
		w – Write
		x – Execute
		
		If file file1 currently has the permissions 
			_rw_rw_rw, 
				owner, 
				group  
				all users 
					all have read and write permission. 
		To remove the read and write permissions from the all users group.
			chmod a-rw file1
		To add the permissions above you would invoke the command: 
			chmod a+rw file1


		Using Binary References To Set Permissions
		------------------------------------------
		Another option to set the permission 
			using binary references 

		__rw_rw_rw
		 011011011
		3	3	3

		First number 
			represents the Owner permission; 
		second represents 
			the Group permissions; 
		last number represents 
			the permissions for all other users. 

		r = 4
		w = 2
		x = 1
		You add the numbers to get the integer/number representing the permissions you wish to set. You will need to include the binary permissions for each of the three permission groups.

		To set a file to permissions on file1 to read 
			_rwxr_____, 
			 7	4	0 
			 
			chmod 740 file1 




		Owners and Groups
		-----------------
		Owner and Group assigned to a file or directory can be modified.

		chown command 
			Used to change owner and group assignments
		
			chown owner:group filename
			
		so to change the owner of file1 to user1 and the group to dev  
			chown user1:dev file1.

		Advanced Permissions
		--------------------
		The special permissions flag can be marked with any of the following:

		"_": no special permissions
		d:	directory
		l: 	The file or directory is a symbolic link
		s:	setuid/setgid permissions. 
			Not set displayed in the special permission part of permissions display, 
			Represented as s in the read portion of the owner or group permissions.
		t – indicates the sticky bit permissions. 
			Not set displayed in the special permission part of the permissions display
			Represented as t in the executable portion of the all users permissions
		
		chmod +x filename - Make a file executable
    
		
		Setuid/Setgid Special Permissions
		---------------------------------
		setuid/setguid permissions 
			used to tell the system to run an executable as the owner with the owner’s permissions.
			This was we can allow users execute few commands with escalated privilege without giving sudo privilege.
			
		Be careful using setuid/setgid bits in permissions. 
		If you incorrectly assign permissions to a file owned by root with the setuid/setgid bit set
		then you may open your system to intrusion.

		You can only assign the setuid/setgid bit by explicitly defining permissions. 
		The character for the setuid/setguid bit is s.

		So do set the setuid/setguid bit on file2.sh you would issue the command 
			chmod g+s file2.sh.

		Sticky Bit Special Permissions
		------------------------------
		The sticky bit can be very useful in shared environment because when it has been assigned to the permissions on a directory it sets it so only file owner can rename or delete the said file.

		You can only assign the sticky bit by explicitly defining permissions. The character for the sticky bit is t.

		To set the sticky bit on a directory named dir1 you would issue the command 
			chmod +t dir1.
			Users with sticky bit permission would be able to add content into it but not delete.

		When Permissions Are Important
		------------------------------
		To some users of Mac- or Windows-based computers you don’t think about permissions, but those environments don’t focus so aggressively on user based rights on files unless you are in a corporate environment. But now you are running a Linux-based system and permission based security is simplified and can be easily used to restrict access as you please.

		So I will show you some documents and folders that you want to focus on and show you how the optimal permissions should be set.

		home directories– The users’ home directories are important because you do not want other users to be able to view and modify the files in another user’s documents of desktop. To remedy this you will want the directory to have the drwx______ (700) permissions, so lets say we want to enforce the correct permissions on the user user1’s home directory that can be done by issuing the command chmod 700 /home/user1.
		bootloader configuration files– If you decide to implement password to boot specific operating systems then you will want to remove read and write permissions from the configuration file from all users but root. To do you can change the permissions of the file to 700.
		system and daemon configuration files– It is very important to restrict rights to system and daemon configuration files to restrict users from  ing the contents, it may not be advisable to restrict read permissions, but restricting write permissions is a must. In these cases it may be best to modify the rights to 644.
		firewall scripts – It may not always be necessary to block all users from reading the firewall file, but it is advisable to restrict the users from writing to the file. In this case the firewall script is run by the root user automatically on boot, so all other users need no rights, so you can assign the 700 permissions.


	Other commands chgrp and chattr
	https://tecadmin.net/tutorial/linux/linux-file-permissions/
		chgrp - Change group ownership of a file or directory
    


	


	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Default file access permission: umask
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

What is umask?

Umask stands for 
	"user mask." 
	restricts the default permissions assigned to 
		newly created files and directories.

Permissions: 
	File permissions in Linux are typically represented by three sets of characters (user, group, others), 
		each consisting of three possible permissions:

Default Permissions:
	r: Read
	w: Write
	x: Execute


Files: 
	By default, 
		new files created with permissions of 666 (read, write, and execute for user, group, and others).
	Directories: 
		By default, new directories are created with permissions of 777 
			(read, write, and execute for user, group, and others).

How umask Works:

	The umask value is a three-digit octal number (e.g., 022, 0022). Each digit represents the permissions that are not granted to the user, group, and others, respectively, when a new file or directory is created.

Example:

	umask 
	umask 0543 
	umask 022:
	For files: 666 (default) - 022 (umask) = 644 (read and write for owner, read for group and others).

	For directories: 777 (default) - 022 (umask) = 755 (read, write, and execute for owner, read and execute for group and others).
Key Points:

	Security: 
		A well-configured umask is crucial for system security. By restricting default permissions, you can prevent unintended file access.
	Customization: 
		You can adjust the umask value to suit your specific security needs and work environment.
	System-wide vs. User-specific: 
		The umask can be set system-wide (affecting all users) or on a per-user basis.
In Summary:

The umask command plays a vital role in controlling file permissions in Linux, helping to enhance system security and prevent unauthorized access to files and directories. By understanding and appropriately setting the umask, you can effectively manage file access within your system.


	









	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Outside syllabus: systemctl commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	The systemctl command is a new tool to control the systemd system and service. 
	Replacement of old SysV init system management. 
	Most of modern Linux operating systems are using this new tool. 
	
	systemctl [OPTION] [SERVICE]
	
	sudo systemctl start mysql.service
	sudo systemctl stop mysql.service
	
	
	sudo systemctl reload mysql.service
	sudo systemctl restart mysql.service
	sudo systemctl reload-or-restart mysql.service

	sudo systemctl status mysql.service
	sudo systemctl enable mysql.service
	sudo systemctl disable mysql.service
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
LINKING FILES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Concept of file links, inode
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Inodes

		Inodes, or index nodes, 
			data structure in Linux that 
			store metadata about files. 
		They are essential for the operating system to function properly. 
	What do inodes store? 
		File size: The size of the file
		Storage location: The storage device and location of the file
		Access permissions: The permissions for accessing the file
		Owner information: The owner of the file
		Timestamps: The timestamps for the file
	How do inodes work?
		Each file system object has one inode 
		Inodes are stored in blocks, similar to how files are stored in units of a given size 
		Inodes are unique identifiers for each piece of metadata on a filesystem 
	What happens if there are too many inodes?
		If all inodes are used, no new files or directories can be created 
		This can happen if there are too many log files or session files 
		To resolve this, delete unnecessary files or directories to free up inodes 



	How to view inode numbers of files in a directory 
		Use the ls -i or ls –inode command
	The inode numbers for each file in the directory will be displayed
	How to view the total number of inodes on your system 
		Use the df -i command
	The total number of inodes on your system will be displayed
	How to view the inode of a directory 
		Use the ls -i command with the -d and -l options
	The inode of the directory will be displayed


File Links
----------

Hard Links:

	Multiple names (or links) 
		point to the same inode.   
	Deleting one hard link 
		does not delete the file itself, 
		as long as at least one other hard link exists.   
	Limitations:
		Cannot create hard links across different file systems.   
		Cannot create hard links to directories.

	Symbolic Links (Soft Links):

		A special type of file that contains the path to another file.   
		Deleting the original file will break the symbolic link.
		Can link to files across different file systems.   
		Can link to directories.   
In Summary

Inodes are the core data structures that hold metadata about files within a file system.   
File links provide different ways to refer to the same file data.
Hard links provide multiple names for the same file within the same file system.   
Symbolic links are more flexible and can point to files across different file systems.   
Key Concepts

Soft Links (Symbolic Links)
------------------------
		A soft link 
			like a shortcut. 
			It's a separate file 
				points to the path of the original file.  
		Think of it like a web bookmark or a Windows shortcut. 

	Key characteristics:

		Can link across file systems. 
		Can link to directories. 
		If the original file is deleted or moved, the soft link becomes broken (it no longer points to a valid location).
		Uses a different inode than the original file. 
	Creating a soft link:

	 
		ln -s /path/to/original_file link_name

		Example: ln -s /home/user/myfile.txt mylink.txt

		Verifying a soft link:

			ls -l link_name: 
				The output will show an l 
					at the beginning of the permissions, 
						indicating a symbolic link. 
				It will also show where the link points to. 
			Example: lrwxrwxrwx 1 user user 17 Oct 26 10:00 mylink.txt -> /home/user/myfile.txt
			readlink link_name: 
				This command will specifically show the target of the symbolic link.
Hard Links
---------
	A hard link 
		two file names 
		both point to the same inode 
			(the underlying data block on the disk).  
		It's as if the file has two (or more) names. 

	Key characteristics:

		Cannot link across file systems. 
		Cannot link to directories (generally, although some systems allow it).
		If you delete a hard link, 
			the file still exists 
				as long as at least one hard link points to it. 
			The file is only truly deleted when the last hard link is removed. 
		Shares the same inode as the original file. 
	Creating a hard link:
	--------------------
	 

	ln /path/to/original_file link_name
	Example: ln /home/user/myfile.txt myothername.txt

	Verifying a hard link:

		ls -li: 
			The -i option 
				shows the inode number. 
				Hard links will have the same inode number. 
	Example:
		123456 -rw-r--r-- 2 user user 1024 Oct 26 10:00 myfile.txt
		123456 -rw-r--r-- 2 user user 1024 Oct 26 10:00 myothername.txt
	Notice that both myfile.txt and myothername.txt 
		have the inode number 123456. 
		The '2' in the third column indicates there are two links to this inode. 


	Feature			Soft Link (Symbolic Link)			Hard Link
	Type			Separate file pointing to a path	Another name for the same file (same inode)
	File System		Can cross file systems				Cannot cross file systems
	Directories		Can link to directories				Generally cannot link to directories
	Deletion of Original	Breaks the link				Doesn't affect other hard links
	Inode			Different inode						Same inode


	When to use hard link 
		When the primary file is delete 
			if you still need the file 


	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Types of linking files: hard links and soft/symbolic links
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	Hard Links:

What they are:
	Multiple names (or links) that point to the same inode of a file. 
Essentially, they are different entries in the file system that all point to the same underlying file data.
Key Characteristics:
Cannot link to directories: Only applicable to regular files. 
Cannot cross file systems: Hard links must reside within the same file system. 
Deleting a hard link: Does not delete the file itself. The file is only deleted when all hard links to it are removed. 
Analogy: Imagine a book with multiple labels. Each label is a hard link, pointing to the same physical book.
Soft Links (Symbolic Links)

What they are:
A special type of file that contains the path to another file (or directory). 
They are essentially shortcuts. 
Key Characteristics:
Can link to anything: Can point to files, directories, or even other symbolic links, even across different file systems.   
Deleting the original file: Makes the symbolic link "broken" or invalid.   
Analogy: A sticky note with the location of another document. If the original document is moved or deleted, the sticky note becomes useless.
Here's a table summarizing the key differences:

------------------------------------------------------------------------------------------------
Feature		Hard Link										Soft Link
------------------------------------------------------------------------------------------------
Type		Multiple names for the same inode				A separate file containing a path
Data		Points directly to file data					Points to the path of the file
Directories	Cannot link to directories						Can link to directories
Cross-FS	Cannot cross file systems						Can cross file systems
Deletion	Deleting a hard link doesn't delete the file	Deleting the original file breaks the link
------------------------------------------------------------------------------------------------
 
In essence:

	Hard links: 
		Multiple names for the same file data.   
	Soft links: 
		Shortcuts to other files (or directories)
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Purpose of linking files
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	1. Disk Space Savings:

	Hard Links: 
		If multiple users or applications need to access the same large file 
			(like a database or a shared library), 
			creating hard links allows them to 
				share the same physical data on disk, 
				saving significant disk space. 
2. Convenience and Organization:

Symbolic Links:
	Shortcuts: 
		Create shortcuts to 
			frequently accessed files or directories, 
			making them easier to access. 
	Centralized Locations: 
		Link files from various locations 
			to a central directory for easier management and backup.
	Consistent Access: 
		Ensure that applications 
			always access the correct version of a file, 
			even if its location changes.
3. Application Development:

	Libraries: 
		Symbolic links are 
			often used to link libraries and other shared files to specific directories, making it easier to manage dependencies and update software.

	 
4. System Administration:

	Configuration Files: 
		System administrators can use symbolic links to 
			create multiple "copies" of configuration files in different locations, 
			making it easier to manage and maintain system settings.
In summary:

	File linking provides a flexible and efficient way to manage files and directories within the file system. By creating links, you can save disk space, improve organization, and simplify file access and management. 


Sources and related content

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
COMMUNICATION
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Offline & Online communication commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Sending mails
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Receiving mails
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Chatting using write
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
PROCESS MANAGEMENT
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Concept of processes
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Process statistics: ps
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
		The command 
			ps aux in Linux 
				display information about the processes currently running on the system. 
				
		ps: 
			This is the core command, 
			short for "process status." 
			It's used to display information about running processes.

		a: 
			display all processes, 
			including daemons 
				those that do not have a controlling terminal (e.g., daemons).

		u: 
			display more detailed information about each process, 
			such as the 
				user ID, 
				CPU and 
				memory usage, and 
				start time.

		x: 
			includes deamons
				processes that do not have a controlling terminal in the output.

		What ps aux typically displays:

			USER: The user who owns the process.
			PID: The Process ID, a unique identifier for each process.
			%CPU: The percentage of CPU time used by the process.
			%MEM: The percentage of memory used by the process.
			VSZ: Virtual Memory Size (in kilobytes).
			RSS: Resident Set Size (in kilobytes), the amount of memory currently used by the process.
			TTY: The terminal associated with the process.
			STAT: The process status (e.g., "S" for sleeping, "R" for running, "Z" for zombie).
			START: The time when the process was started.
			TIME: The total CPU time used by the process.
			COMMAND: The command that started the process.

		Example:

		USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
		root         1  0.0  0.0   892   572 ?        Ss   17:07   0:00 /init
		root       227  0.0  0.0   900   80 ?        S    17:07   0:00 /init
		root       228  0.0  0.0   900   88 ?        S    17:07   0:00 /init
		zaphod     229  0.0  0.1 749596 31000 pts/0    Ssl+ 17:07   0:15 docker
		Using ps aux

			Identify Resource-Intensive Processes: 
				By examining 
					%CPU and 
					%MEM columns, 
						you can identify processes that are consuming a significant amount of system resources.
			Troubleshoot System Issues: 
				ps aux 
					can help you identify and investigate processes 
						that may be causing system slowdowns or other problems.
			Monitor System Activity: You can use ps aux to get a snapshot of the current activity on your system.
			Note: The output of ps aux can be quite lengthy. You can pipe the output to other commands like grep or head to filter and display specific information:

		ps aux | grep "chrome": Displays processes related to the Chrome browser.
		ps aux | head -n 5: Displays the first five lines of the output.
		By using ps aux effectively, you can gain valuable insights into the processes running on your system and effectively manage system resources.
	ps -aux | grep "process" - Search for a specific process
	ps -ef | grep docker | awk -F " " '${print $2}'
	
	
			
    ps -eLf - Display threads for all processes
		The command ps -eLf in Linux provides a detailed view of all running processes on the system, including their threads. Let's break down what each option signifies:

		ps: This is the core command, short for "process status." It's used to display information about running processes.

		-e: This option tells ps to display information about all processes running on the system, including those that do not have a controlling terminal (e.g., daemons).

		-l: This option displays a long listing of process information, providing more details than the default output.

		-f: This option displays the full command line for each process, which can be helpful for identifying and understanding the purpose of a process.

		Key Information Displayed by ps -eLf:

		F: Flags associated with the process.
		UID: User ID of the process owner.
		PID: Process ID, a unique identifier for each process.
		PPID: Parent process ID.
		C: CPU utilization percentage.
		STIME: Start time of the process.
		TTY: Terminal associated with the process (if any).
		TIME: CPU time used by the process.
		CMD: Full command line used to start the process.
		Example:

		F   UID        PID  PPID  C   STIME TTY          TIME CMD
		4096 root       1     0  0 17:03 ?        00:00:00 /sbin/init
		4096 root       2     1  0 17:03 ?        00:00:00 [kthreadd]
		4096 root       3     2  0 17:03 ?        00:00:00 [ksoftirqd/0]
		Using ps -eLf

		Identify Resource-Intensive Processes: By examining the %CPU and %MEM columns, you can identify processes that are consuming a significant amount of system resources.
		Troubleshoot System Issues: ps -eLf can help you identify and investigate processes that may be causing system slowdowns or other problems.
		Analyze Process Relationships: The PPID column allows you to see the parent-child relationships between processes.
		Note: The output of ps -eLf can be quite lengthy. You can pipe the output to other commands like grep or head to filter and display specific information.

		By using ps -eLf, you can gain a comprehensive understanding of the processes running on your system, including their relationships and resource utilization.

		ps -e | awk '{print $1}'  | sort pcpu 
			filter command output using awk 
			
			
		ps -aux Vs ps -eLf 
		------------------
ps -aux

Purpose: Provides a comprehensive overview of all running processes, including those run by other users and those without a controlling terminal.
Output Format:
	USER: The username of the process owner.
	PID: The Process ID (a unique identifier for each process).
	%CPU: The percentage of CPU time the process is using.
	%MEM: The percentage of memory the process is using.
	VSZ: Virtual Memory Size (in kilobytes).
	RSS: Resident Set Size (in kilobytes), the actual physical memory the process is using.
	TTY: The terminal associated with the process (if any).
	STAT: The process state (e.g., sleeping, running, zombie).
	START: The time the process started.
	TIME: The total CPU time the process has used.
	COMMAND: The command that launched the process.
Key Features:
Displays processes from all users.
Includes processes without a controlling terminal (daemons, etc.).
Shows a good balance of information about resource usage and process state.
ps -eLf

Purpose: Offers a more detailed view of processes, including thread information and security context.
Output Format:
	F: Flags associated with the process.
	UID: User ID of the process owner.
	PID: Process ID.
	PPID: Parent Process ID.
	C: CPU utilization percentage.
	STIME: Start time of the process.
	TTY: Terminal associated with the process.
	TIME: CPU time used by the process.
	CMD: Full command line used to start the process.

Key Features:
	Displays all processes (like ps -aux).
	Includes thread information (the L in -eLf is for "threads").
	Shows the parent process ID (PPID), which is helpful for understanding process relationships.
	Provides more detailed information about the process state and flags.
When to use which:

ps -aux: 
	Use this when you want a general overview of running processes, 
		especially 
			if you're looking for resource-intensive processes 
			or 
			trying to identify a specific process by its name.
ps -eLf: 
	Use this when you need more detailed information, 
		such as thread information, 
		parent process IDs, or a 
		deeper understanding of the process state. 
		It's also useful for debugging and analyzing process behavior.		
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Child and parent processes
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	In Linux, processes are organized in a hierarchical tree structure, with each process having a parent-child relationship.

Parent Process: The process that initiates or creates another process.
Child Process: A process that is created by a parent process.
Key Concepts:

fork() System Call: The primary mechanism for creating child processes. When a process calls fork(), a new process is created that is an exact copy of the parent process (including memory, open files, etc.).
Process ID (PID): Each process has a unique PID. The parent process's PID is known as the PPID (Parent Process ID) of the child process.
Process Tree: This hierarchical structure forms a tree-like diagram, where the initial process (often called init) is the root, and all other processes are its descendants.
Example:

You log in to your Linux system. The login process creates a shell process (e.g.,  ).
The shell process is the parent process of any commands you execute within it.
When you run a command like ls, the shell creates a child process to execute the ls command.
If ls encounters a symbolic link, it might create another child process to resolve the link.
Benefits of the Parent-Child Relationship:

Resource Management: The parent process can control and manage the resources allocated to its child processes.
Error Handling: The parent process can handle errors that occur within its child processes.
Data Sharing: In some cases, parent and child processes can share memory or other resources.
Viewing Process Relationships:

ps -ef: This command provides a detailed view of processes, including their parent process IDs (PPIDs).
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Running process in background process - &
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	The ampersand symbol (&) in Linux commands is used to run a process in the background.

Here's how it works:

Foreground vs. Background:
	Foreground: 
		When you execute a command without the ampersand, 
			it runs in the foreground. 
			shell waits for the command to complete 
				before allowing you to enter further commands. 
		You cannot interact with the shell while the foreground process is running.
	Background: 
		Appending & to a command 
			tells the shell to execute it in the background. 
			shell immediately returns control to you, 
			allow you to continue using the terminal while 
				the command runs independently.
Example:

Foreground:

 

gzip large_file.tar 
	You would have to wait for the gzip command to finish before you can enter another command.

Background:

 

gzip large_file.tar &
	The gzip command starts executing in the background. The shell displays the Process ID (PID) of the background process, and you can immediately continue using the terminal for other tasks.

Key Benefits:

	Improved Efficiency: Run long-running tasks without blocking your workflow.
	Multitasking: Run multiple processes concurrently.
Important Notes:

	Output: If a background process produces output to the terminal, it may interfere with your current work. You can redirect the output to a file using > or >> to avoid this.
	Job Control: You can manage background processes using commands like jobs, fg, and bg.
	By using the & symbol, you can significantly improve your productivity and efficiency when working in the Linux terminal.
		
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Suspending process
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	In Linux, you can suspend a foreground process using Ctrl+Z.

What it does:

	Pressing Ctrl+Z 
		while a process is running 
			in the foreground sends the 
			SIGTSTP signal to the process.
	This signal suspends the process and moves it to the background.
After Suspending:

	The shell will inform you that the process has been stopped and 
		assign it a job ID.
	You can then use the following commands to manage the suspended process:
	jobs: 
		Lists all suspended jobs.
	fg: 
		Brings a suspended job back to the foreground to continue running.
	You can specify the job ID with fg %<job_id> (e.g., fg %1 for the first suspended job).
	bg: 
		Moves a suspended job back to the background to run independently.
Example:

Run a command: vim myfile.txt
	Suspend the process: Press Ctrl+Z while editing the file in Vim.
	Check suspended jobs: jobs (You'll see Vim listed as a suspended job)
	Resume in foreground: fg %1 (if Vim is the first suspended job)
	Resume in background: bg %1
Key Points:

	Suspending a process allows you to temporarily pause it and continue working on other tasks.
	You can later resume the suspended process or move it back to the background.
	Job control commands like jobs, fg, and bg are essential for managing suspended processes.
	By effectively using Ctrl+Z and the job control commands, you can efficiently manage your workflow in the Linux terminal.	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Resuming process in foreground/background mode
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

https://www.redhat.com/en/blog/jobs-bg-fg
	
	Resuming a Suspended Process

You can resume a suspended process in either the foreground or background using the following commands:

fg:

	Brings a suspended job back to the foreground.

	You can specify the job ID with fg %<job_id> (e.g., fg %1 for the first suspended job).
	Once in the foreground, you can interact with the process as if you had started it normally.
bg:

	Moves a suspended job back to the background to run independently.
	You can specify the job ID with bg %<job_id> (e.g., bg %2 for the second suspended job).
Example

Suspend a process:

Start a process (e.g., vim myfile.txt).
Press Ctrl+Z to suspend it.
Check suspended jobs:

Use the jobs command to see a list of suspended jobs and their IDs.
Resume in the foreground:

Use fg %<job_id> (e.g., fg %1) to bring the suspended process back to the foreground and continue working with it.
Resume in the background:

Use bg %<job_id> (e.g., bg %1) to send the suspended process back to the background to run independently.
Key Points:

Job Control: These commands are essential for managing suspended processes effectively.
Flexibility: You can easily switch between foreground and background execution for your processes as needed.
By mastering these commands, you can efficiently manage your workflow and improve your productivity in the Linux terminal.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Terminating process
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The kill command in Linux is used to terminate processes. Here's how it works:

1. Identifying the Process:

	Find the Process ID (PID): You need to know the PID of the process you want to terminate. You can find this using commands like:
	ps aux | grep "process_name" (e.g., ps aux | grep firefox)
	top (and then select the process and press 'k')
2. Sending Signals:

	kill [signal] PID:
	[signal]: The signal to send to the process.
	SIGTERM (15): The default signal. Requests graceful termination, allowing the process to clean up before exiting.
	SIGKILL (9): Forces immediate termination. Cannot be ignored by the process. Use with caution, as it can lead to data loss.
	SIGINT (2): Simulates a keyboard interrupt (Ctrl+C).
	Examples:

	kill 1234: Sends the default signal (SIGTERM) to the process with PID 1234.
	kill -9 1234: Forcibly terminates the process with PID 1234 using the SIGKILL signal.
	kill -15 1234: Sends the SIGTERM signal (equivalent to kill 1234).
	Important Notes:

	Use with Caution: Always be cautious when terminating processes, especially with SIGKILL. Incorrectly terminating critical system processes can cause instability or data loss.
	Alternatives:
	pkill: Allows you to kill processes based on their names or patterns.
	killall: Kills all processes with a specific name.
By understanding the kill command and its options, you can effectively manage running processes on your Linux system.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
SCHEDULING PROCESS/JOB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Terminal information
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Scheduling jobs: at
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Managing repeating jobs: crontab
		- listing
		- modifying
		- removing
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	crontab is a powerful tool in Linux/Unix systems for scheduling tasks to run automatically at specific times or intervals. Here's how to manage crontab entries:

1. Listing Crontab Entries:

crontab -l: This command lists all the currently scheduled tasks for the current user.
2. Modifying Crontab Entries:

crontab -e: This command opens the crontab file in your default text editor.
Crontab File Format:

Each line in the crontab file represents a single scheduled task.
The format of each line is:
* * * * * command
Where:
Minute (0-59)
Hour (0-23)
Day of the month (1-31)
Month (1-12 or JAN-DEC)
Day of the week (0-7 or SUN-SAT; 0 or 7 is Sunday)
Command to be executed
Example:

0 0 * * * /usr/bin/find /var/tmp -mtime +30 -exec rm {} \; 
This command runs every day at midnight and deletes files in /var/tmp that are older than 30 days.

3. Removing Crontab Entries:

crontab -r: This command removes all scheduled tasks for the current user.
Important Notes:

Root Privileges: To manage system-wide cron jobs (those that affect all users), you'll need root privileges (e.g., sudo crontab -e).
Crontab File Location: User crontab files are typically stored in /var/spool/cron/crontabs/.
Testing: Test your crontab entries carefully to ensure they work as expected before relying on them for critical tasks.
By using these commands, you can effectively schedule tasks to run automatically at specific times, automating various system maintenance and administrative tasks.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	
Day – 3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
FILE ARCHIVE AND COMPRESSION
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• splitting and merging files
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Splitting Files

split Command:
This command is specifically designed for splitting files.

Syntax: split [OPTION]... INPUTFILE [PREFIX]

Common Options:

	-b SIZE: Split the input file into parts of the specified size (e.g., -b 10M for 10 megabyte files).
	-l LINES: Split the input file into parts with the specified number of lines.
	-C SIZE: Split the input file into parts with a maximum size (bytes).
Example:

	split -b 10M large_file.txt
	Splits large_file.txt into parts of approximately 10 megabytes each.
	Creates output files with names like xax, xab, xac, etc.
Merging Files

cat Command:
The simplest way to merge multiple files is using the cat command.

Syntax: cat FILE1 FILE2 ... > OUTPUTFILE

Example:

cat part1 part2 part3 > merged_file
	Concatenates the contents of part1, part2, and part3 and writes the output to a new file called merged_file.
Key Considerations:

	File Types: For binary files, it's crucial to split and merge them correctly to avoid data corruption.
	Order: When merging files, ensure they are merged in the correct order to maintain data integrity.
Example Scenario

	Split a large log file: split -b 10M large_log.txt log_part_
	Merge the split files: cat log_part_* > merged_log.txt
By using these commands, you can effectively split and merge files in Linux, making it easier to handle large files, transfer data, and manage storage space.
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• tape archive - tar
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	tar -xvf - Extract Files from a Tarball
	Command Example:
	 
	 
	 
	tar -xvf archive.tar
	Details:
	This command extracts the contents of the tar archive archive.tar.
	How it works:
	Reads the tar file and unpacks its contents into the current directory.
	Options:
	-C <directory>: Extract files to a specific directory.
	 
	 
	 
	tar -xvf archive.tar -C /path/to/destination
	
	tar -cvf - Create a tarball (archive) of files
    tar -tvf - List the contents of a tarball
    tar -xvf - Extract files from a tarball
    
	tar -czvf - Create a compressed tarball (gzip)
    tar -xzvf - Extract files from a compressed tarball (gzip)
    
	tar command in Linux is a powerful tool for creating, extracting, and manipulating archive files. Here are some of the most commonly used options:

1. Creating Archives:

	-c: Create a new archive.
	-f <archive_file>: Specify the name of the archive file.
	-z: Use gzip compression (creates a .tar.gz file).
	-j: Use bzip2 compression (creates a .tar.bz2 file).
	-v: Verbose output (displays detailed information about the archiving process).
	-p: Preserve file permissions and ownership.
Example:

	tar -czvf my_archive.tar.gz file1.txt file2.txt directory1
	Creates a compressed tar archive named my_archive.tar.gz containing the files file1.txt, file2.txt, and the directory directory1.
2. Extracting Archives:

	-x: Extract files from an archive.
	-f <archive_file>: Specify the name of the archive file.
	-v: Verbose output (displays detailed information about the extraction process).
	-C <directory>: Specify the directory to extract files to.
Example:

tar -xzvf my_archive.tar.gz

Extracts the contents of my_archive.tar.gz to the current directory.
tar -xzvf my_archive.tar.gz -C /home/user/

Extracts the contents of my_archive.tar.gz to the /home/user/ directory.
3. Listing Archive Contents:

-t: List the contents of an archive without extracting it.
Example:

tar -tf my_archive.tar.gz
Displays a list of files and directories contained within the my_archive.tar.gz file.
4. Other Useful Options:

-u: Update an existing archive by adding or replacing files.
-r: Add files to an existing archive.
-d: Delete files from an existing archive.
--exclude: Exclude specific files or directories from the archive.
Important Notes:

The tar command has many more options. This is just a selection of the most commonly used ones.
Always use caution when modifying or deleting archive files.
Consider backing up important data before performing any archiving or extraction operations.
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• File compression using gzip
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	The gunzip command in Linux is used to decompress files that have been compressed using the gzip utility.

Basic Syntax:

 

gunzip [OPTION]... file.gz 
Common Options:

-k or --keep: This option tells gunzip to keep the original .gz file after decompressing it. By default, gunzip deletes the .gz file.
-f or --force: Forces decompression even if the original file already exists.
-c: Decompresses the file and writes the output to standard output (you can redirect this to another file using >).
-v or --verbose: Displays verbose information during the decompression process.
Examples:

gunzip myfile.txt.gz: Decompresses myfile.txt.gz and deletes the original compressed file.
gunzip -k myfile.txt.gz: Decompresses myfile.txt.gz and keeps the original compressed file.
gunzip -c myfile.txt.gz > myfile.txt: Decompresses myfile.txt.gz and writes the output to a new file named myfile.txt.
Note:

gunzip is specifically designed for decompressing files that have been compressed using the gzip utility.
For decompressing other archive formats like .zip or .tar.gz, you would use different commands like unzip or tar -xzf.
By using the gunzip command and its options, you can easily decompress files compressed with gzip and restore them to their original state.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• File decompression using gunzip
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
SED


sed 
	stream editor. 
	used for 
		filtering and 
		transforming text.  
	way to edit text, 
	

Here's a breakdown of what sed does and why it's useful:

Key Concepts:

	Stream Editor: 
		sed processes text as a stream, 
			it reads the input line by line, 
			applies the specified operations, and 
			outputs the modified lines. 
		It doesn't load the entire file into memory at once, 
			efficient for working with large files.
	Non-interactive: 
		not like a text editor where you make changes 
		
	Pattern Matching: 
		sed uses regular expressions (or simple patterns) to 
			identify 
				lines or 
				parts of lines 
					that you want to modify.
	Substitution, Deletion, Insertion: 
		sed can perform a wide range of operations, 
		including:
			Substitution: 
				Replacing text that matches a pattern with new text. 
			Deletion: 
				Removing lines or parts of lines that match a pattern. 
			Insertion: 
				Adding new text before or after specific lines. 
			Appending: 
				Adding new text after specific lines. 
			Changing: 
				Replacing entire lines with new text. 
Why is sed useful?

	Automation: 
		sed is excellent for automating text editing tasks. You can include sed commands in scripts to perform complex transformations on text files without manual intervention.
	Batch Processing: 
		You can use sed to modify multiple files at once.
	Filtering: 
		sed can be used to extract specific parts of a text file based on patterns.
	Data Transformation: 
		sed is often used to reformat data, convert between different formats, or clean up messy data.



The general syntax of a `sed` command is:



```bash
sed [options] 'command(s)' [filename(s)]
```

* **`options`:**  Modify the behavior of `sed` (e.g., `-n` for suppressing default printing).
* **`'command(s)'`:** The `sed` commands to be executed. These are enclosed in single quotes.  Multiple commands can be separated by semicolons.
* **`filename(s)`:** The input file(s). If no filename is given, `sed` reads from standard input.

**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
• Print (p)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**

The `p` command prints the specified lines.

* **Example:**
    * `sed -n '2p' test.txt`: 
		Prints only the second line of `test.txt`.  
	The `-n` option suppresses the default printing of all lines.
		* `sed -n '1,3p' test.txt`: 
			Prints lines 1 through 3.
		* `sed -n '/pattern/p' test.txt`: 
			Prints lines containing the `pattern`.

**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
• Substitution (s)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**

The `s` command substitutes a pattern with a replacement string.

* **Syntax:** `s/pattern/replacement/flags`
* **Flags:**
    * `g`: 
		Global substitution (replaces all occurrences on a line).
    * `i`: 
		Case-insensitive substitution.
    * `n`: 
		Replace only the nth occurrence.
* **Examples:**
    * `sed 's/old/new/' test.txt`: 
		Replaces the first occurrence of "old" with "new" on each line.
    * `sed 's/old/new/g' test.txt`: 
		Replaces all occurrences of "old" with "new" on each line.
    * `sed 's/old/new/i' test.txt`: 
		Replaces "old" with "new" case-insensitively.
    * `sed 's/old/new/2' test.txt`: 
		Replaces the second occurrence of "old" with "new" on each line.
    * `sed -n 's/old/new/p' test.txt`: 
		Substitutes "old" with "new" and 
			prints only the lines where a substitution occurred.

**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
• Delete (d)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**

The `d` command deletes lines.

* **Examples:**
    * `sed '2d' test.txt`: 
		Deletes the second line.
    * `sed '1,3d' test.txt`: 
		Deletes lines 1 through 3.
    * `sed '/pattern/d' test.txt`: 
		Deletes lines containing the `pattern`.

**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
• Append (a), Insert (i), and Change (c)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**

* **Append (a):** Adds text *after* the specified line.
    * `sed '2a This is a new line' test.txt`: 
		Appends "This is a new line" after the second line.

* **Insert (i):** Adds text *before* the specified line.
    * `sed '2i This is a new line' test.txt`: 
		Inserts "This is a new line" before the second line.

* **Change (c):** Replaces the specified line(s) with new text.
    * `sed '2c This is a changed line' test.txt`: Changes the second line to "This is a changed line."

**~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
• Writing changes to file (-i)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**

By default, 
	`sed` prints the modified output to the terminal. 
	To save the changes to a file, use the `-i` option.

* **`-i`:**  Modifies the file *in place*.
    * `sed -i 's/old/new/g' test.txt`: 
		Replaces all occurrences of "old" with "new" in `test.txt` and 
			saves the changes directly to the file.

* **`-i.bak` (or similar):** 
	Creates a backup of the original file.  
	This is highly recommended!
    * `sed -i.bak 's/old/new/g' test.txt`:  Modifies `test.txt` and creates a backup file named `test.txt.bak`.

These are the most frequently used `sed` commands and options.  Remember that `sed` is very powerful and offers much more functionality, which you can explore further using the `man sed` command.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Syntax of sed Commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		refer above 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Print
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	refer above 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Substitution
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	refer above 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Delete
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	refer above 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Append, Insert, and Change
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	refer above 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Writing changes to file
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	refer above 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
AWK
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


AWK is a powerful text processing tool in Linux and Unix-like systems. It's more than just a command; it's a programming language designed for pattern scanning and text manipulation.  Think of it as a combination of sed (stream editor) and a scripting language.

Here's a detailed overview of AWK:

Basic Concepts:

Pattern Scanning: AWK reads input line by line and checks each line against specified patterns.
Actions: When a line matches a pattern, AWK executes a set of actions (code) associated with that pattern.
Fields: AWK automatically splits each input line into fields (columns) based on a field separator (usually whitespace). You can refer to these fields as $1, $2, $3, etc.
Variables: AWK supports variables, allowing you to store and manipulate data.
AWK Program Structure:

An AWK program typically has the following structure:

Awk

awk 'pattern1 { action1 } pattern2 { action2 } ...' filename
pattern: A regular expression or condition to match.
action: AWK code to execute when the pattern is matched. Actions are enclosed in curly braces {}.
filename: The input file. If omitted, AWK reads from standard input.
Common AWK Operations:

Printing Fields:

Awk

awk '{print $1, $3}' data.txt  # Prints the first and third fields of each line
Filtering Records:

Awk

awk '/pattern/ {print}' data.txt  # Prints lines containing "pattern"
awk '$3 > 10 {print $1, $2}' data.txt # Prints lines where the 3rd field is greater than 10
Field Separator:

Awk

awk -F',' '{print $1, $2}' data.csv  # Sets the field separator to a comma
BEGIN and END Blocks:

BEGIN { actions }: Code executed before processing any input. Useful for initializing variables, setting headers, etc.
END { actions }: Code executed after processing all input. Useful for printing summaries, calculations, etc.
Awk

awk 'BEGIN { FS=","; print "Name\tAge" } { print $1, $2 } END { print "Finished" }' data.csv
Variables and Operators:

AWK supports various variables and operators (arithmetic, string, comparison, etc.).

Awk

awk '{ total += $2 } END { print "Total:", total }' numbers.txt  # Calculates the sum of the second field
Control Flow:

AWK has control flow statements like if, for, and while.

Awk

awk '{ if ($3 == "admin") print $1 }' users.txt
Built-in Functions:

AWK provides many built-in functions for string manipulation, math, and more.

Awk

awk '{ print toupper($1) }' names.txt  # Prints the first field in uppercase
Example: Processing a CSV file:

Let's say data.csv contains:

John,25,New York
Jane,30,London
Peter,20,Paris
Awk

awk 'BEGIN { FS=","; print "Name\tAge\tCity" } { print $1, $2, $3 }' data.csv
Output:

Name    Age     City
John    25      New York
Jane    30      London
Peter   20      Paris
Key Advantages of AWK:

Powerful Text Processing: AWK excels at manipulating and extracting data from text files.
Pattern Matching: Regular expression support allows for flexible pattern matching.
Concise Syntax: AWK code can be very concise, making it easy to write complex text processing scripts.
Versatile: AWK can be used for a wide range of tasks, from simple filtering to complex data analysis.

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Getting Starting with awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Predefined variables of awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	AWK has several predefined variables that provide useful information and control the behavior of your AWK scripts. Here's a breakdown of the most commonly used ones:

Record-Related Variables:

NR (Number of Records): The current record number (line number). It increments with each line AWK reads.
FNR (File Number of Records): Similar to NR, but it resets for each new file when processing multiple input files.
NF (Number of Fields): The number of fields (columns) in the current record.
$0: The entire current record (line).
$1, $2, $3, etc.: The first, second, third, etc., field in the current record.
Field Separator Variables:

FS (Field Separator): The input field separator. By default, it's whitespace (spaces and tabs). You can change it using the -F option on the command line or by assigning a value to FS within your AWK script. Example: awk -F',' '{print $1}' data.csv or BEGIN { FS="," } within the script.
OFS (Output Field Separator): The output field separator. It's the character that separates fields when you print them. By default, it's a space.
Other Useful Variables:

RS (Record Separator): The input record separator. By default, it's a newline character (\n), meaning each line is a record. You can change this to process records that span multiple lines.
ORS (Output Record Separator): The output record separator. By default, it's a newline character.
FILENAME: The name of the current input file. This is especially useful when processing multiple files.
ARGC: The number of command-line arguments.
ARGV: An array containing the command-line arguments.
Examples:

Printing line numbers and lines:

Awk

awk '{ print NR, $0 }' myfile.txt
Printing the last field of each line:

Awk

awk '{ print $NF }' myfile.txt
Setting the field separator to a comma and printing the first two fields:

Awk

awk -F',' '{ print $1, $2 }' data.csv
Printing the filename and line number:

Awk

awk '{ print FILENAME, NR, $0 }' file1.txt file2.txt
Calculating the sum of the second field in a file:

Awk

awk '{ sum += $2 } END { print "Sum:", sum }' numbers.txt
Printing lines longer than 80 characters:

Awk

awk 'length($0) > 80 { print NR, $0 }' long_lines.txt
Processing multiple files and printing the filename and FNR:

Awk

awk '{ print FILENAME, FNR, $0 }' file1.txt file2.txt file3.txt
Understanding and using these predefined variables can significantly enhance the power and flexibility of your AWK scripts. They allow you to access and manipulate various aspects of the input data and control the output format.  Remember to consult the man awk page for a comprehensive list of all predefined variables.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Doing arithmetic with awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	AWK excels at text processing, but it also has built-in capabilities for performing arithmetic operations. Here's a breakdown of how to do arithmetic with AWK:

Basic Arithmetic Operators:

AWK supports the standard arithmetic operators:

+ (addition)
- (subtraction)
* (multiplication)
/ (division)
% (modulo - remainder after division)
^ or ** (exponentiation)
Example:

Let's say you have a file named data.txt with numbers, one per line:

10
20
30
To calculate the sum of these numbers, you can use the following AWK command:

Bash

awk '{ sum += $1 } END { print "Sum:", sum }' data.txt
{ sum += $1 }: This part of the AWK script is executed for each line in the file. sum += $1 is equivalent to sum = sum + $1. It adds the value of the first field ($1, which is the number on each line) to the variable sum. AWK initializes sum to 0 automatically.
END { print "Sum:", sum }: This part is executed after all lines have been processed. It prints the final value of sum.
Other Arithmetic Operations:

Increment/Decrement:

++ (increment): x++ (post-increment), ++x (pre-increment)
-- (decrement): x-- (post-decrement), --x (pre-decrement)
Assignment Operators:

=: x = 10
+=: x += 5 (equivalent to x = x + 5)
-=: x -= 2 (equivalent to x = x - 2)
*=: x *= 3 (equivalent to x = x * 3)
/=: x /= 2 (equivalent to x = x / 2)
%=: x %= 3 (equivalent to x = x % 3)
^=: x ^= 2 (equivalent to x = x ^ 2)
Example with more complex calculations:

Let's say data2.txt has two numbers per line, separated by a space:

5 2
10 3
15 4
To calculate the product of the two numbers on each line and then the total sum of the products:

Bash

awk '{ product = $1 * $2; total += product; print "Product:", product } END { print "Total Sum:", total }' data2.txt
Built-in Math Functions:

AWK provides several built-in math functions:

sin(x): Sine of x (in radians).
cos(x): Cosine of x (in radians).
exp(x): Exponential of x.
log(x): Natural logarithm of x.
sqrt(x): Square root of x.
int(x): Integer part of x.
rand(): Generates a random number between 0 and 1.
srand([seed]): Sets the seed for the rand() function.
Example using sqrt():

Bash

awk '{ print sqrt($1) }' numbers.txt  # Prints the square root of each number in numbers.txt
Important Considerations:

Number vs. String Context: AWK automatically converts between numbers and strings as needed. However, it's important to be aware of the context to avoid unexpected behavior. If a field is treated as a string, arithmetic operations might not work as intended.
Floating-Point Arithmetic: AWK uses floating-point arithmetic, so be mindful of potential precision issues.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• User Defined variables in awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	In AWK, you can define your own variables to store and manipulate data within your scripts. These user-defined variables are incredibly useful for calculations, storing intermediate results, and controlling the flow of your AWK program.   

How to Define and Use User-Defined Variables:

Assignment: You assign a value to a variable using the = operator.  AWK variables are dynamically typed, meaning you don't need to declare their type (number or string). AWK will infer the type based on how you use the variable.   

Awk

{ name = "John"; age = 30; print name, age }
Naming Conventions: Variable names in AWK follow similar rules to most programming languages:

They can contain letters, digits, and underscores.   
They must start with a letter or underscore.
They are case-sensitive (Name and name are different variables).
Initialization: AWK variables are automatically initialized to an empty string or 0 (depending on context) when they are first used.  It's good practice to explicitly initialize variables, especially if you intend to use them for numerical calculations.

Awk

BEGIN { count = 0; message = "" }  # Initialize variables in the BEGIN block
Scope: AWK variables have global scope by default, meaning they are accessible throughout the entire AWK script.

Using Variables: You can use variables in expressions, conditions, and actions.   

Awk

{ price = $2 * 1.1; print $1, price }  # Calculate price with tax
Examples:

Counting lines:

Awk

{ count++ } END { print "Total lines:", count }
Calculating a sum:

Awk

{ sum += $1 } END { print "Sum:", sum }
Storing the maximum value:

Awk

{ if ($1 > max) max = $1 } END { print "Max:", max }
Concatenating strings:

Awk

{ message = message $1 " " } END { print "Message:", message }
Using variables in conditions:

Awk

{ if ($2 > 18) print $1, "is an adult" }
Using variables with printf for formatted output:

Awk

{ name = $1; age = $2; printf "%s is %d years old\n", name, age }
Passing variables from the command line:

You can pass values to AWK variables from the command line using the -v option:

Bash

awk -v threshold=10 '{ if ($1 > threshold) print $1 }' data.txt
Best Practices:

Initialize Variables: Always initialize your variables, especially when performing calculations, to avoid unexpected results.
Descriptive Names: Use meaningful variable names to make your code easier to understand.
Comments: Add comments to explain the purpose of your variables.
By effectively using user-defined variables, you can create more powerful and flexible AWK scripts to process and analyze text data. They are essential for storing intermediate results, performing calculations, and controlling the logic of your AWK programs.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Use of printf statement
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The printf statement in AWK provides formatted output, giving you precise control over how your data is displayed. It's similar to the printf function in C and other languages.  Here's a breakdown of how to use it:

Syntax:

Awk

printf "format string", value1, value2, ...
format string: This string contains text and format specifiers that control how the values are printed.
value1, value2, ...: The data you want to print.
Format Specifiers:

Format specifiers begin with % and indicate the type of data and how it should be formatted.  Here are some common ones:

%s: String.
%d or %i: Signed integer.
%f or %F: Floating-point number (decimal).
%e or %E: Floating-point number in scientific notation.
%c: Character.
%%: A literal % character.
Modifiers:

You can add modifiers to format specifiers to further control the output:

Width: %10s (string with a width of 10 characters). If the string is shorter, it will be padded with spaces.
Precision: %.2f (floating-point number with 2 decimal places).
Flags:
-: Left-align the output (by default, it's right-aligned). %-10s
+: Always include a sign (+ or -) for numeric values.
0: Pad with zeros instead of spaces. %05d
Examples:

Printing strings and numbers:

Awk

awk '{ printf "Name: %s, Age: %d\n", $1, $2 }' data.txt
Formatted floating-point numbers:

Awk

awk '{ printf "Price: $%.2f\n", $1 }' prices.txt
Left-aligned strings with a specific width:

Awk

awk '{ printf "%-20s %d\n", $1, $2 }' names_and_ages.txt
Scientific notation:

Awk

awk '{ printf "%e\n", $1 }' large_numbers.txt
Combining format specifiers and modifiers:

Awk

awk '{ printf "ID: %04d, Value: $%+8.2f\n", $1, $2 }' formatted_data.txt
Using printf in BEGIN and END blocks:

Awk

BEGIN { printf "Start of report\n" }
{ printf "%s\t%d\n", $1, $2 }
END { printf "End of report\n" }
Key Differences between print and printf:

print: Simple output, fields are separated by the Output Field Separator (OFS). Adds a newline at the end of each print statement.
printf: Formatted output, requires a format string. Does not automatically add a newline; you must include \n in the format string if you want a new line.
Why use printf?

Control over output: printf allows you to precisely control the appearance of your output, including field widths, alignment, number of decimal places, and more.
Readability: Formatted output can make your data much easier to read and understand.
Report generation: printf is essential for creating well-formatted reports.
By mastering printf, you can significantly improve the presentation and usability of the data you process with AWK.  It's a crucial tool for generating reports, tables, and other formatted output.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• In-built function
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	AWK comes equipped with a rich set of built-in functions that greatly extend its text processing capabilities. These functions cover various areas, including string manipulation, mathematical operations, and time functions. Here's a breakdown of some of the most commonly used categories and examples:

String Functions:

length(string): Returns the length of the string.

Awk

awk '{ print length($1) }' myfile.txt  # Prints the length of the first field
substr(string, start, length): Extracts a substring from the string.

Awk

awk '{ print substr($1, 2, 5) }' myfile.txt # Extracts 5 characters from $1 starting at the 2nd character
index(string1, string2): Returns the starting position of string2 within string1 (0 if not found).

Awk

awk '{ print index($1, "abc") }' myfile.txt
tolower(string): Converts the string to lowercase.

Awk

awk '{ print tolower($1) }' myfile.txt
toupper(string): Converts the string to uppercase.

Awk

awk '{ print toupper($1) }' myfile.txt
split(string, array, separator): Splits the string into an array using the separator as the delimiter. Returns the number of elements in the array.

Awk

awk '{ n = split($1, parts, ","); for (i=1; i<=n; i++) print parts[i] }' data.csv
sprintf(format, ...): Formats a string using printf style formatting.

Awk

awk '{ printf "%-10s %5d\n", $1, $2 }' data.txt
match(string, regular_expression):  Tests if the string matches the regular expression.  Sets RSTART and RLENGTH.

gsub(regular_expression, replacement, string): Globally substitutes occurrences of the regex with the replacement in the string. Returns number of substitutions made.

sub(regular_expression, replacement, string): Substitutes the first occurrence of the regex with the replacement in the string. Returns number of substitutions made.

Mathematical Functions:

sin(x), cos(x), tan(x): Trigonometric functions (in radians).
exp(x): Exponential function.
log(x): Natural logarithm.
sqrt(x): Square root.
int(x): Integer part of x (truncates).
rand(): Generates a pseudo-random number between 0 and 1.
srand([seed]): Sets the seed for the random number generator.
Time Functions:

systime(): Returns the current Unix timestamp (seconds since the epoch).
strftime(format, timestamp): Formats a timestamp into a human-readable date and time string.
Example using strftime:

Awk

BEGIN { t = systime(); print strftime("%Y-%m-%d %H:%M:%S", t) }
Other Functions:

close(filename): Closes a file.
fflush(): Flushes buffered output.
system(command): Executes a shell command.
Example using system:

Awk

{ if ($3 == "error") system("mail -s 'Error Detected' admin@example.com < error_message.txt") }
Key Points:

Case Sensitivity: AWK function names are case-sensitive (e.g., tolower() not ToLower()).
Arguments: Functions can take zero or more arguments.
Return Values: Functions often return a value (e.g., length() returns the length of a string).
This is not an exhaustive list, but it covers the most commonly used AWK built-in functions.  Refer to the man awk page for a complete and detailed description of all available functions. Using these functions effectively can significantly simplify and enhance your AWK scripts for text processing and data manipulation.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Use of Format Specification Code
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Format specification codes are used in various programming languages and tools (including printf in C, AWK, and other languages, as well as string formatting methods in Python, etc.) to control the way data is displayed or formatted as text. They provide a way to specify the type of data, its width, precision, alignment, and other formatting options.

Here's a breakdown of how they work and some common examples:

General Structure:

A format specification code typically follows this structure:

%[flags][width][.precision][length]type
Let's break down each part:

%: The percent sign marks the beginning of a format specifier.
flags (optional): Flags modify the formatting behavior. Common flags include:
-: Left-align the output. (Default is right-alignment)
+: Always include a sign (+ or -) for numeric values.
0: Pad with zeros instead of spaces.
(space): Add a space before positive numbers.
width (optional): Specifies the minimum width of the output field. If the data is shorter than the width, it will be padded.
.precision (optional): For floating-point numbers, specifies the number of digits after the decimal point. For strings, it specifies the maximum length.
length (optional): Used in C/C++ to specify the size of the data type (e.g., h, l, ll for short, long, and long long integers). Less common in scripting languages.
type (required): Specifies the data type:
s: String.
d or i: Signed integer.
u: Unsigned integer.
f or F: Floating-point number (decimal notation).
e or E: Floating-point number (scientific notation).
g or G: Floating-point number (either decimal or scientific, whichever is shorter).
c: Character.
p: Pointer (memory address - C/C++ specific).
%: A literal % character.
Examples:

Printing a string:

C

printf("Name: %s\n", "John"); // Output: Name: John
printf("Name: %10s\n", "John"); // Output: Name:       John (right-aligned, width 10)
printf("Name: %-10s\n", "John"); // Output: Name: John       (left-aligned, width 10)
Printing an integer:

C

printf("Age: %d\n", 30);      // Output: Age: 30
printf("Age: %03d\n", 30);      // Output: Age: 030 (padded with zeros)
printf("Age: %+d\n", 30);      // Output: Age: +30 (includes sign)
Printing a floating-point number:

C

printf("Price: %.2f\n", 12.99);  // Output: Price: 12.99
printf("Price: %7.2f\n", 12.99);  // Output: Price:   12.99 (right-aligned, width 7)
printf("Price: %e\n", 12345.67); // Output: Price: 1.234567e+04 (scientific notation)
Combining format specifiers:

C

printf("Name: %-10s Age: %3d Price: $%.2f\n", "John", 30, 12.99);
// Output: Name: John       Age:  30 Price: $12.99
AWK Example:

Awk

{ printf "Name: %-15s Score: %03d\n", $1, $2 }' data.txt
Key Points:

The number of format specifiers must match the number of values you're providing.
Incorrect format specifiers can lead to unexpected output or errors.
Format specifiers are essential for creating well-formatted and readable output, especially when dealing with tables, reports, or numerical data.
Understanding and using format specification codes is crucial for controlling how your programs present data to the user. They provide a way to customize the output to meet specific requirements and improve readability.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• BEGIN & END constructs
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The BEGIN and END constructs in AWK are special blocks of code that are executed at specific times during the AWK program's execution. They provide a way to set up your AWK script before processing the input data and perform cleanup or summarization after all the data has been processed.

BEGIN Block:

Purpose: The BEGIN block is executed before AWK starts processing any input records (lines) from the input file or standard input.
Use Cases:
Initialization: You can initialize variables, set field separators (FS), print headers, or perform any setup tasks that need to be done only once at the beginning of the script.
Setting up the environment: You might set initial values for variables that control the AWK program's behavior.
Example:

Awk

BEGIN {
  FS = ",";  # Set the field separator to a comma
  print "Name\tAge\tCity";  # Print a header row
  total_age = 0;  # Initialize a variable
}

{  # This block is executed for each input line
  print $1, $2, $3;  # Print the fields
  total_age += $2;  # Add the age to the total
}

END {
  avg_age = total_age / NR;  # Calculate the average age
  printf "Average age: %.2f\n", avg_age;  # Print the average age
}
END Block:

Purpose: The END block is executed after AWK has processed all input records.
Use Cases:
Summarization: You can perform calculations on the data that has been processed (like calculating sums, averages, counts), and print summary reports.
Cleanup: You might close files or perform any other cleanup tasks.
Printing final results: Displaying the results of calculations or data processing.
Example (Continuing the previous example):

The END block in the previous example calculates the average age and prints it after all the input lines have been processed.

Combined Example and Explanation:

Let's say you have a file named data.csv with the following content:

John,25,New York
Jane,30,London
Peter,20,Paris
The AWK script I provided earlier would produce the following output:

Name    Age     City
John    25      New York
Jane    30      London
Peter   20      Paris
Average age: 25.00
Explanation:

BEGIN Block:

FS = ",";: Sets the input field separator to a comma, so AWK knows to treat values separated by commas as different fields.
print "Name\tAge\tCity";: Prints a header line with tabs separating the column names.
total_age = 0;: Initializes the total_age variable to 0.
Main Block (No Pattern):

This block is executed for each line in data.csv.
print $1, $2, $3;: Prints the first, second, and third fields (name, age, city) separated by tabs (because OFS is a space by default).
total_age += $2;: Adds the age (second field) to the total_age.
END Block:

avg_age = total_age / NR;: Calculates the average age by dividing the total_age by the number of records (NR).
printf "Average age: %.2f\n", avg_age;: Prints the average age formatted to two decimal places.
Key Points:

BEGIN and END blocks are optional. You don't have to include them in your AWK scripts if they are not needed.
If you have multiple BEGIN or END blocks, they are executed in the order they appear in the script.
BEGIN and END are especially useful for tasks that need to be done before or after processing the main data, such as setting up, summarizing, or cleaning up.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• if condition in awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	AWK supports if statements for conditional execution of code blocks, just like most programming languages.  Here's how you use them:

Basic Syntax:

Awk

if (condition) {
  // Code to execute if the condition is true
}
condition: An expression that evaluates to true or false. This can involve comparisons using operators like == (equal to), != (not equal to), > (greater than), < (less than), >= (greater than or equal to), <= (less than or equal to), and regular expression matching using ~ (matches) and !~ (does not match).
{ ... }: The code block enclosed in curly braces is executed only if the condition is true.
else Clause:

You can also include an else clause to specify code to execute if the condition is false:

Awk

if (condition) {
  // Code to execute if the condition is true
} else {
  // Code to execute if the condition is false
}
else if (or nested if):

AWK doesn't have a direct else if keyword, but you can achieve the same effect by nesting if statements within the else block:

Awk

if (condition1) {
  // Code for condition1
} else {
  if (condition2) {
    // Code for condition2
  } else {
    // Code if neither condition1 nor condition2 is true
  }
}
Examples:

Checking if a value is greater than 10:

Awk

{ if ($1 > 10) print $1, "is greater than 10" }
Printing different messages based on age:

Awk

{
  if ($2 >= 18) {
    print $1, "is an adult"
  } else {
    print $1, "is a minor"
  }
}
Multiple conditions (using && for AND, || for OR):

Awk

{
  if ($2 >= 18 && $3 == "USA") {
    print $1, "is an adult in the USA"
  }
}
Regular expression matching:

Awk

{
  if ($1 ~ /^[A-Z]/) {  # Check if the first field starts with an uppercase letter
    print $1, "starts with an uppercase letter"
  }
}
Nested if statements:

Awk

{
  if ($2 > 10) {
    if ($2 < 20) {
      print $1, "is between 10 and 20"
    } else {
      print $1, "is 20 or more"
    }
  }
}
Important Points:

Parentheses: The condition in the if statement must be enclosed in parentheses ().
Curly Braces: The code blocks associated with if and else are enclosed in curly braces {}. If the code block consists of only one statement, the curly braces are optional, but it's generally good practice to always use them for clarity.
Comparison Operators: Be careful to use == for equality comparisons, not =. = is the assignment operator.
Regular Expressions: You can use regular expressions with the ~ and !~ operators for pattern matching.
By using if statements effectively, you can add decision-making logic to your AWK scripts, making them more powerful and versatile for text processing.  You can control which parts of your code are executed based on the values in the input data.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Loops in awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	AWK provides several looping constructs to iterate over records (lines) or perform repetitive actions. Here's a breakdown of the most commonly used loops:

1. while loop:

The while loop executes a block of code as long as a condition is true.

Awk

while (condition) {
  // Code to execute while the condition is true
}
condition: An expression that evaluates to true or false.
The code within the curly braces {} is executed repeatedly as long as the condition remains true.
Example:

Awk

{
  i = 1;
  while (i <= 5) {
    print i;
    i++;
  }
}
This will print numbers from 1 to 5.

2. for loop:

The for loop is similar to the for loop in C and other languages. It's often used when you know the number of iterations in advance.

Awk

for (initialization; condition; increment/decrement) {
  // Code to execute
}
initialization: Executed once before the loop starts. Usually used to initialize a loop counter.
condition: Checked before each iteration. The loop continues as long as the condition is true.
increment/decrement: Executed after each iteration. Usually used to update the loop counter.
Example:

Awk

{
  for (i = 1; i <= 5; i++) {
    print i;
  }
}
This also prints numbers from 1 to 5.

3. do-while loop:

The do-while loop is similar to the while loop, but the code block is executed at least once before the condition is checked.   

Awk

do {
  // Code to execute
} while (condition);
Example:

Awk

{
  i = 1;
  do {
    print i;
    i++;
  } while (i <= 5);
}
This will also print numbers from 1 to 5.

4. Array Traversal (using for...in):

AWK provides a special for loop for iterating over the indices of an array:

Awk

for (index in array) {
  // Code to execute for each index in the array
  print index, array[index];
}
Example:

Awk

{
  split($1, parts, ",");  # Split the first field into an array
  for (i in parts) {
    print i, parts[i];
  }
}
If the first field is "apple,banana,orange", this will print:

1 apple
2 banana
3 orange
5. Implicit Loop over Records:

AWK implicitly loops over each record (line) in the input file.  You don't always need an explicit loop to process every line.

Awk

{  # This block is implicitly executed for each line
  print $1, $2;
}
break and continue statements:

Within loops, you can use the break statement to exit the loop prematurely and the continue statement to skip the rest of the current iteration and proceed to the next iteration.   

Example using break:

Awk

{
  for (i = 1; i <= 10; i++) {
    if (i > 5) {
      break;  # Exit the loop when i is greater than 5
    }
    print i;
  }
}
This will print numbers from 1 to 5.

Example using continue:

Awk

{
  for (i = 1; i <= 10; i++) {
    if (i % 2 == 0) {
      continue;  # Skip even numbers
    }
    print i;
  }
}
This will print only the odd numbers from 1 to 9.

Choosing the right loop depends on the specific task you're trying to accomplish.  for loops are useful when you know the number of iterations. while and do-while loops are suitable when the number of iterations depends on a condition. The for...in loop is specifically for iterating over array indices. And the implicit loop is the most common way to process records in a file.


Sources and related content

	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Examples in awk
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Other commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	
	Hard links
----------
ln <file name> <new link name>
ls li #displays the links
ls ll #displays all but links highlighted.
man ln #manual of link

Create user
--------------
sudo cat /etc/password
sudo cat /etc/shadow

sudo useradd vilas
sudo cat /etc/group
sudo groupadd admins_group
adding user to a group
sudo useradd <user> -g <group>
	e.g. sudo useradd vilas -g admins_group
Go to application - users, we can see them listed.

Network
--------
ip addr
	lo - loopback
	enp - ethernet
		link/ether - internet MAC address
		inet - IPV4
		inet6- IPV6

ethtool enp03 #network device
	
##############################
Sockets and /var/run/docker
---------------------------

There are two types of sockets.
	- IP Sockets.
		- Commonly referred to as Sockets   
		- Bound to a port (and address), we send TCP requests to, and get responses from IP Sockets.
		- use the network - i.e. does RPC
	- Unix Socket
		- Used for IPC (Interprocess Communication). 
		- Also called Unix Domain Sockets (UDS). Unix Sockets use the local filesystem for communication

The Docker daemon can listen for Docker Engine API requests via three different types of Socket: 
	- unix, 
	- tcp, 
	- fd.

By default, a unix domain socket (or IPC socket) is created at 
/var/run/docker.sock

Let us see some live examples:

Docker Server uses this socket to listen to the REST API, and the clients use the socket to send API requests to the server.

curl can talk to a Unix Socket via the --unix-socket flag. Since Docker Server API is exposed as REST, we’d need to send commands over HTTP. Also, as this server is local (remember, the file system), we can pass any hostname in the URL (or stick to the localhost, that will work fine too!). The server does not care about the hostname, just the path.

curl --unix-socket /var/run/docker.sock http://localhost/images/json | jq
curl --unix-socket /var/run/docker.sock http://localhost/containers/json | jq
curl -i -X POST --unix-socket /var/run/docker.sock "http://foo/images/a95fgf458dfd/tag?repo=redis&tag=foo"
curl --no-buffer --unix-socket /var/run/docker.sock http://localhost/events





	gzip - Compress files
    gzip -d - Decompress a compressed file
    
	df - Display disk space usage
    df -h - Display disk space usage in human-readable format
    
	du - Estimate file and directory space usage
    
	free - Display system memory usage
    
	netstat - Display network statistics
    ssh - Securely connect to a remote server
    wget - Download files from the internet
    curl - Transfer data with URLs
    reboot - Reboot the system
    history - Display command history
    
	
    finger - Display user information
    unzip -l - List the contents of a ZIP archive
    unzip -d - Extract files from a ZIP archive to a specific directory
    time - Measure the execution time of a command
    df -i - Display inode usage
    du -s - Display only the total size of a directory
    uname -a - Display all system information
		uname -a is a command used in Linux and other Unix-like systems to display system information. It provides detailed information about the operating system, including:

		Kernel name: The name of the operating system kernel (e.g., "Linux").
		Nodename: The hostname of the machine.
		Release: The kernel release number.
		Version: The kernel version number.
		Machine: The hardware architecture (e.g., "x86_64").
		Domainname: The domain name of the system (if configured).
		Operating System: The name of the operating system distribution (e.g., "Ubuntu", "CentOS").
		This information can be useful for system administrators, developers, and users who need to understand the system they are working on.
	
    who -H - Show header in 'who' command output
    history -c - Clear command history
    sudo !! - Re-run the last command with sudo
    locate - Find files by name
    find /path/to/search -name "filename" - Search for a file by name in a specific directory
    grep -C 2 "pattern" - Display lines before and after the matched pattern
    df -hT - Display disk space usage with filesystem type
    du -h -d 1 - Display the size of the directories in the current directory
    zip -r - Create a ZIP archive recursively
    lsof -i - Display information about open internet connections
    killall - Signal processes by name
    top -u username - Monitor resource usage for a specific user
    ifconfig interface up/down - Bring a network interface up or down
    scp -r - Securely copy directories between local and remote systems
    grep -rli "pattern" . - Search for a pattern in files and display matching filenames only


	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
SHELL Scripting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Shell Scripting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	D:\PraiseTheLord\HSBGInfotech\linux\ShellScripting
	D:\PraiseTheLord\HSBGInfotech\Others\vilas\linux\ShellScripting.txt 

	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Environment Variables – env
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	Environment variables are dynamic name-value pairs that are stored in the system's memory and accessible by all running processes. They provide a way to configure the behavior of programs and the shell itself.  The env command in Linux is used to display or set environment variables.   

Key Concepts:

Name-Value Pairs: An environment variable consists of a name (e.g., HOME, PATH) and a value (e.g., /home/user, /usr/bin:/bin).
Dynamic: Environment variables can be changed or set at any time, and these changes can affect how programs behave.
Inherited: Child processes inherit the environment variables of their parent process. This is how settings are passed down to applications.
Scope: Environment variables can have different scopes (e.g., system-wide, user-specific, or process-specific). 
Using the env command:

Displaying all environment variables:

Bash

env
   This command will list all currently set environment variables and their values. The output will be in the format VARIABLE_NAME=value. 

Running a command with a modified environment:

Bash

env VARIABLE_NAME=new_value command
This will run command with VARIABLE_NAME set to new_value.  This change is only temporary and affects the environment of the command being executed.  The parent shell's environment remains unchanged.

Setting environment variables:

You can set environment variables directly in the shell using the following syntax:

Bash

export VARIABLE_NAME=value
The export keyword makes the variable available to subsequently executed commands.  If you omit export, the variable is set, but it's not automatically exported to child processes.

Unsetting environment variables:

Bash

unset VARIABLE_NAME
This will remove the environment variable.

Commonly Used Environment Variables:

HOME: The path to the user's home directory. 
PATH: A colon-separated list of directories where the shell searches for executable programs.   
USER: The current username.
SHELL: The path to the user's default shell.
TERM: The type of terminal being used. 
PWD: The current working directory.  
LANG: The locale settings for language and character encoding. 
Examples:

Displaying the value of the HOME variable:

Bash

echo $HOME
Running ls with a modified PATH:

Bash

env PATH=/tmp ls
This will run ls with /tmp as the only directory in the PATH.

Setting and exporting a variable:

Bash

export MY_VARIABLE="Hello, world!"
echo $MY_VARIABLE
Setting a variable without exporting:

Bash

MY_VARIABLE="This is not exported"
bash  # Start a new subshell
echo $MY_VARIABLE  # This will be empty in the subshell
exit  # Exit the subshell
Setting environment variables for a specific program:

Bash

FOO=bar myprogram
This sets the environment variable FOO to bar only for the execution of myprogram.

Importance of Environment Variables:

Environment variables are crucial for configuring software and the shell. They allow you to customize the behavior of programs without modifying their code.  They are also used to pass information between processes.  Understanding how to use and manage environment variables is an essential skill for working effectively in a Linux environment.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Types of shells: sh, ksh, bash, csh, tcsh
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	1. sh (Bourne Shell):

	History: The oldest of the shells listed, it's the original Unix shell written by Stephen Bourne at Bell Labs.
	Features: Basic command-line interpretation, scripting capabilities (using control flow structures, variables, etc.). 
	Limitations: Lacks many interactive features found in later shells (like command-line editing, history, aliases).
	Use Today: Often used as a minimal shell for scripting and system startup scripts due to its small size and portability. It's POSIX compliant, meaning scripts written for sh should work on any POSIX-compliant system. /bin/sh is often a symbolic link to another shell (like bash on many Linux systems), but it runs in a compatibility mode to behave like the original Bourne shell.
2. ksh (Korn Shell):

	History: Developed by David Korn at Bell Labs. 
	Features: An improved version of sh, adding many interactive features like command-line editing, job control, and aliases. Also introduced features like functions and floating-point arithmetic.
	Use Today: Still used, particularly in environments where its specific features are required or preferred. It's also a basis for the POSIX shell standard.
3. bash (Bourne Again Shell):

	History: Created by Brian Fox for the GNU project. 
	Features: The most popular shell on Linux. It's a superset of sh, meaning it's backward compatible with sh scripts. Adds a wealth of features, including extensive command-line editing, command history, shell functions, aliases, job control, arrays, and more. 
	Use Today: The default shell for most Linux distributions. Its rich feature set and wide availability make it the shell of choice for most users and scripting.
4. csh (C Shell):

	History: Developed by Bill Joy at UC Berkeley.
	Features: Designed to be more interactive and C-like in syntax. Introduced features like aliases, command history, and job control.
	Limitations: Scripting syntax is different from sh and can be less consistent. Floating-point arithmetic is not as robust as ksh or bash.
	Use Today: Less common as a primary interactive shell, though some users still prefer its interactive features. Its scripting style is distinct.
5. tcsh (Tenex C Shell):

	History: An improved version of csh.
	Features: Adds further enhancements to csh, such as improved command-line completion, more sophisticated history mechanisms, and other interactive features.
	Use Today: Like csh, less common as a primary shell but still used by some who appreciate its interactive features.
Summary Table:

Shell		History			Key Features							Scripting	Interactive	POSIX
sh			Oldest, Bourne	Basic command processing				Yes			Limited	Yes
ksh			Korn, Bell Labs	Improved sh, job control, 				Yes	Good	Mostly
								aliases, functions
bash		GNU Project		Feature-rich, command history, 			Yes	Excellent	Subset
								functions, aliases, arrays
csh			UC Berkeley		C-like syntax, aliases, 				Good		No
								history, job control
								Different from sh
tcsh		Improved csh	Enhanced csh, better completion			Excellent	No
							 and history	Different from sh
Export to Sheets
Which shell to use?

General use on Linux: bash is the most common and generally recommended choice.
Portable scripting: sh (or a POSIX-compliant shell) is best for scripts that need to run on various Unix systems.
Specific needs: If you have a strong preference for a C-like syntax, you might consider csh or tcsh, but be aware of the scripting differences.
In most Linux environments, /bin/sh is a symbolic link to /bin/bash, but when invoked as sh, bash runs in a compatibility mode, mimicking the behavior of the original Bourne shell for script compatibility.  This makes bash very versatile, serving both as a feature-rich interactive shell and a POSIX-compliant scripting environment.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• System defined shell variables: $HOME, $USER, $LOGNAME, $PATH, etc.,
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	User and Session Information:

$HOME: The absolute path to the user's home directory. (e.g., /home/john)
$USER: The current username. (e.g., john)
$LOGNAME: The user's login name (often the same as $USER, but can be different in some cases).
$UID: The user's numeric user ID.
$SHELL: The path to the user's default login shell. (e.g., /bin/bash)
$TERM: The type of terminal being used (e.g., xterm, vt100). This influences how programs format output for the terminal.
$MAIL: The path to the user's mailbox file.
System Information:

$PATH: A colon-separated list of directories where the shell searches for executable commands. This is crucial; if a directory isn't in $PATH, you can't run programs in it directly by name (unless you provide the full path). (e.g., /usr/bin:/bin:/usr/local/bin)
$PWD: The current working directory.
$OLDPWD: The previous working directory (used with the cd - command).
$HOSTNAME: The hostname of the machine.
$OSTYPE: The operating system type.
$ARCH: The system architecture (e.g., x86_64, arm64).
Locale and Language:

$LANG: The primary locale setting for language and character encoding. (e.g., en_US.UTF-8)
$LC_ALL: Overrides all other LC_* locale settings.
$LC_CTYPE: Character classification and case conversion.
$LC_NUMERIC: Numeric formatting (decimal point, thousands separator).
$LC_TIME: Date and time formatting.
$LC_COLLATE: Collation order for sorting strings.
$LC_MONETARY: Monetary formatting.
$LC_MESSAGES: Language for system messages.
Shell Behavior:

$PS1: The primary shell prompt string. This controls how your command prompt looks. You can customize it extensively.
$PS2: The secondary shell prompt string (used for multi-line commands).
$IFS (Internal Field Separator): Used by the shell to split words in command lines. By default, it includes space, tab, and newline.
Other Important Variables:

$RANDOM: Generates a pseudo-random integer.
$SECONDS: The number of seconds the shell has been running.
$EDITOR or $VISUAL: The default text editor used by various commands.
$PAGER: The default pager program (used for displaying long output).
How to Use and View:

Viewing: echo $VARIABLE_NAME (e.g., echo $HOME)
Setting (or Modifying): export VARIABLE_NAME=value (e.g., export EDITOR=vim)
Viewing all: env or printenv
Importance:

These variables are crucial because they:

Configure program behavior: Many programs rely on environment variables to determine how they should run (e.g., the $PATH variable).
Provide context to the shell: The shell uses these variables to display information, process commands, and manage the user's environment.
Allow customization: You can customize your shell environment by modifying these variables (especially $PS1).
Understanding and managing these environment variables is a fundamental aspect of working with the shell and Linux systems.  They provide a flexible way to control the behavior of programs and the shell itself.
	
	
	
	
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• User defined variables: local and global shell variables
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	202

Global Variables:

Declared outside any function.
Accessible (and can be modified) from anywhere in the script, including inside functions.
Local Variables:

Declared inside a function using the local keyword.
Only accessible within the function where they are defined. They have function scope.
Variables with the same name declared as local variables inside different functions are completely independent of each other.
Using local prevents accidental modification of global variables from within a function, which is a good practice for writing more maintainable code.
Scope: The scope of a variable determines where it can be accessed.  Global variables have global scope, while local variables have local (function) scope.

Example Breakdown:

The script first declares a global variable global_var.
my_function demonstrates how to declare and use a local variable local_var. It also shows that you cannot access local_var from outside the function.
The main part of the script shows that you can access the global variable but not the local variable of the function.
another_function demonstrates how to modify a global variable and declare a local variable within another function.
The script then shows that the global variable is modified after the call to another_function, but the local variable of another_function is not accessible.
Finally the script demonstrates how to declare a local variable in the main part of the script, outside of any function.
How to Run:

Save the code to a file (e.g., variable_scope.sh).
Make it executable: chmod +x variable_scope.sh
Run it: ./variable_scope.sh
Output:

The output of the script will clearly demonstrate the scope of the variables, showing when they are accessible and when they are not.  Pay close attention to the lines where the script tries to access local variables from outside their defining functions; these will be empty.  This highlights the importance of scope in preventing unintended side effects and keeping your code organized.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Customizing Your Prompt
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Customizing your shell prompt (the text you see before you type commands) is a great way to personalize your terminal and make it more informative.  Here's how you can customize it, focusing on bash (the most common shell on Linux):

The $PS1 Variable:

The primary way to customize your prompt is by setting the PS1 environment variable.  This variable contains the string that defines your prompt.  You can include special characters (escape sequences) in the PS1 string to display various information.

Common Escape Sequences:

Here are some of the most useful escape sequences you can use in your PS1:

\u: Your username.
\h: The hostname of your machine (shortened).
\H: The full hostname of your machine.
\w: The current working directory (abbreviated).
\W: The current working directory (full path).
\$: A $ for normal users or a # for the root user.
\!: The history number of the current command.
\t: The current time in 24-hour HH:MM:SS format.
\@: The current time in 12-hour HH:MM:SS format.
\d: The current date in YYYY-MM-DD format.
\n: A newline character (moves the prompt to the next line).
\\: A literal backslash.
\[ ... \]: Used to enclose non-printing characters (like colors) so that they don't affect the prompt's length calculation.
Setting the PS1 Variable:

You can set the PS1 variable in your shell's configuration files.  Here's where you'd typically put the setting:

~/.bashrc: For interactive non-login shells (most terminal windows). This is the most common place to customize your prompt.
~/.bash_profile: For login shells (when you log in to a terminal).
To set the PS1 variable, add a line like this to your .bashrc file:

Bash

PS1='[\u@\h \w]\$ '
This would create a prompt like:

[user@hostname ~/current/directory]$ 
Example Customizations:

Colorful Prompt:

Bash

PS1='\[\e[32m\][\u@\h \w]\$ \[\e[0m\]'
This adds color to the prompt.  \e[32m sets the color to green, and \e[0m resets the color back to the default.  The \[ ... \] sequences are crucial for colors so that the prompt length is calculated correctly.

Prompt with Time:

Bash

PS1='[\u@\h \w \t]\$ '
Multi-line Prompt:

Bash

PS1='\[\e[34m\]\u@\h \[\e[0m\]\n\[\e[32m\]\w\$ \[\e[0m\]'
This creates a two-line prompt, with the username and hostname on the first line and the current directory and $ on the second line.

Showing Git Branch (requires a separate script or tool):

You can integrate Git branch information into your prompt. This often requires a separate script or tool that checks the Git status.  There are many examples available online.  A simplified version might look like this (but wouldn't handle all Git scenarios):

Bash

parse_git_branch() {
  git branch 2> /dev/null | grep "^*" | cut -d ' ' -f 2
}
PS1='[\u@\h \w \$(parse_git_branch)]\$ '
How to Make Changes Take Effect:

After you've modified your .bashrc file, you need to source it to apply the changes:

Bash

source ~/.bashrc
Or, you can simply close and reopen your terminal.

Online Resources:

There are countless examples of custom PS1 settings online.  Search for "bash prompt generator" or "bash PS1 examples" to find inspiration and tools to help you create your perfect prompt.  Experiment with different escape sequences and colors to create a prompt that suits your preferences and workflow.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Built-In Versus Linux Commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	In a Linux shell (like Bash), there's a distinction between built-in commands and external (or Linux) commands.  Understanding this difference is important for efficient shell scripting and usage.

Built-in Commands (Shell Builtins):

Definition: 
	These commands are part of the shell itself. They are implemented directly within the shell's code.
Execution: 
	When you type a built-in command, the shell executes it directly, without creating a new process.
Examples: 
	cd, 
	echo, 
	export, 
	set, 
	unset, 
	alias, 
	type, 
	read, 
	test, 
	return, 
	break, 
	continue, 
	exec, 
	exit, 
	hash, 
	local.
Advantages:
Speed: They are generally faster because they don't involve the overhead of creating a new process.
Shell Manipulation: They can directly manipulate the shell's internal state (e.g., changing directories with cd, setting environment variables with export, creating aliases).
How to identify: You can use the type command to check if a command is a builtin.
Bash

type cd
cd is a shell builtin
type ls
ls is aliased to `ls --color=auto'
External Commands (Linux Commands/Utilities):

Definition: These commands are separate executable programs that reside in the file system (usually in directories like /usr/bin, /bin, /usr/local/bin).
Execution: When you type an external command, the shell creates a new process (a child process) to run that program. The shell then waits for the program to finish before continuing.
Examples: ls, grep, awk, sed, find, cp, mv, rm, cat, date, ps, top, ssh.
How to identify: The type command will generally identify these as files, aliases or functions.
Bash

type ls
ls is aliased to `ls --color=auto'
type grep
grep is /usr/bin/grep
Advantages:
Variety: There's a vast range of external commands available for all sorts of tasks.
Independent: They are independent programs, so they can be developed and updated separately from the shell.
Key Differences Summarized:

Feature	Built-in Commands	External Commands
Implementation	Part of the shell	Separate programs
Execution	Directly by shell	New process created
Speed	Generally faster	Generally slower (process creation overhead)
Shell Access	Direct access to shell state	Limited access (through environment variables, etc.)

Export to Sheets
Example Scenario:

When you type cd /home/user, cd is a built-in command. The shell itself changes its current directory.  When you type ls -l, ls is an external command. The shell creates a new process to execute the ls program, which then lists the files in the current directory.

Importance of the Distinction:

Shell Scripting: When writing shell scripts, you need to know which commands are built-ins because they behave differently. For example, you can't use /bin/cd in a script to change the script's working directory; you must use the shell's cd built-in.
Performance: Using built-ins where possible can improve the performance of your scripts, especially if they are executed frequently.
Portability: Built-in commands are generally more portable because they are part of the shell standard, whereas external commands might vary between systems (although core utilities like ls, grep, etc., are widely available).
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The   Hash Table
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	
	Hash tables (also known as associative arrays or dictionaries) are a powerful data structure that allows you to store and retrieve values using keys, rather than numeric indices.  While Bash doesn't have a dedicated "hash table" type, it does offer associative arrays, which serve the same purpose.  Associative arrays were introduced in Bash version 4.0.

Here's how you work with associative arrays in Bash:

1. Declaring an Associative Array:

You must declare an associative array before you can use it.

Bash

declare -A my_assoc_array
The -A option tells Bash that my_assoc_array is an associative array.

2. Assigning Values:

You assign values to an associative array using keys (which are strings) as indices:

Bash

my_assoc_array["name"]="John Doe"
my_assoc_array["age"]="30"
my_assoc_array["city"]="New York"
3. Accessing Values:

You access values using the keys:

Bash

echo "${my_assoc_array["name"]}"  # Output: John Doe
echo "${my_assoc_array["age"]}"   # Output: 30
echo "${my_assoc_array["city"]}"  # Output: New York
Important:  Always quote the key inside the square brackets, especially if the key contains spaces or special characters.

4. Listing Keys:

You can get a list of all keys in the array using ${!my_assoc_array[@]} or ${!my_assoc_array[*]}:

Bash

echo "${!my_assoc_array[@]}"  # Output: name age city (separated by spaces)

# Iterate over the keys:
for key in "${!my_assoc_array[@]}"; do
  echo "Key: $key"
done
5. Listing Values:

You can get a list of all values using ${my_assoc_array[@]} or ${my_assoc_array[*]} (similar to indexed arrays):

Bash

echo "${my_assoc_array[@]}" # Output: John Doe 30 New York
6. Iterating through Key-Value Pairs:

The most common way to use associative arrays is to iterate through the key-value pairs:

Bash

for key in "${!my_assoc_array[@]}"; do
  value="${my_assoc_array[$key]}"
  echo "Key: $key, Value: $value"
done
7. Checking if a Key Exists:

You can check if a key exists in the array:

Bash

if [[ -v my_assoc_array["name"] ]]; then  # -v checks if a variable is set
  echo "Key 'name' exists"
fi

if [[ ! -v my_assoc_array["country"] ]]; then
  echo "Key 'country' does not exist"
fi
8. Unsetting an Element:

You can remove an element from the array using unset:

Bash

unset my_assoc_array["age"]
9. Example: Counting Word Occurrences:

Bash

declare -A word_counts

while read word; do
  if [[ -v word_counts[$word] ]]; then
    word_counts[$word]=$((word_counts[$word] + 1))
  else
    word_counts[$word]=1
  fi
done < input.txt

for word in "${!word_counts[@]}"; do
  count="${word_counts[$word]}"
  echo "$word: $count"
done
This script reads words from input.txt and counts how many times each word appears.

Key Considerations:

Bash 4.0+: Associative arrays are only available in Bash version 4.0 and later.
Keys are Strings: Keys in associative arrays are always strings.
Quoting: Quote keys, especially if they contain spaces or special characters.
Iteration: Iterating through the keys (${!my_assoc_array[@]}) and then using those keys to access the values is the standard way to process all elements in an associative array.
	
-------------------------	
	Bash

#!/bin/bash

# Demonstrate the use of the hash table

# 1. Initial state: The hash table is likely empty (or contains very few entries).

echo "Initial hash table:"
hash  # Display the hash table (should be mostly empty)

# 2. Run an external command for the first time.  This will populate the hash table.

echo "Running 'ls -l' for the first time..."
ls -l  # The shell will search the PATH for 'ls' and add it to the hash table

echo "Hash table after running 'ls -l':"
hash  # Display the updated hash table

# 3. Run 'ls -l' again.  It should be faster now because the shell will use the hash table.

echo "Running 'ls -l' again (should be faster)..."
ls -l  # The shell should use the hash table entry for 'ls'

# 4. Add a new directory to the PATH. Let's create a dummy directory and put a script in it.

mkdir my_scripts
echo '#!/bin/bash' > my_scripts/my_script.sh
chmod +x my_scripts/my_script.sh
export PATH="$PATH:$PWD/my_scripts" # Add the current directory's my_scripts directory to the PATH

# 5. The new script is in the path, but not in the hash table.

echo "Running 'my_script.sh' for the first time..."
my_script.sh  # The shell will search the updated PATH

echo "Hash table after running 'my_script.sh':"
hash # my_script.sh should now be in the hash table

# 6. Run the new script again.

echo "Running 'my_script.sh' again (should be faster)..."
my_script.sh # Should be faster now

# 7. Remove the script from the hash table.

echo "Removing 'my_script.sh' from the hash table..."
hash -d my_script.sh

echo "Hash table after removing 'my_script.sh':"
hash

# 8. Run the script again (it will search the path again).

echo "Running 'my_script.sh' again (will search PATH again)..."
my_script.sh

# 9. Clear the hash table.

echo "Clearing the hash table..."
hash -r

echo "Hash table after clearing:"
hash

# 10. Run ls again, it will have to search the PATH again
echo "Running ls -l again, it will have to search PATH again..."
ls -l

# Cleanup: Remove the dummy directory and script.
rm -rf my_scripts

exit 0
Explanation:

Initial State: The script starts by displaying the initial hash table, which should be mostly empty (or contain a few system commands).

First Run: When ls -l is run for the first time, the shell searches the PATH environment variable to find the ls executable.  Once it finds it, it adds the path to the hash table.

Second Run: The second time ls -l is run, the shell checks the hash table first.  Since ls is there, it uses the stored path directly, skipping the PATH search.  This makes the second run faster.

Adding to PATH: The script then creates a directory, adds a script to it, and adds that directory to the PATH.

New Script (First Run):  When the new script my_script.sh is executed the first time, it's not in the hash table, so the shell must search the updated PATH.

New Script (Second Run): The second time the new script is run, it should be faster because it's now in the hash table.

Removing from Hash: The script then removes the new script from the hash table using hash -d.

Running After Removal:  The next time the script is run, the shell has to search the PATH again.

Clearing the Hash: The hash -r command clears the entire hash table.

Running after clear: When ls is run again, it has to search the PATH again.

How to run:

Save the script to a file (e.g., hash_demo.sh).
Make it executable: chmod +x hash_demo.sh
Run it: ./hash_demo.sh
The output of the script will demonstrate how the hash table is populated and used by the shell to speed up command lookups. You'll see the hash table's contents change as commands are executed and removed.  You will not notice a dramatic speed increase with such a simple example, but when running many commands, especially in scripts, the performance improvement from the hash table can be significant.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Set and Shopt Commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The set and shopt commands in Bash (and other shells) are used to control various shell options and behaviors. They provide a way to customize how the shell interprets commands, handles variables, and operates in general.

set Command:

The set command has a wide range of options, but here are some of the most frequently used:

set -u (or set -o nounset): Treats unset variables as an error. If you try to use an unset variable, the script will exit with an error.  This is excellent for catching typos and ensuring variables are initialized.

set -e (or set -o errexit): Causes the script to exit immediately if any command fails (returns a non-zero exit status). This is crucial for robust scripting, preventing a script from continuing after an error.

set -x (or set -o xtrace): Enables command tracing.  The shell will print each command before it's executed, which is very helpful for debugging.

set +u, set +e, set +x: These turn off the corresponding options enabled by set -u, set -e, and set -x.

set -o noclobber: Prevents redirection from overwriting existing files (using > for redirection will fail if the file exists).  This helps prevent accidental data loss.

set +o noclobber: Turns off the noclobber option.

set -A array_name value1 value2 ...: Assigns values to an array.

set (without options): Displays all shell variables and their values.

shopt Command:

The shopt command is used specifically for setting and unsetting shell options that control more advanced shell behaviors.

shopt -s option_name: Enables the specified option.
shopt -u option_name: Disables the specified option.
shopt option_name: Displays whether the specified option is enabled or disabled.
shopt (without arguments): Lists the status of all shell options.
Common shopt Options:

shopt -s extglob: Enables extended globbing patterns (like +(...), *(...), ?(...), @(...), !(...)). This allows for more complex file matching.
shopt -u extglob: Disables extended globbing.
shopt -s nocaseglob: Makes file matching case-insensitive.
shopt -u nocaseglob: Makes file matching case-sensitive.
shopt -s histappend: Appends new commands to the history file instead of overwriting it.
shopt -s cdable_vars: Allows you to use environment variables as directory names with cd. For example, if MYDIR is set to /path/to/my/dir, you can use cd $MYDIR.
shopt -s expand_aliases: Expands aliases.
shopt -s checkwinsize: Checks the terminal window size and updates the COLUMNS and LINES environment variables.
shopt -s dotglob: Allows globbing to match files that begin with a dot (.).
Examples:

Bash

# Using set
set -u  # Treat unset variables as an error
set -e  # Exit on command failure
set -x  # Enable command tracing

# Using shopt
shopt -s extglob  # Enable extended globbing
shopt -s nocaseglob # Make filename matching case-insensitive

# Check the status of an option
shopt extglob

# List all shopt settings
shopt
Where to put these commands:

~/.bashrc: For interactive non-login shells (most terminal windows). This is where you'd typically put settings that affect your interactive shell behavior.
~/.bash_profile: For login shells (when you log in to a terminal).
Within a script: You can put set and shopt commands directly within a shell script to control its behavior. This is particularly useful for options like -u, -e, and -x to ensure robust script execution.
Best Practices:

Use set -u and set -e in your scripts to catch errors early.
Use set -x for debugging.
Be careful with set -o noclobber as it can sometimes be inconvenient.
Use shopt for more advanced shell options.
By using set and shopt effectively, you can fine-tune your shell environment and write more robust and maintainable shell scripts.  They provide a powerful way to customize the shell's behavior to match your specific needs.
	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
SHELL SCRIPTS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• What is a Shell Script?
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	A shell script is essentially a program written for the shell (like Bash, Zsh, or Sh).  It's a text file that contains a sequence of commands that the shell interprets and executes.  Think of it as a way to automate tasks that you would normally do manually in the terminal.   

Here's a breakdown of what a shell script is and why it's useful:

Key Concepts:

Text File: A shell script is a plain text file. You create it using a text editor.   
Commands: The script contains commands that you would normally type in your terminal. These can be anything from simple commands like ls or cd to more complex commands or even other shell scripts.   
Automation: The primary purpose of a shell script is to automate tasks. Instead of typing the same sequence of commands repeatedly, you can put them in a script and run the script.   
Control Flow: Shell scripts can include control flow structures like if statements, for loops, and while loops, allowing you to create more complex and conditional logic.
Variables: Shell scripts can use variables to store and manipulate data.   
Functions: You can define functions within a shell script to organize your code and make it more modular.   
Shebang: The first line of a shell script is often a "shebang" (e.g., #!/bin/bash). This tells the system which interpreter to use to run the script (in this case, Bash).
Why Use Shell Scripts?

Automation: Automate repetitive tasks, saving time and reducing the risk of errors.   
Batch Processing: Process multiple files or data sets in a single operation.   
System Administration: Perform system maintenance tasks, backups, and other administrative jobs.   
Task Scheduling: Use cron jobs to schedule shell scripts to run automatically at specific times.   
Custom Tools: Create your own custom command-line tools.   
Simplified Complex Tasks: Break down complex tasks into smaller, manageable steps.   
Example:

Let's say you want to create a backup of your important files every day.  You could write a shell script like this:

Bash

#!/bin/bash

# Create a timestamped filename for the backup
timestamp=$(date +%Y%m%d)
backup_file="backup_$timestamp.tar.gz"

# Create the backup archive
tar -czvf "$backup_file" /path/to/your/important/files

# (Optional) Copy the backup to a remote server
# scp "$backup_file" user@remote_server:/backup/directory

echo "Backup created: $backup_file"
This script would:

Get the current date and use it to create a filename for the backup.
Use the tar command to create a compressed archive of your files.
(Optionally) copy the backup to a remote server.
Print a message indicating that the backup has been created.
You can then schedule this script to run automatically every day using cron.

Key Benefits of Shell Scripting:

Efficiency: Automate tasks and save time.   
Consistency: Ensure tasks are performed the same way every time.
Reduced Errors: Minimize manual errors.
Flexibility: Combine existing commands in powerful ways.   
Portability: Shell scripts can often be run on different Unix-like systems.
Shell scripting is a fundamental skill for anyone working in a Linux or Unix environment. It allows you to take control of your system and automate a wide range of tasks.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Use of shell script
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Shell scripts are used for a vast array of tasks in Linux and Unix-like environments. Here are some of the most common use cases, categorized for clarity:

System Administration:

Backups: Automating the process of backing up files and directories, often with compression and timestamping.
System Monitoring: Checking system health (CPU usage, memory, disk space) and generating alerts or logs.
User Management: Creating, modifying, or deleting user accounts.
Software Installation and Configuration: Automating the installation and configuration of software packages.
Log File Analysis: Parsing and analyzing log files to identify errors, trends, or other important information.
Process Management: Starting, stopping, or restarting services and daemons.
File System Maintenance: Cleaning up temporary files, checking disk space, and performing other file system maintenance tasks.
Scheduling Tasks: Using cron jobs to schedule scripts to run automatically at specific times or intervals.
Automation:

Data Processing: Transforming and manipulating data in text files or other formats.
Report Generation: Creating formatted reports from data.
Batch Processing: Performing the same set of operations on multiple files or data sets.
Workflow Automation: Automating complex sequences of commands and tasks.
Build and Deployment: Automating the build process for software projects and deploying applications to servers.
User-Specific Tasks:

Personal Productivity: Automating repetitive tasks in your own workflow.
Custom Commands: Creating your own custom command-line tools.
File Management: Renaming, moving, copying, or deleting files in bulk.
Application Launching: Creating scripts to launch applications with specific options.
Development:

Build Scripts: Automating the compilation and linking of code.
Testing: Running automated tests.
Deployment: Deploying applications to different environments.
Examples of Specific Tasks:

Creating a daily backup of your home directory: A script could use tar to create a compressed archive of your home directory and save it to a backup location.
Checking disk space and sending an email alert if it's low: A script could use df to check disk space and mail to send an email if the threshold is reached.
Converting a batch of images to a different format: A script could use convert (ImageMagick) to convert multiple images from PNG to JPG.
Automating the process of deploying a web application: A script could copy files to a web server, restart the web server, and update the database.
Creating a script to quickly search for files containing specific text: This could combine find and grep.
Writing a script to automate the installation of all the necessary packages on a new system: This could use the system's package manager (e.g., apt, yum, pacman).
Key Benefits of Using Shell Scripts:

Efficiency: Automate repetitive tasks and save time.
Consistency: Ensure tasks are performed the same way every time.
Reduced Errors: Minimize manual errors.
Flexibility: Combine existing commands in powerful ways.
Portability: Shell scripts can often be run on different Unix-like systems.
Maintainability: Scripts can be easily modified and updated.
Shell scripting is a powerful tool for anyone working in a Linux or Unix environment.  It allows you to automate a wide range of tasks, from simple file management to complex system administration and development workflows.  Learning shell scripting is an invaluable skill for increasing productivity and managing your system effectively.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Writing shell script
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	1. Creating the Script File:

Use a text editor (like nano, vim, gedit, or any IDE) to create a new file. Give it a descriptive name with a .sh extension (e.g., my_script.sh, backup_script.sh). The .sh extension isn't strictly required, but it's a convention that helps identify shell scripts.
2. The Shebang (Important!):

The very first line of your script should be the shebang line:
Bash

#!/bin/bash
This line tells the system which interpreter to use to execute the script. In this case, /bin/bash specifies Bash. If you're using a different shell (like Zsh), change it accordingly (e.g., #!/bin/zsh). While scripts might run without a shebang, it's best practice to include it for clarity and portability.
3. Commands:

Write the commands you want the script to execute, one command per line. These can be any commands you'd normally use in the terminal.
Bash

ls -l  # List files in long format
pwd   # Print the current working directory
echo "Hello, world!" # Print a message
4. Comments:

Use # to add comments to your script. Comments are ignored by the shell but are essential for explaining your code.
Bash

# This is a comment
ls -l # This command lists files (with a trailing comment)
5. Variables:

Use variables to store and manipulate data.
Bash

name="John"
echo "Hello, $name!"  # Use $ to access the variable's value

age=30
((age++)) # Increment age
echo "Age: $age"
6. Input and Output:

read: Reads input from the user.
Bash

read -p "Enter your name: " user_name
echo "Hello, $user_name!"
echo: Prints output to the terminal.
Redirection: Use > to redirect output to a file, >> to append, and < to redirect input from a file.
Bash

ls -l > file_list.txt  # Redirect output of ls to file_list.txt
echo "Error message" > error.log
cat < input.txt # Use content of input.txt as input for cat command
7. Control Flow:

if statements:
Bash

if [ $age -ge 18 ]; then  # -ge means "greater than or equal to"
  echo "$name is an adult."
else
  echo "$name is a minor."
fi
for loops:
Bash

for file in *.txt; do
  echo "Processing: $file"
  # ... commands to process each file ...
done
while loops:
Bash

count=0
while [ $count -lt 5 ]; do
  echo $count
  ((count++))
done
case statements:
Bash

case $fruit in
  apple)
    echo "Red fruit"
    ;;
  banana)
    echo "Yellow fruit"
    ;;
  *) # Default case
    echo "Unknown fruit"
    ;;
esac
8. Functions:

Define functions to organize your code.
Bash

greet() {
  echo "Hello, $1!"  # $1 is the first argument to the function
}

greet "Alice"
greet "Bob"
9. File Permissions:

After creating the script, make it executable:
Bash

chmod +x my_script.sh
10. Running the Script:

Run the script from the terminal:
Bash

./my_script.sh  # Use ./ to specify the current directory
Example (Combining Concepts):

Bash

#!/bin/bash

# Function to check if a file exists
file_exists() {
  if [ -f "$1" ]; then
    return 0  # True (file exists)
  else
    return 1  # False (file does not exist)
  fi
}

# Main part of the script
filename="myfile.txt"

if file_exists "$filename"; then
  echo "$filename exists."
  cat "$filename" # Print the file contents
else
  echo "$filename does not exist. Creating it..."
  touch "$filename"  # Create an empty file
fi

# Loop through files in the current directory
for f in *; do
  echo "File: $f"
done
This example demonstrates functions, if statements, file existence checks, and loops.  It's a more complete example of a shell script.  Remember to practice and experiment!  The best way to learn shell scripting is by doing.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Invoking scripts using sh
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	You can invoke a shell script using sh in a few ways, but it's important to understand the implications.  sh often (but not always) refers to a minimal, POSIX-compliant shell (like the original Bourne shell or a compatible shell).  This means your script needs to adhere to POSIX standards for it to work correctly.

Here are the common ways to invoke a script using sh:

1. Explicitly using sh:

Bash

sh my_script.sh
This is the most direct way.  It tells the system to use the sh interpreter to execute my_script.sh.

2. Using the shebang (if present and executable):

If your script has the shebang line #!/bin/sh (or a similar shebang pointing to a POSIX-compliant shell), and it's executable (chmod +x my_script.sh), you can run it directly:

Bash

./my_script.sh
Even if /bin/sh is a symbolic link to bash on your system, bash will run in a compatibility mode, interpreting the script as if it were being run by sh.

3. Sourcing the script (in the current shell):

Bash

. my_script.sh  # or source my_script.sh
This is different from executing the script. Sourcing runs the script's commands within the current shell environment. This means any changes the script makes to environment variables, aliases, or functions will remain in effect after the script finishes.  This is often used for setting up shell configurations.

Important Considerations and Potential Issues:

POSIX Compliance: The biggest issue is POSIX compliance.  If your script uses Bash-specific features (like arrays, associative arrays, extended globbing, or some Bash-specific syntax), it might not work correctly when invoked with sh.  sh expects a more basic, POSIX-compatible syntax.

Symbolic Links: On many Linux systems, /bin/sh is actually a symbolic link to /bin/bash.  In this case, even if you use sh, Bash will execute the script.  However, it will run in a compatibility mode, trying to adhere to POSIX standards.  This means that even though Bash is running, it will disable or restrict many of its advanced features.

Portability: If you want your script to be portable across different Unix-like systems, it's generally better to stick to POSIX-compliant syntax so that it will work consistently with sh on different systems.

Shebang Consistency: It's crucial that the shebang in your script (#!/bin/sh or #!/bin/bash) matches how you intend to run it.  If you intend it to be a POSIX-compliant script, use #!/bin/sh.  If you need Bash-specific features, use #!/bin/bash.

Example (POSIX-compliant script):

Bash

#!/bin/sh

name="John"
echo "Hello, $name!"

if [ "$1" = "hello" ]; then
  echo "Argument is hello"
else
  echo "Argument is not hello"
fi

for i in 1 2 3; do
  echo $i
done
This script should work correctly when invoked with sh because it uses POSIX-compliant syntax.

Example (Bash-specific script - might NOT work with sh):

Bash

#!/bin/bash

my_array=("apple" "banana" "orange")  # Bash array

for fruit in "${my_array[@]}"; do # Bash array iteration
  echo "$fruit"
done

[[ $1 == "hello" ]] && echo "Argument is hello" # Bash conditional expression
This script uses Bash-specific array syntax and conditional expressions. It will likely produce errors or unexpected behavior if run directly with sh.

In summary:

If you want maximum portability, write your scripts using POSIX-compliant syntax and use #!/bin/sh as the shebang.
If you need Bash-specific features, use #!/bin/bash and invoke the script using bash or ./my_script.sh (assuming it's executable).
Be aware that even if /bin/sh points to bash, running a script with sh or ./my_script.sh (with a #!/bin/sh shebang) will make bash run in a POSIX-compatible mode, potentially limiting its features.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Invoking scripts by granting execute permission
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Granting execute permission to a shell script is essential for being able to run it directly from the command line.  Here's how it works and what you need to know:

1. The Execute Permission (x):

In Linux (and other Unix-like systems), file permissions control who can access and interact with a file.  There are three basic types of permissions:

Read (r): Allows viewing the file's contents.
Write (w): Allows modifying the file's contents.
Execute (x): Allows running the file as a program (if it's an executable file, like a script or a compiled program).
2. Setting the Execute Permission:

You use the chmod command to change file permissions. To grant execute permission to a script, you use the following command:

Bash

chmod +x my_script.sh
chmod: The command for changing file permissions.
+x: Adds execute permission.
my_script.sh: The name of the script file.
3. Running the Script:

Once the execute permission is set, you can run the script directly from the command line:

Bash

./my_script.sh
./: This is important! It tells the shell to look for the script in the current directory. If you don't include ./, the shell will search the directories listed in your PATH environment variable, and if your script isn't in one of those directories, it won't be found.
4. The Shebang (Important!):

For the script to execute correctly, it must have a shebang line as the very first line:

Bash

#!/bin/bash  # Or #!/bin/sh, #!/usr/bin/python3, etc.
The shebang tells the system which interpreter to use to run the script (Bash in this example).  Without the shebang, the system might try to execute the script using the default shell, which might not be what you intend.

5. Example:

Let's say you have a script named my_script.sh with the following content:

Bash

#!/bin/bash

echo "Hello, world!"
Here's the process to make it executable and run it:

Bash

chmod +x my_script.sh
./my_script.sh
This will output:

Hello, world!
6.  Running without ./ (if the script's directory is in PATH):

If the directory containing your script is already in your PATH environment variable, you can run the script without ./:

Bash

my_script.sh  # If the script is in a directory in $PATH
However, this is generally not recommended for scripts in your current directory. It's best to use ./ to avoid confusion and ensure you're running the intended script.

7.  Permissions and Security:

Be cautious about granting execute permission to scripts from untrusted sources. Running a script can execute arbitrary commands on your system.
You can use other chmod options to fine-tune permissions (e.g., chmod 755 my_script.sh for typical script permissions).
In summary:

Write the script: Create the script file with the desired commands and the correct shebang.
Set execute permission: chmod +x my_script.sh
Run the script: ./my_script.sh (or my_script.sh if the directory is in PATH).
Following these steps will allow you to execute your shell scripts directly from the command line.  Remember the shebang; it's crucial for the correct interpreter to be used.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Quoting mechanism
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Quoting is essential in shell scripting because it controls how the shell interprets special characters and words.  Without proper quoting, your commands might not behave as expected.  Here's a breakdown of the different quoting mechanisms in the shell and when to use them:

1. Single Quotes ('):

Purpose: Encloses a string literal exactly as it appears, preventing the shell from interpreting any special characters within the quotes.
What's protected: All characters within single quotes are treated literally, including spaces, tabs, newlines, and most special characters (except ' itself). Variable substitution ($) and command substitution ($(...)) are not performed.
When to use: When you want the shell to treat a string literally, without any interpretation. This is often used for strings containing special characters, regular expressions, or commands that you don't want the shell to expand.
Example:
Bash

echo 'Hello, $USER! Today is $(date)'  # Output: Hello, $USER! Today is $(date)
grep '^\s*#' myfile.txt  # Search for lines starting with zero or more whitespace characters and a #
2. Double Quotes ("):

Purpose: Encloses a string, allowing for some interpretation of special characters.
What's protected: Most characters are treated literally, but variable substitution ($) and command substitution ($(...)) are performed. Some special characters (like *, ?, [, ]) are still interpreted for filename globbing unless they are escaped with a backslash (\).
When to use: When you want variable or command substitution to occur within a string, but you still want to protect spaces and other characters from word splitting. This is the most commonly used type of quoting.
Example:
Bash

name="John"
echo "Hello, $name! Today is $(date)"  # Output: Hello, John! Today is Tue Oct 24 10:00:00 IST 2023 (or the current date and time)
ls -l "My file with spaces.txt" # Correctly handles filenames with spaces
3. Backslashes (\):

Purpose: Escapes a single character, preventing the shell from interpreting its special meaning.
What's protected: The character immediately following the backslash is treated literally.
When to use: When you want to include a single special character within a string without its special meaning.
Example:
Bash

echo "This string contains a \"quote\" and a \$dollar sign."  # Output: This string contains a "quote" and a $dollar sign.
echo "The path is: /home/user/files\ with\ spaces" # Prevents spaces from being interpreted as word separators.
4. Command Substitution ($(...) or `... `):

Purpose: Captures the output of a command and substitutes it into a string.
What's protected: The command within the parentheses or backticks is executed, and its output is used as part of the string.
When to use: When you want to include the output of a command within another command or string.
Example:
Bash

files=$(ls)  # Store the output of ls in the files variable
echo "Files: $files"
echo "Current date and time: $(date)"
5. Arithmetic Expansion $((...)):

Purpose: Performs arithmetic calculations.
What's protected: The expression within the double parentheses is evaluated as an arithmetic expression.
When to use: When you want to perform calculations within a shell script.
Example:
Bash

x=10
y=20
sum=$((x + y))
echo "Sum: $sum"
Which quote to use?

Single quotes ('): For literal strings, regular expressions, or commands that should not be expanded.
Double quotes ("): For strings where you want variable and command substitution to occur, but you also need to protect spaces and other characters from word splitting. This is the most common case.
Backslashes (\): For escaping individual special characters.
Command substitution ($(...)): For capturing and using the output of a command.
Arithmetic expansion $((...)): For performing calculations.
Understanding the different quoting mechanisms is crucial for writing correct and robust shell scripts.  Choosing the right type of quote will ensure that your commands are interpreted as you intend them to be.
	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Day – 4
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
POSITIONAL PARAMETERS/COMMANDLINE ARGUMENTS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Concept of command line arguments
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Command-line arguments are values that you provide to a script when you run it from the terminal. They allow you to make your scripts more flexible and reusable by passing different parameters to them.

How it Works:

When you run a script, the shell automatically stores the command-line arguments in special variables.

$0: The name of the script itself (including the path if you specify one).
$1, $2, $3, etc.: The first, second, third, etc., arguments, respectively.
$#: The total number of arguments (excluding the script name).
$* or $@: All the arguments as a single word or as separate words (see explanation below).
Example:

Let's say you have a script named greet.sh:

Bash

#!/bin/bash

echo "Script name: $0"
echo "Hello, $1!"
echo "There are $# arguments."
echo "All arguments (using \$*): $*"
echo "All arguments (using \$@): $@"
echo "First argument: $1"
echo "Second argument: $2"
echo "Third argument: $3"
If you run it like this:

Bash

./greet.sh John Doe "How are you?"
The output will be:

Script name: ./greet.sh
Hello, John!
There are 3 arguments.
All arguments (using $*): John Doe How are you?
All arguments (using $@): John Doe How are you?
First argument: John
Second argument: Doe
Third argument: How are you?
Key Differences between $* and $@:

"$*" (double quotes around $*): Treats all arguments as a single word. If you have arguments with spaces, they will be treated as one long string.
"$@" (double quotes around $@): Treats each argument as a separate word, even if they contain spaces. This is almost always what you want.
Example illustrating $* vs $@:

Bash

#!/bin/bash

echo "Using \$*:"
for arg in "$*"; do
  echo "Argument: $arg"
done

echo "" # Empty Line for better readability

echo "Using \$@:"
for arg in "$@"; do
  echo "Argument: $arg"
done
If you run this script with ./test_args.sh "Hello world" how are you, the output will be:

Using $*:
Argument: Hello world how are you

Using $@:
Argument: Hello world
Argument: how
Argument: are
Argument: you
As you can see, "$@" correctly treats "Hello world" as a single argument, while "$*" combines all arguments into one.

Using shift to process arguments:

The shift command is very useful for iterating through command-line arguments. It discards the first argument ($1) and shifts all the other arguments down by one (so $2 becomes $1, $3 becomes $2, and so on).

Bash

#!/bin/bash

while [ $# -gt 0 ]; do  # Loop as long as there are arguments
  case "$1" in
    -n)
      echo "Name option found. Next argument is a name: $2"
      shift 2  # Shift by 2 to consume the option and its value
      ;;
    -a)
      echo "Age option found. Next argument is an age: $2"
      shift 2
      ;;
    *) # Default case (if no option is matched)
      echo "Processing argument: $1"
      shift  # Shift by 1 to process the next argument
      ;;
  esac
done
Example usage of the script with options:

Bash

./process_args.sh -n "John Doe" -a 30 "Extra argument 1" "Extra argument 2"
Checking for arguments:

You can use [ $# -eq 0 ] to check if no arguments were provided.

Bash

if [ $# -eq 0 ]; then
  echo "No arguments provided."
  exit 1
fi
Best Practices:

Always use "$@" when iterating over arguments to handle arguments with spaces correctly.
Use shift to process arguments, especially when dealing with options.
Validate user input. Check if the correct number of arguments is provided and if they are of the expected type. Use [ $# -eq n ] to check if the number of arguments is equal to n.
Use meaningful variable names for your arguments.
Command-line arguments are a powerful tool for creating flexible and reusable shell scripts.  By understanding how to use them, you can write scripts that can adapt to different situations and perform a wider range of tasks.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Positional parameters – $1, $2, …,$?, $#, $*, $@, $$
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Let's break down the special shell variables that represent positional parameters and other related values. These are crucial for working with command-line arguments and understanding how your shell scripts interact with the input they receive.

Positional Parameters:

$1, $2, $3, ... , $9: These represent the first, second, third, ..., ninth command-line arguments passed to the script. If you have more than nine arguments, you'll need to use shift (explained later) to access them.
${10}, ${11}, ...: For arguments beyond the ninth, you must use curly braces {} to enclose the number. $10 would be interpreted as $1 followed by a 0, not the tenth argument.
Special Parameters:

$0: The name of the script itself (including the path if you specify one).
$#: The total number of command-line arguments (excluding the script name).
$*: All the arguments as a single word. If an argument contains spaces, it is treated as one single argument. This is less commonly used than $@.
$@: All the arguments as separate words. This is almost always the preferred way to access all arguments, especially if they might contain spaces.
$?: The exit status of the last executed command. A value of 0 usually indicates success, while any other value indicates an error.
$$: The process ID (PID) of the current shell.
Key Differences between $* and $@ (with examples):

The crucial difference lies in how they handle arguments with spaces.

Example Script (args.sh):

Bash

#!/bin/bash

echo "Using \$*:"
for arg in "$*"; do
  echo "Argument: $arg"
done

echo ""  # Empty line for readability

echo "Using \$@:"
for arg in "$@"; do
  echo "Argument: $arg"
done
Running the script:

Bash

./args.sh "Hello world" how are you
Output:

Using $*:
Argument: Hello world how are you

Using $@:
Argument: Hello world
Argument: how
Argument: are
Argument: you
As you can see, "$*" treats "Hello world" as a single argument, while "$@" correctly treats it as a separate argument.  Always use "$@" when you want to iterate over command-line arguments.

Using shift:

The shift command is essential for processing more than nine arguments or for iterating through arguments in a loop.  shift discards the first argument ($1) and shifts all the other arguments down by one (so $2 becomes $1, $3 becomes $2, and so on).

Example:

Bash

#!/bin/bash

while [ $# -gt 0 ]; do  # Loop as long as there are arguments
  echo "Processing argument: $1"
  shift  # Shift to the next argument
done
Example with options:

Bash

#!/bin/bash

while [ $# -gt 0 ]; do
  case "$1" in
    -n)
      echo "Name option found. Name is: $2"
      shift 2  # Shift twice to consume the option and its value
      ;;
    -a)
      echo "Age option found. Age is: $2"
      shift 2
      ;;
    *) # Default case (if no option is matched)
      echo "Unknown option: $1"
      shift
      ;;
  esac
done
Checking the exit status ($?):

Bash

#!/bin/bash

ls non_existent_file.txt  # This will likely fail

if [ $? -ne 0 ]; then
  echo "The previous command failed with exit status: $?"
  exit 1  # Exit the script with an error code
fi

echo "The previous command succeeded." # This won't be reached if the ls command fails.
Getting the PID ($$):

Bash

echo "The PID of this script is: $$"
This is often used for creating unique filenames or for logging purposes.

Summary Table:

Variable	Description
$1 - $9	First through ninth command-line arguments
${10}, ${11}, ...	Tenth and subsequent arguments (use curly braces)
$0	Name of the script
$#	Number of arguments (excluding the script name)
$*	All arguments as a single word
$@	All arguments as separate words (use this one!)
$?	Exit status of the last command
$$	Process ID of the current shell

Export to Sheets
Understanding these special variables is fundamental to writing shell scripts that can handle user input and interact with the system effectively.  Remember to use "$@" for iterating over arguments and shift for processing them sequentially or handling options.  The exit status ($?) is crucial for error handling, and the PID ($$) can be useful in various situations.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• $? – exit status of program
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The $? variable in shell scripting (Bash, sh, zsh, etc.) holds the exit status (or return code) of the most recently executed command.  It's a crucial part of error handling and controlling the flow of your scripts.

What is an Exit Status?

Every command (program, script, or even a single built-in command) that runs in a shell returns an exit status – a small integer value that indicates whether the command succeeded or failed.

0 (Zero): A zero exit status typically signifies success. The command completed without any errors.
Non-zero: A non-zero exit status indicates that an error occurred. Different non-zero values can represent different types of errors. For example, a file not found error might have a different exit status than a permission denied error.
How to Check the Exit Status:

Immediately after running a command, you can check its exit status by accessing the $? variable:

Bash

command_that_might_fail  # Run the command
echo "Exit status: $?"    # Print the exit status
Using the Exit Status in Scripts:

The exit status is most useful within shell scripts to control the flow of execution.  Here are some common ways to use it:

Conditional Execution (if statement):

Bash

if command_that_might_fail; then  # If the command succeeds ($? is 0)
    echo "Command succeeded!"
    # ... more commands to execute if successful ...
else  # If the command fails ($? is non-zero)
    echo "Command failed with exit status: $?"
    exit 1  # Exit the script with an error code (1)
fi
Checking for specific error codes:

Bash

command_that_might_fail
exit_code=$?

if [ $exit_code -eq 1 ]; then
    echo "File not found error!"
elif [ $exit_code -eq 2 ]; then
    echo "Permission denied error!"
else
    echo "Some other error occurred: $exit_code"
fi
Using && (AND) and || (OR) operators for short-circuiting:

command1 && command2: command2 will only run if command1 succeeds (returns 0).
command1 || command2: command2 will only run if command1 fails (returns non-zero).
Bash

# Example: Create a directory and then change to it (only if the creation is successful)
mkdir my_directory && cd my_directory

# Example: Try to remove a file, and if it fails, print an error message
rm my_file || echo "Failed to remove my_file"
Functions and Return Codes:

You can use the return command within a function to explicitly set the exit status of the function:

Bash

my_function() {
    if some_condition; then
        return 0  # Success
    else
        return 1  # Failure
    fi
}

my_function
if [ $? -eq 0 ]; then
    echo "Function succeeded."
fi
Best Practices:

Check exit statuses: Always check the exit status of commands in your scripts, especially those that might fail. This will make your scripts more robust.
Use meaningful exit codes: Use different non-zero exit codes to represent different error conditions. This will make debugging easier.
Document your exit codes: If you're writing a complex script, document the meaning of the different exit codes you use.
Example (Robust File Removal):

Bash

#!/bin/bash

file_to_remove="my_important_file.txt"

if rm "$file_to_remove"; then
    echo "File '$file_to_remove' removed successfully."
else
    echo "Error removing file '$file_to_remove'.  Exit status: $?"
    exit 1  # Exit with an error code
fi

# ... rest of your script ...
By using $? and checking exit statuses, you can write shell scripts that are more reliable and can gracefully handle errors, preventing unexpected behavior and making them much easier to maintain.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Shifting arguments - shift
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The shift command in shell scripting is a powerful tool for processing command-line arguments, especially when you have a variable number of arguments or when you're working with options and their values.

How shift Works:

shift discards the first argument ($1) and shifts all the remaining arguments down by one position.  So, $2 becomes $1, $3 becomes $2, and so on.  The original $1 is lost.

Basic Usage:

Bash

shift [n]
n (optional): If you provide a number n, it shifts the arguments by n positions. If you omit n, it defaults to 1.
Example (Shifting by one):

Bash

#!/bin/bash

echo "Arguments before shift: $@"  # Prints all arguments

shift

echo "Arguments after shift: $@"   # Prints all arguments *except* the first one
echo "First argument was: $1" # This will now print the original $2

shift 2 # Shift by 2

echo "Arguments after second shift: $@"   # Prints all arguments *except* the first three
echo "Second argument was: $1" # This will now print the original $4
Example (Processing arguments in a loop):

Bash

#!/bin/bash

while [ $# -gt 0 ]; do # Loop while there are arguments left ($# is the number of arguments).
  echo "Processing argument: $1"
  shift  # Shift to the next argument
done
This script will iterate through all the command-line arguments, one by one.

Example (Handling options and their values):

This is where shift becomes incredibly useful.  Let's say your script takes options like -n <name> and -a <age>.

Bash

#!/bin/bash

while [ $# -gt 0 ]; do
  case "$1" in
    -n)
      echo "Name option found. Name is: $2"
      name="$2"  # Store the name
      shift 2  # Shift by 2 to consume the option and its value
      ;;
    -a)
      echo "Age option found. Age is: $2"
      age="$2"  # Store the age
      shift 2  # Shift by 2
      ;;
    -h) # Help option
      echo "Usage: $0 [-n <name>] [-a <age>]"
      exit 0
      ;;
    *)  # Default case (if an invalid option is given)
      echo "Invalid option: $1"
      exit 1
      ;;
  esac
done

# Now you can use the 'name' and 'age' variables
echo "Name: $name"
echo "Age: $age"
Explanation of the options example:

while [ $# -gt 0 ]: The loop continues as long as there are arguments left.
case "$1" in: The case statement checks the value of the first argument ($1).
-n or -a: If the first argument is -n or -a, it means we've found an option. The script then takes the next argument ($2) as the value for that option.
shift 2: This is the crucial part. After processing the option and its value, we shift two positions to move past both the option (-n or -a) and its corresponding value.
*): The default case handles any invalid options.
shift (without a number): In the default case, we shift by one to process the next argument.
Key Points:

shift modifies the positional parameters in place. The original values are lost after the shift.
shift is essential for processing command-line arguments, especially when dealing with options and values or when the number of arguments is not fixed.
Using shift makes your scripts more robust and easier to maintain.
By using shift effectively, you can write shell scripts that can handle a variety of command-line input and provide more flexible functionality.  The options example is a very common and practical use case of shift.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Setting Positional Parameters
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	You can't directly set the positional parameters ($1, $2, etc.) in a shell script in the same way you would assign a value to a regular variable.  Positional parameters are automatically set by the shell when the script is invoked with arguments.  However, there are ways to achieve the effect of changing or manipulating them, primarily using the set command and the shift command.

1. Using the set command:

The set command can be used to set the positional parameters.  It treats the arguments you give it as the new positional parameters.

Bash

set "new value 1" "new value 2" "new value 3"

echo "First argument: $1"   # Output: First argument: new value 1
echo "Second argument: $2"  # Output: Second argument: new value 2
echo "Third argument: $3"  # Output: Third argument: new value 3
echo "Number of arguments: $#" # Output: Number of arguments: 3

# After the set command the original command line arguments passed to script are lost.
Important: Using set like this replaces the original command-line arguments passed to the script. They are no longer accessible.

2. Using shift (for processing or discarding):

While shift doesn't set positional parameters to arbitrary values, it's crucial for manipulating them.  It's used to discard the first n positional parameters and shift the rest down.  This is how you process arguments sequentially or handle options and their values.

Bash

# Example: Processing arguments one by one
while [ $# -gt 0 ]; do
  echo "Processing: $1"
  shift  # Discard $1 and shift the rest
done


# Example: Discard the first two arguments
echo "Original first argument: $1"
echo "Original second argument: $2"

shift 2

echo "New first argument (was the original third): $1"
echo "New second argument (was the original fourth): $2"
3. Combining set and shift (less common):

You can combine set and shift to achieve more complex manipulations, but this is less common. You might use set to create a new set of positional parameters, and then use shift to process them.  However, keep in mind that the original command-line arguments will still be lost.

4. Arrays (more flexible):

If you need to manipulate arguments more freely (add, remove, or access them in any order), it's often better to store them in an array.

Bash

#!/bin/bash

# Store all command-line arguments in an array
args=("$@")

# Access elements by index
echo "First argument: ${args[0]}"
echo "Second argument: ${args[1]}"
echo "Number of arguments: ${#args[@]}"

# Modify array elements
args[0]="modified value"
echo "Modified first argument: ${args[0]}"

# Add an element
args[3]="new argument"

# Iterate over the array
for arg in "${args[@]}"; do
  echo "Argument: $arg"
done

# You can even set positional parameters from the array if really needed, but it's not common.
set "${args[@]}"
echo "First positional parameter: $1"

Which method to use?

set: Use this if you genuinely want to replace the original command-line arguments with a new set of values. This is less common than other approaches.
shift: Use this for processing arguments sequentially, especially when handling options and values. This is the standard and most common way to work with command line arguments.
Arrays: Use arrays when you need more flexibility in accessing and manipulating arguments (random access, adding, removing). This is often the best approach for complex argument handling.
Important Note:  Modifying positional parameters directly (without using set or shift in the ways described) is generally not possible. They are automatically set by the shell when the script is invoked.  The techniques described above are how you effectively work with or change them.  Using arrays is generally the cleanest and most flexible way to handle command line arguments if you need more control than just processing them sequentially.
	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
EVALUATING CONDITIONS IN SHELL SCRIPTS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Conditional execution: test, [], [[ ]]
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	In shell scripting, conditional execution is essential for making decisions and controlling the flow of your scripts.  The test command, [ ] (single brackets), and [[ ]] (double brackets) are all used for conditional testing, but they have some key differences.

1. The test Command:

Purpose: The test command evaluates a conditional expression and returns a status of 0 (true) or 1 (false). It's a separate command, not a shell built-in (though some shells may have a built-in version for performance).
Syntax:
Bash

test expression
or (more commonly):

Bash

[ expression ]  # Note the spaces around the brackets!
Operators: test uses a specific set of operators (often different from the operators used in other conditional contexts).

File tests: -f (file exists and is a regular file), -d (directory exists), -x (executable), -r (readable), -w (writable), etc.
String comparisons: = (equals), != (not equals). Important: Be very careful with spaces around these operators. [ "a" = "b" ] is correct, but [ "a"= "b" ] is wrong.
Numeric comparisons: -eq (equals), -ne (not equals), -gt (greater than), -lt (less than), -ge (greater than or equals), -le (less than or equals).
Logical operators: -a (AND), -o (OR), ! (NOT).
Example:

Bash

if test -f "myfile.txt"; then
  echo "myfile.txt exists"
fi

if [ "$name" = "John" ]; then  # Important: Quote variables to handle spaces and empty values.
  echo "Name is John"
fi

if [ 10 -gt 5 ]; then
    echo "10 is greater than 5"
fi
2. Single Brackets [ ]:

Purpose: [ ] is functionally equivalent to the test command. It's just syntactic sugar.
Syntax:
Bash

[ expression ]  # Spaces are crucial!
Operators: Same operators as the test command.

Example:  (Same as the test examples above)

Bash

if [ -f "myfile.txt" ]; then
  echo "myfile.txt exists"
fi

if [ "$name" = "John" ]; then
  echo "Name is John"
fi

if [ 10 -gt 5 ]; then
    echo "10 is greater than 5"
fi
3. Double Brackets [[ ]] (Bash-specific):

Purpose: [[ ]] is a Bash extension that provides more powerful and flexible conditional testing. It's not POSIX compliant, so it might not work in other shells.
Syntax:
Bash

[[ expression ]]
Operators: [[ ]] supports the same file and numeric comparison operators as test and [ ], but it also adds:

String matching: =~ (regular expression matching).
Pattern matching: == (pattern matching).
No word splitting or globbing: You don't need to quote variables as carefully as with [ ].
Logical operators: && (AND), || (OR), ! (NOT). These are generally preferred over -a, -o, and ! from test and [].
Example:

Bash

if [[ -f "myfile.txt" ]]; then # No need to quote or escape special characters
  echo "myfile.txt exists"
fi

if [[ $name == "John" ]]; then # No need to quote $name
  echo "Name is John"
fi

if [[ $name =~ ^J.* ]]; then  # Regular expression matching
  echo "Name starts with J"
fi

if [[ 10 -gt 5 ]]; then
    echo "10 is greater than 5"
fi

if [[ "$file" == *.txt ]]; then # Pattern matching
  echo "$file is a txt file"
fi
Which to Use?

test or [ ]: Use these for POSIX-compliant scripts that need to work on various shells. Be very careful with quoting and spaces.
[[ ]]: Use this in Bash scripts when you need the extra features (regular expression matching, pattern matching, simplified quoting, and the more common &&/|| logical operators). It's generally preferred in Bash scripts because it's less error-prone.
Key Differences Summarized:

Feature	test	[ ]	[[ ]]
Syntax	test expr	[ expr ]	[[ expr ]]
POSIX	Yes	Yes	No (Bash extension)
Quoting	Important	Important	Less crucial
Regex	No	No	Yes (=~)
Pattern Matching	No	No	Yes (==)
Logical Operators	-a, -o, !	-a, -o, !	&&, `
Word Splitting/Globbing	Happens	Happens	Doesn't happen

Export to Sheets

	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	• Numeric comparison operators
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Numeric comparison operators are used to compare numerical values in shell scripts (Bash, sh, ksh, zsh, etc.).  They are essential for conditional execution and performing calculations based on numerical relationships.  Here's a breakdown of the common numeric comparison operators:

General Syntax:

Numeric comparisons are typically used within conditional expressions, most often with test, [ ] (single brackets), or [[ ]] (double brackets).

Operators:

Operator	Meaning	Example (using [ ])	Example (using [[ ]])
-eq	Equal to	[ 10 -eq 10 ]	[[ 10 -eq 10 ]]
-ne	Not equal to	[ 10 -ne 5 ]	[[ 10 -ne 5 ]]
-gt	Greater than	[ 10 -gt 5 ]	[[ 10 -gt 5 ]]
-lt	Less than	[ 5 -lt 10 ]	[[ 5 -lt 10 ]]
-ge	Greater than or equal to	[ 10 -ge 10 ]	[[ 10 -ge 10 ]]
-le	Less than or equal to	[ 5 -le 5 ]	[[ 5 -le 5 ]]

Export to Sheets
Important Notes:

Spaces are crucial: Be sure to include spaces around the comparison operators. [ 10 -eq 10 ] is correct, but [ 10-eq 10 ] will result in an error.
Integers: These operators are designed for comparing integers. While some shells might handle floating-point comparisons, it's not guaranteed to be consistent or accurate. For floating-point comparisons, it's generally better to use other tools (like bc or awk).
Variables: When comparing variables, it's good practice to quote them, especially if they might contain spaces or be empty. This prevents word splitting and globbing issues. For instance, [ "$x" -eq 10 ] is safer than [ $x -eq 10 ]. However, within [[ ]], quoting is often not strictly necessary unless you expect the variables to contain characters that might be interpreted by the shell (globbing characters, for example).
Context: The numeric comparison operators are typically used within conditional expressions (like if statements) to control the flow of your script.
Examples:

Bash

#!/bin/bash

x=15
y=10

if [ "$x" -gt "$y" ]; then
  echo "$x is greater than $y"
fi

if [[ $x -le 20 ]]; then # Quoting less important in [[ ]]
  echo "$x is less than or equal to 20"
fi

if [ "$y" -eq 10 ]; then
  echo "$y is equal to 10"
fi

if [[ $x -ne $y ]]; then # Quoting less important in [[ ]]
  echo "$x is not equal to $y"
fi

# Using arithmetic expansion within the comparison
z=$((x + y))
if [ "$z" -ge 25 ]; then
  echo "The sum of $x and $y is greater than or equal to 25"
fi
Floating-Point Comparisons (using bc):

If you need to compare floating-point numbers, the bc command (basic calculator) is a good option:

Bash

#!/bin/bash

x=3.14
y=2.71

if [[ $(bc <<< "$x > $y") -eq 1 ]]; then # bc returns 1 if true, 0 if false.
  echo "$x is greater than $y"
fi

if [[ $(bc <<< "scale=2; $x == $y") -eq 1 ]]; then # scale sets the decimal precision.
  echo "$x is equal to $y"
fi
Which to use?

For integer comparisons, [ ] or [[ ]] are common. [[ ]] is generally preferred in Bash scripts because it handles quoting and other special characters more gracefully.
For floating-point comparisons, use bc or awk.
By understanding and correctly using numeric comparison operators, you can write shell scripts that make decisions based on numerical values, perform calculations, and automate tasks effectively.  Remember the spaces around the operators and the quoting best practices!




	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• String comparison operators
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	String comparison operators are used in shell scripts (Bash, sh, ksh, zsh, etc.) to compare strings.  They are crucial for conditional execution based on string values.  Here's a breakdown of the common string comparison operators:

General Syntax:

String comparisons are typically used within conditional expressions, most often with test, [ ] (single brackets), or [[ ]] (double brackets).

Operators:

Operator	Meaning	Example (using [ ])	Example (using [[ ]])
= or ==	Equal to	[ "hello" = "hello" ] or [ "hello" == "hello" ]	[[ "hello" == "hello" ]] or [[ "hello" = "hello" ]]
!=	Not equal to	[ "hello" != "world" ]	[[ "hello" != "world" ]]
<	Less than (lexicographically)	[ "apple" < "banana" ]	[[ "apple" < "banana" ]]
>	Greater than (lexicographically)	[ "zebra" > "apple" ]	[[ "zebra" > "apple" ]]

Export to Sheets
Important Notes:

Quoting is essential (especially with [ ]): Always quote your string variables, especially when using [ ]. This prevents word splitting and globbing issues. For example, [ "$string1" = "$string2" ] is correct, but [ $string1 = $string2 ] can lead to unexpected behavior if the variables contain spaces or special characters. With [[ ]], quoting is often less critical, but it is still good practice, especially if your strings contain globbing characters (*, ?, []).
Lexicographical order: The < and > operators compare strings lexicographically (alphabetical order).
[[ ]] enhancements (Bash-specific): [[ ]] provides some additional features for string comparisons:
Pattern matching: == can be used for basic pattern matching (globbing).
Regular expression matching: =~ allows you to compare a string against a regular expression.
Examples:

Bash

#!/bin/bash

string1="hello"
string2="world"

if [ "$string1" = "$string1" ]; then  # Correct: quoting is crucial with [ ]
  echo "Strings are equal"
fi

if [[ $string1 == $string1 ]]; then # Quoting is less critical with [[ ]]
  echo "Strings are equal"
fi

if [ "$string1" != "$string2" ]; then
  echo "Strings are not equal"
fi

if [[ $string1 != $string2 ]]; then  # Quoting is less critical with [[ ]]
  echo "Strings are not equal"
fi

if [[ "$string1" == h* ]]; then  # Pattern matching with [[ ]]
  echo "string1 starts with 'h'"
fi

if [[ "$string1" =~ ^h.*$ ]]; then  # Regular expression matching with [[ ]]
  echo "string1 starts with 'h' (regex)"
fi

if [[ "$string1" < "$string2" ]]; then
  echo "string1 is less than string2"
fi

if [[ "$string2" > "$string1" ]]; then
  echo "string2 is greater than string1"
fi

# Empty string check
if [ -z "$string1" ]; then
  echo "string1 is empty"
fi

if [[ -z "$string1" ]]; then # [[ ]] version of above check
  echo "string1 is empty"
fi

if [ -n "$string1" ]; then
  echo "string1 is not empty"
fi

if [[ -n "$string1" ]]; then # [[ ]] version of above check
  echo "string1 is not empty"
fi
Which to use?

[ ]: Use this for POSIX-compliant scripts that need to work on various shells. Be extremely careful with quoting.
[[ ]]: Use this in Bash scripts when you need the extended features (pattern matching, regular expression matching) and want to avoid some of the quoting pitfalls of [ ]. It's generally preferred in Bash scripts.
Key Differences Summarized:

Feature	[ ]	[[ ]]
POSIX	Yes	No (Bash extension)
Quoting	Crucial	Less crucial
Pattern Matching	No	Yes (==)
Regular Expressions	No	Yes (=~)
Word Splitting/Globbing	Happens	Doesn't happen

Export to Sheets
In most modern Bash scripting, [[ ]] is the preferred choice for string comparisons because it's more powerful and less error-prone.  However, if you need your script to be portable to other shells, stick with [ ] (and double-check your quoting!).
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Logical operations
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	Logical operations in shell scripts allow you to combine and manipulate conditional expressions.  They are essential for creating complex logic and controlling the flow of your scripts.  Here's a breakdown of the common logical operators:

1. AND (&&):

Meaning: The && operator returns true if both operands are true. If the first operand is false, the second operand is not evaluated. This is often called "short-circuiting."
Example:
Bash

if [[ condition1 && condition2 ]]; then
  echo "Both conditions are true"
fi

# Example with commands:
command1 && command2  # command2 only runs if command1 succeeds

# Example illustrating short-circuiting:
false_command && echo "This will not be printed"
true_command && echo "This will be printed"
2. OR (||):

Meaning: The || operator returns true if at least one of the operands is true. If the first operand is true, the second operand is not evaluated. This is also short-circuiting.
Example:
Bash

if [[ condition1 || condition2 ]]; then
  echo "At least one condition is true"
fi

# Example with commands:
command1 || command2  # command2 only runs if command1 fails

# Example illustrating short-circuiting:
true_command || echo "This will not be printed"
false_command || echo "This will be printed"
3. NOT (!):

Meaning: The ! operator negates a condition. If the operand is true, ! makes it false, and vice-versa.
Example:
Bash

if [[ ! condition ]]; then
  echo "Condition is false"
fi

# Example with a command:
if ! command_that_might_fail; then
    echo "The command failed"
fi
4. Parentheses () for grouping:

You can use parentheses to group conditional expressions and control the order of evaluation.

Example:
Bash

if [[ (condition1 && condition2) || condition3 ]]; then
  echo "Either (condition1 AND condition2) is true, OR condition3 is true"
fi
5. Combining with test, [ ], and [[ ]]:

Logical operators are typically used within conditional constructs like if statements, while loops, or in conjunction with the test command, [ ] (single brackets), or [[ ]] (double brackets).

With test or [ ]:  (POSIX-compliant, but less preferred in Bash)

Bash

if [ condition1 -a condition2 ]; then # -a is AND, -o is OR
  echo "Both conditions are true"
fi

if [ condition1 -o condition2 ]; then
  echo "At least one condition is true"
fi

if ! [ condition ]; then
  echo "Condition is false"
fi
With [[ ]]: (Bash-specific, generally preferred in Bash)

Bash

if [[ condition1 && condition2 ]]; then
  echo "Both conditions are true"
fi

if [[ condition1 || condition2 ]]; then
  echo "At least one condition is true"
fi

if [[ ! condition ]]; then
  echo "Condition is false"
fi
Which to use?

&&, ||, and ! are the preferred logical operators, especially when used with [[ ]] in Bash scripts. They are clearer and more consistent with other programming languages.
-a, -o, and ! (used with test and [ ]) are POSIX-compliant but can be less readable. Avoid them in Bash scripts unless you need strict POSIX compatibility.
Example (Combining concepts):

Bash

#!/bin/bash

file="myfile.txt"

if [[ -f "$file" && ( ! -w "$file" || -x "$file" ) ]]; then
  echo "File '$file' exists AND (it's NOT writable OR it's executable)"
fi

if [[ ! -d "mydir" || ( -d "mydir" && ! -x "mydir" ) ]]; then
  echo "Either 'mydir' does NOT exist OR (it exists AND it's NOT executable)"
fi
Understanding logical operations is fundamental to writing shell scripts that can make complex decisions and automate tasks based on different conditions.  Using the appropriate operators and grouping expressions with parentheses will allow you to create powerful and flexible scripts.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• File testing Operators
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	File testing operators in shell scripting allow you to check various properties of files and directories.  These operators are essential for writing scripts that interact with the file system and make decisions based on the existence, type, permissions, and other attributes of files.  They are typically used within conditional expressions (e.g., if statements, while loops).

Here's a breakdown of the most common file testing operators:

Basic File Tests:

Operator	Meaning
-e	File exists (regardless of type)
-f	File exists and is a regular file
-d	File exists and is a directory
-l	File exists and is a symbolic link
-b	File exists and is a block special file (e.g., a block device)
-c	File exists and is a character special file (e.g., a terminal)
-p	File exists and is a named pipe (FIFO)
-S	File exists and is a socket

Export to Sheets
Permission Tests:

Operator	Meaning
-r	File exists and is readable by the current user
-w	File exists and is writable by the current user
-x	File exists and is executable by the current user

Export to Sheets
Other Useful Tests:

Operator	Meaning
-s	File exists and has a size greater than zero (not empty)
-G	File exists and is owned by the effective group ID of the current user
-O	File exists and is owned by the effective user ID of the current user
-N	File exists and has been modified since it was last read

Export to Sheets
How to Use Them:

File testing operators are usually used within test, [ ] (single brackets), or [[ ]] (double brackets).  [[ ]] is generally preferred in Bash scripts because it handles quoting and other characters more safely.

Examples (using [[ ]] - recommended in Bash):

Bash

#!/bin/bash

file="myfile.txt"
directory="mydir"
link="mylink"

if [[ -f "$file" ]]; then
  echo "$file is a regular file"
fi

if [[ -d "$directory" ]]; then
  echo "$directory is a directory"
fi

if [[ -l "$link" ]]; then
  echo "$link is a symbolic link"
fi

if [[ -r "$file" ]]; then
  echo "$file is readable"
fi

if [[ -w "$file" ]]; then
  echo "$file is writable"
fi

if [[ -x "$file" ]]; then
  echo "$file is executable"
fi

if [[ -s "$file" ]]; then
  echo "$file is not empty"
fi

if [[ -e "$file" ]]; then
    echo "$file exists"
fi

if [[ -G "$file" ]]; then
    echo "$file is owned by the effective group ID of the current user"
fi

if [[ -O "$file" ]]; then
    echo "$file is owned by the effective user ID of the current user"
fi

if [[ -N "$file" ]]; then
    echo "$file has been modified since it was last read"
fi


# Combining tests with logical operators
if [[ -f "$file" && -r "$file" ]]; then
  echo "$file is a regular file and is readable"
fi

if [[ -d "$directory" || -f "$file" ]]; then
  echo "Either $directory is a directory OR $file is a regular file"
fi

if [[ ! -f "$file" ]]; then
  echo "$file is NOT a regular file"
fi
Examples (using [ ] - POSIX compliant, but be careful with quoting!):

Bash

if [ -f "$file" ]; then  # Quoting is crucial here!
  echo "$file is a regular file"
fi

if [ -d "$directory" ]; then # Quoting is crucial here!
  echo "$directory is a directory"
fi

# ... (other tests are similar, just remember the quoting!)
Key Differences and Recommendations:

[[ ]] (double brackets) are a Bash extension, offering more features and safer syntax (less need for quoting). Use [[ ]] in Bash scripts unless you need POSIX compatibility.
[ ] (single brackets) are POSIX compliant but require careful quoting to avoid word splitting and globbing issues.
test is equivalent to [ ].
Which to use?

In Bash scripts, [[ ]] is generally the preferred choice for file testing due to its enhanced features and safer handling of strings.  If you need POSIX compatibility, use [ ] (but be extra careful with quoting!).  The examples above should give you a solid foundation for using file testing operators in your scripts.  Remember to choose the operator that best suits the specific file property you need to check.
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Integer arithmetic – expr
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The expr command in shell scripting is used for performing integer arithmetic and other basic operations like string length, substring extraction, and pattern matching.  While expr is POSIX compliant and available on most Unix-like systems, it's often considered a bit clunky and less convenient than other methods for arithmetic, especially in Bash.  Bash's arithmetic expansion $((...)) is generally preferred for integer arithmetic in modern Bash scripts.

expr Syntax:

Bash

expr argument1 operator argument2
Arguments: Arguments must be integers or strings that can be interpreted as integers.
Operators: expr uses specific operators, and spaces are required around them.
Common expr Operators:

Operator	Meaning	Example
+	Addition	expr 5 + 3
-	Subtraction	expr 10 - 4
*	Multiplication	expr 2 \* 3 (note the backslash for escaping *)
/	Integer division	expr 10 / 3
%	Modulus (remainder)	expr 10 % 3

Export to Sheets
Important Notes:

Spaces are essential: expr 5+3 is incorrect. You must have spaces around the operators: expr 5 + 3.
* must be escaped: The asterisk * has special meaning in the shell (filename globbing), so it must be escaped with a backslash \ when used for multiplication in expr: expr 2 \* 3.
Integer arithmetic only: expr is designed for integer arithmetic. For floating-point operations, use bc or awk.
Command substitution: The result of an expr command is often captured using command substitution ($(...) or `... `).
Examples:

Bash

#!/bin/bash

x=10
y=5

sum=$(expr "$x" + "$y")  # Important: Quote variables!
echo "Sum: $sum"

difference=$(expr "$x" - "$y")
echo "Difference: $difference"

product=$(expr "$x" \* "$y")  # Escape the asterisk!
echo "Product: $product"

quotient=$(expr "$x" / "$y")
echo "Quotient: $quotient"

remainder=$(expr "$x" % "$y")
echo "Remainder: $remainder"

# Using expr in a conditional expression
if [ $(expr "$x" -gt "$y") -eq 1 ]; then # expr returns 1 for true, 0 for false.
  echo "$x is greater than $y"
fi
Bash Arithmetic Expansion ($((...)) - The Preferred Way in Bash:

Bash provides a much more convenient way to perform integer arithmetic using $((...)).  It's generally preferred over expr in Bash scripts because it's cleaner, more efficient, and handles quoting more naturally.

Examples using $((...)):

Bash

#!/bin/bash

x=10
y=5

sum=$((x + y))
echo "Sum: $sum"

difference=$((x - y))
echo "Difference: $difference"

product=$((x * y))  # No need to escape *
echo "Product: $product"

quotient=$((x / y))
echo "Quotient: $quotient"

remainder=$((x % y))
echo "Remainder: $remainder"

if (( x > y )); then # No need for [ ] or test
  echo "$x is greater than $y"
fi
Key Differences and Recommendations:

expr is POSIX compliant and portable, but it's clunky due to the required spaces around operators and the need to escape *.
$((...)) is a Bash extension, but it's much more convenient and efficient for integer arithmetic in Bash scripts. It's generally the preferred method.
For floating-point arithmetic, use bc or awk.
Which to use?

If you need strict POSIX compliance, you might have to use expr.
In Bash scripts, $((...)) is almost always the better choice for integer arithmetic. It's cleaner, more efficient, and less prone to errors.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Floating number arithmetic - bc
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	The bc command (basic calculator) is a powerful tool for performing floating-point arithmetic in shell scripts.  It's especially useful when you need more precision than integer arithmetic or when dealing with decimal numbers.

How bc Works:

bc is a command-line utility that acts as an arbitrary-precision calculator.  It reads expressions from standard input, evaluates them, and prints the results to standard output.

Basic Usage:

Bash

bc [options] [file]
If you don't provide a file, bc enters interactive mode, where you can type expressions and see the results immediately.
options can be used to control bc's behavior (e.g., setting the scale for decimal places).
Key Concepts and Features:

Scale: The scale variable controls the number of decimal places in the output. You set it using scale=number.
Operators: bc supports a wide range of arithmetic operators: + (addition), - (subtraction), * (multiplication), / (division), % (modulus), ^ (exponentiation).
Functions: bc also has built-in functions like sqrt() (square root), sin() (sine), cos() (cosine), atan() (arctangent), exp() (exponential), log() (logarithm).
Input from files or strings: You can provide input to bc from a file or directly as a string using here strings (<<<).
Interactive mode: If you run bc without any arguments, it enters interactive mode.
Examples:

Basic arithmetic:

Bash

result=$(bc <<< "2.5 + 3.7")
echo "$result"  # Output: 6.2
Setting the scale:

Bash

result=$(bc <<< "scale=2; 10 / 3")
echo "$result"  # Output: 3.33

result=$(bc <<< "scale=5; 10 / 3")
echo "$result"  # Output: 3.33333
Floating-point comparisons:

Bash

x=3.14
y=2.71

if [[ $(bc <<< "$x > $y") -eq 1 ]]; then  # bc returns 1 for true, 0 for false
  echo "$x is greater than $y"
fi

if [[ $(bc <<< "scale=2; $x == $y") -eq 1 ]]; then  # scale is important for equality checks!
  echo "$x is equal to $y"
fi
Using functions:

Bash

result=$(bc <<< "scale=3; sqrt(2)")
echo "$result"  # Output: 1.414
Here strings:

Bash

result=$(bc <<< "scale=2; 10 / 3 + 5.2")  # Multiple operations
echo "$result"
Interactive mode:

Bash

bc
scale=2
10 / 3
# Output: 3.33
quit
Using bc in scripts:

bc is most often used within command substitution ($(...)) to capture the result of a calculation and store it in a shell variable.  Remember to set the scale appropriately for the desired precision.

Example (calculating average):

Bash

#!/bin/bash

sum=0
count=0

while read num; do
  sum=$(bc <<< "$sum + $num")
  count=$((count + 1))
done < numbers.txt  # Read numbers from the file numbers.txt

if [[ $count -gt 0 ]]; then
  average=$(bc <<< "scale=2; $sum / $count")
  echo "Average: $average"
else
  echo "No numbers to average."
fi
Key Considerations:

Precision: The scale setting is crucial for controlling the number of decimal places.
Integer vs. Floating-Point: bc is specifically designed for floating-point arithmetic. For integer arithmetic, Bash's $((...)) is usually more efficient.
Comparison: When comparing floating-point numbers, be sure to set an appropriate scale and compare using == (or check if the difference is within a small tolerance).
bc is an essential tool for performing floating-point calculations in shell scripts.  Its ability to handle arbitrary precision and its support for various functions make it a powerful calculator for scripting purposes.  Remember to use scale to control the precision and to use command substitution to capture the results of your calculations.
	
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
CONTROL FLOW STRUCTURES
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The if-then-else Construct
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		refer Shell scripting folder 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• if Example
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		refer Shell scripting folder 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The Case Statement
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		refer Shell scripting folder 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• case Code Example
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		refer Shell scripting folder 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• SELECT example
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		refer Shell scripting folder 
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Conditional Loop syntax
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o while loop Example
		o until Loop Example
		o for loop Example
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• /dev/null Logic
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• HERE document
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Day – 5
Array variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Creating array Variable
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	There are several ways to create and work with array variables in shell scripting (Bash, zsh, ksh, etc.). Here's a breakdown of the most common methods and operations:

1. Creating Arrays:

Using parentheses and space separation:
Bash

my_array=("apple" "banana" "orange")
This is the most common and recommended way.  Elements are enclosed in parentheses and separated by spaces.

Using indices:
Bash

my_array[0]="apple"
my_array[1]="banana"
my_array[2]="orange"
This method allows you to assign values to specific indices.  Indices can be any integer, including 0, 1, 2, etc.  You can even use associative arrays (explained later) with string indices in some shells.

Declaring an empty array:
Bash

declare -a my_array  # Good practice, especially in functions to avoid unintended side effects
my_array=() # Another way to declare an empty array
This is useful when you want to start with an empty array and populate it later.

2. Accessing Array Elements:

Using indices:
Bash

echo "${my_array[0]}"  # Access the first element (apple)
echo "${my_array[1]}"  # Access the second element (banana)
echo "${my_array[2]}"  # Access the third element (orange)
${my_array[@]} or ${my_array[*]}: Expands to all array elements.  The difference is in how they handle elements with spaces.

"${my_array[@]}": Treats each element as a separate word, even if it contains spaces. This is almost always what you want.
"${my_array[*]}": Combines all elements into a single word.
Bash

for fruit in "${my_array[@]}"; do  # Iterate over each element (correctly handles spaces)
  echo "$fruit"
done

echo "${my_array[@]}"  # Prints all elements, separated by spaces.
echo "${my_array[*]}"  # Prints all elements as a single word.
${#my_array[@]} or ${#my_array[*]}: Returns the number of elements in the array.
Bash

echo "Number of elements: ${#my_array[@]}"
3. Modifying Arrays:

Assigning to an index:
Bash

my_array[1]="grape"  # Change the second element to "grape"
Appending elements:
Bash

my_array+=("kiwi")  # Add "kiwi" to the end of the array
my_array[${#my_array[@]}]="mango" # Another way to append to the end of the array
Unsetting an element:
Bash

unset my_array[1]  # Remove the second element
Removing an array:
Bash

unset my_array  # Remove the entire array
4. Associative Arrays (Bash 4+):

Associative arrays allow you to use strings as indices.  They are like dictionaries or hash tables in other languages.

Bash

declare -A my_assoc_array  # Declare an associative array

my_assoc_array["red"]="apple"
my_assoc_array["yellow"]="banana"
my_assoc_array["orange"]="orange"

echo "${my_assoc_array["red"]}"  # Output: apple
echo "${my_assoc_array["yellow"]}" # Output: banana

for color in "${!my_assoc_array[@]}"; do # Iterate over the keys
  fruit="${my_assoc_array[$color]}"
  echo "$color: $fruit"
done
5. Array Slicing (Bash 4.2+):

You can extract a portion of an array using array slicing.

Bash

my_array=("apple" "banana" "orange" "grape" "kiwi")

echo "${my_array[@]:1:2}" # Start at index 1, take 2 elements (banana orange)
echo "${my_array[@]:2}"   # Start at index 2, take all remaining elements (orange grape kiwi)
6. Array Length:

${#my_array[@]} or ${#my_array[*]}: Returns the number of elements in the array.
Best Practices:

Use "${my_array[@]}" for iterating: This ensures that elements with spaces are treated correctly.
Quote variables: When accessing array elements, especially within [ ] or test, quote the variable: "${my_array[0]}". With [[ ]] or for ... in, quoting is generally less essential unless your elements contain globbing characters.
Declare arrays: It's good practice to declare arrays using declare -a my_array (for indexed arrays) or declare -A my_assoc_array (for associative arrays), especially within functions. This makes your code more readable and avoids potential issues with variable shadowing.
Use associative arrays when appropriate: If you need to use string indices, associative arrays are the way to go.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Substituting and counting
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	string="hello world hello"

new_string="${string/hello/goodbye}"  # Replaces the first "hello"
echo "$new_string"  # Output: goodbye world hello

new_string="${string//hello/goodbye}" # Replaces all "hello"s
echo "$new_string"  # Output: goodbye world goodbye

new_string="${string/#hello/goodbye}" # Replaces "hello" at the beginning
echo "$new_string"  # Output: goodbye world hello

new_string="${string/%hello/goodbye}" # Replaces "hello" at the end
echo "$new_string"  # Output: hello world goodbye
sed (Stream Editor):  sed is a powerful command-line utility for stream editing. It's excellent for complex substitutions and working with files.

Bash

# Replace all "hello" with "goodbye"
new_string=$(echo "$string" | sed 's/hello/goodbye/g')  # 'g' flag for global (all occurrences)
echo "$new_string"  # Output: goodbye world goodbye

# Substitute in a file:
sed 's/old_text/new_text/g' input.txt > output.txt

# In-place substitution (be careful!):
sed -i 's/old_text/new_text/g' input.txt
awk: awk is another powerful text processing tool that can also be used for substitutions.

Bash

new_string=$(echo "$string" | awk '{gsub("hello", "goodbye"); print}')
echo "$new_string" # Output: goodbye world goodbye
2. Counting Occurrences:

Using grep and wc: This is a common approach for counting occurrences of a pattern in a string or file.

Bash

count=$(echo "$string" | grep -o "hello" | wc -l) # -o prints each match on a new line
echo "Count of 'hello': $count"  # Output: Count of 'hello': 2

# Count occurrences in a file:
count=$(grep -o "pattern" file.txt | wc -l)
echo "Count in file: $count"
Using awk:

Bash

count=$(echo "$string" | awk '{count=gsub("hello", "hello")} END {print count}')
echo "Count of 'hello': $count" # Output: Count of 'hello': 2

count=$(awk '/hello/ {count++} END {print count}' file.txt)
echo "Count in file: $count"
Looping and Conditional: You can also manually count occurrences using loops and conditional statements, but the methods above are generally more efficient.

Bash

count=0
for word in $string; do # Note: this will split the string by spaces!
  if [[ "$word" == "hello" ]]; then
    ((count++))
  fi
done
echo "Count of 'hello': $count"
Which method to use?

Parameter expansion: Best for simple substitutions within variables, especially when you are only replacing one or a few instances. It is efficient and built into the shell.
sed: Best for more complex substitutions, especially when working with files or when you need regular expressions.
awk: Useful for substitutions and counting, especially when you need to combine these operations with other text processing tasks.
grep and wc: The most common and often the most efficient way to count occurrences of a pattern in a string or a file.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Using integer variables as element numbers
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	
	Here's how you do it and some important considerations:

1. Basic Indexed Arrays:

Bash

my_array=("apple" "banana" "orange")

index=0  # Initialize an integer variable

echo "${my_array[$index]}"  # Access the element at the given index (apple)

index=2
echo "${my_array[$index]}"  # Access the element at index 2 (orange)

# Iterate through an array using an index:
for i in $(seq 0 $(( ${#my_array[@]} - 1 ))); do # seq generates a sequence of numbers
  echo "Element at index $i: ${my_array[$i]}"
done

# A more robust way to iterate:
for i in "${!my_array[@]}"; do # "${!my_array[@]}" expands to the array's indices
  echo "Element at index $i: ${my_array[$i]}"
done

# Using arithmetic operations with the index:
index=1
next_index=$((index + 1))
echo "Next element: ${my_array[$next_index]}" # Access the next element

# Setting array elements using variables:
new_value="grape"
my_array[$index]="$new_value" # Set the element at index $index to the new value
echo "${my_array[$index]}" # Print the updated value
2. Associative Arrays (Bash 4+):

While associative arrays use strings as keys, you can still use integer variables to generate those keys.

Bash

declare -A my_assoc_array

key_num=10
my_assoc_array["item_$key_num"]="value_$key_num"

echo "${my_assoc_array["item_$key_num"]}"  # Access the value using the generated key

# Or directly:
echo "${my_assoc_array["item_10"]}" # Access the value using the key directly
3. Important Considerations:

Quoting: When using variables as indices, it's generally good practice to quote them, especially within [ ] or test. For example: "${my_array[$index]}". Within [[ ]] or when using for i in "${!my_array[@]}", quoting is less critical but still good practice.
Array Bounds: Be very careful not to access array elements outside the valid index range (0 to length - 1). Accessing an out-of-bounds index can lead to unexpected behavior or errors. Always check the array length (${#my_array[@]}) before accessing elements.
Arithmetic Expansion: Use $((...)) for arithmetic operations on your index variables.
Looping: The most common use case is iterating through arrays. The recommended way is to iterate over the indices of the array and use those indices to access the elements:
Bash

for i in "${!my_array[@]}"; do # Safe and correct way to iterate
    echo "Element at index $i: ${my_array[$i]}"
done
This approach is safer and more robust, especially when dealing with arrays that might have "holes" (unassigned indices).  The older for i in "${my_array[@]}" will split elements containing spaces and is not recommended.

Example: Processing arguments and storing them in an array:

Bash

#!/bin/bash

my_args=()  # Initialize an empty array

i=0
while [ $# -gt 0 ]; do
  my_args[$i]="$1"  # Store the argument in the array
  i=$((i + 1))
  shift
done

# Now you can access the arguments using the array and index variables:

echo "Number of arguments: ${#my_args[@]}"

for j in "${!my_args[@]}"; do
  echo "Argument $j: ${my_args[$j]}"
done
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab Session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Writing script for
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o performing backup of file interactively and using positional parameter
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o logging into oracle database and performing spool
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o file testing
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o removing duplicate files
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
UST Global assignment 		
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~		
		Write a shell script that prints “Shell Scripting is Fun!” on the screen.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Modify the shell script from exercise 1 to include a variable. The variable will hold the contents of the message “Shell Scripting is Fun!”
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Write a shell script to output a specified directory’s size.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Write a shell script to show hardware information for Linux systems.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Write a shell script to check if a number input from standard input is odd or even.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Write a shell script to test if a number being entered is a Fibonacci number or not.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Write a script to zip a file.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Write a script to unzip a file.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


		
		
	• Debugging shell script
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The 'getopts' Command
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Processing arguments
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The getopts and OPTARG variable
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The OPTIND variable
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab Session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Traps, Signals and Script Control
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Common signals
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Running Scripts in Background Mode
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Scheduling your script
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Starting the Script at Boot Time
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• The trap commands
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Lab Session
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Other Utility Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Listing CPU & Memory utilization using top command
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	The top command in Linux provides a dynamic view of the currently running processes, CPU usage, memory utilization, and other system information. Here are some of the most frequently used options:

-b (Batch mode):

Runs top in batch mode, suppressing interactive commands and sending output to standard output. This is useful for scripting or redirecting output to a file.
Example: top -b > top_output.txt
-c:

Displays the full command line for each process.
-d <seconds>:

Sets the delay between updates.
Example: top -d 2 (updates every 2 seconds)
-H:

Displays individual threads within each process.
-i <idletime>:

Ignores idle processes for the specified number of seconds.
-n <number>:

Specifies the number of iterations to run before exiting in batch mode.
-o <field>:

Sorts the process list by the specified field (e.g., -o %CPU to sort by CPU usage).
-p <pid1>,<pid2>,...:

Monitors only the specified process IDs.
-u <username>:

Displays only processes belonging to the specified user.
-s:

Enables secure mode, even for root.
Using top interactively:

h or ?: Displays help information within the top interface.
q: Exits the top command.
k: Sends the SIGKILL signal to a selected process.
r: Renice a process (change its priority).
c: Toggle the display of the full command line for each process.
M: Sort by memory usage.
P: Sort by CPU usage.
T: Sort by running time.
N: Sort by process ID.
	
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	• Network Basics:
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o Internet Protocol
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o Addressing – inet(IPv4), inet6(IPv6)
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	
	• Networking utilities: ping, telnet, ifconfig, netstat
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		o continue and break 
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	ping - Test Network Connectivity
		Command Example:
		 
		 
		 
		ping google.com
		Details:
		This command sends ICMP echo requests to google.com to test connectivity and measure response time.
		How it works:
		Displays the time taken for each packet to travel to the destination and back.
		Options:
		-c <count>: Limit the number of packets sent.
		 
		 
		 
		ping -c 5 google.com
		-i <interval>: Set the interval between packets.
		 
		 
		 
		ping -i 2 google.com